@article{250511412v1,
  title={ Uncertainty quantification with approximate variational learning for   wearable photoplethysmography prediction tasks },
  author={ Ciaran Bench and Vivek Desai and Mohammad Moulaeifard and Nils Strodthoff and Philip Aston and Andrew Thompson },
  journal={ arXiv preprint arXiv:2505.11412v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2505.11412v1 },
  abstract={ Photoplethysmography (PPG) signals encode information about relative changes in blood volume that can be used to assess various aspects of cardiac health non-invasively, e.g.\\textbackslash{} to detect atrial fibrillation (AF) or predict blood pressure (BP). Deep networks are well-equipped to handle the large quantities of data acquired from wearable measurement devices. However, they lack interpretability and are prone to overfitting, leaving considerable risk for poor performance on unseen data and misdiagnosis. Here, we describe the use of two scalable uncertainty quantification techniques: Monte Carlo Dropout and the recently proposed Improved Variational Online Newton. These techniques are used to assess the trustworthiness of models trained to perform AF classification and BP regression from raw PPG time series. We find that the choice of hyperparameters has a considerable effect on the predictive performance of the models and on the quality and composition of predicted uncertainties. E.g. the stochasticity of the model parameter sampling determines the proportion of the total uncertainty that is aleatoric, and has varying effects on predictive performance and calibration quality dependent on the chosen uncertainty quantification technique and the chosen expression of uncertainty. We find significant discrepancy in the quality of uncertainties over the predicted classes, emphasising the need for a thorough evaluation protocol that assesses local and adaptive calibration. This work suggests that the choice of hyperparameters must be carefully tuned to balance predictive performance and calibration quality, and that the optimal parameterisation may vary depending on the chosen expression of uncertainty. }
}

@article{241209758v2,
  title={ Toward Foundation Model for Multivariate Wearable Sensing of   Physiological Signals },
  author={ Yunfei Luo and Yuliang Chen and Asif Salekin and Tauhidur Rahman },
  journal={ arXiv preprint arXiv:2412.09758v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.09758v2 },
  abstract={ Time-series foundation models excel at tasks like forecasting across diverse data types by leveraging informative waveform representations. Wearable sensing data, however, pose unique challenges due to their variability in patterns and frequency bands, especially for healthcare-related outcomes. The main obstacle lies in crafting generalizable representations that adapt efficiently across heterogeneous sensing configurations and applications. To address this, we propose NormWear, the first multi-modal and ubiquitous foundation model designed to extract generalized and informative representations from wearable sensing data. Specifically, we design a channel-aware attention mechanism with a shared special liaison [CLS] token to detect signal patterns in both intra-sensor and inter-sensors. This helps the model to extract more meaningful information considering both time series themselves and the relationships between input sensors. This helps the model to be widely compatible with various sensors settings. NormWear is pretrained on a diverse set of physiological signals, including PPG, ECG, EEG, GSR, and IMU, from various public datasets. Our model shows exceptional generalizability across 11 public wearable sensing datasets, spanning 18 applications in mental health, body state inference, vital sign estimation, and disease risk evaluation. It consistently outperforms competitive baselines under zero-shot, partial-shot, and full-shot settings, indicating broad applicability in real-world health applications. }
}

@article{250510556v1,
  title={ An AI-driven framework for the prediction of personalised health   response to air pollution },
  author={ Nazanin Zounemat Kermani and Sadjad Naderi and Claire H. Dilliway and Claire E. Heaney and Shrreya Behll and Boyang Chen and Hisham Abubakar-Waziri and Alexandra E. Porter and Marc Chadeau-Hyam and Fangxin Fang and Ian M. Adcock and Kian Fan Chung and Christopher C. Pain },
  journal={ arXiv preprint arXiv:2505.10556v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2505.10556v1 },
  abstract={ Air pollution poses a significant threat to public health, causing or exacerbating many respiratory and cardiovascular diseases. In addition, climate change is bringing about more extreme weather events such as wildfires and heatwaves, which can increase levels of pollution and worsen the effects of pollution exposure. Recent advances in personal sensing have transformed the collection of behavioural and physiological data, leading to the potential for new improvements in healthcare. We wish to capitalise on this data, alongside new capabilities in AI for making time series predictions, in order to monitor and predict health outcomes for an individual. Thus, we present a novel workflow for predicting personalised health responses to pollution by integrating physiological data from wearable fitness devices with real-time environmental exposures. The data is collected from various sources in a secure and ethical manner, and is used to train an AI model to predict individual health responses to pollution exposure within a cloud-based, modular framework. We demonstrate that the AI model -- an Adversarial Autoencoder neural network in this case -- accurately reconstructs time-dependent health signals and captures nonlinear responses to pollution. Transfer learning is applied using data from a personal smartwatch, which increases the generalisation abilities of the AI model and illustrates the adaptability of the approach to real-world, user-generated data. }
}

@article{250506945v2,
  title={ A systematic review of challenges and proposed solutions in modeling   multimodal data },
  author={ Maryam Farhadizadeh and Maria Weymann and Michael Blaß and Johann Kraus and Christopher Gundler and Sebastian Walter and Noah Hempen and Harald Binder and Nadine Binder },
  journal={ arXiv preprint arXiv:2505.06945v2 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2505.06945v2 },
  abstract={ Multimodal data modeling has emerged as a powerful approach in clinical research, enabling the integration of diverse data types such as imaging, genomics, wearable sensors, and electronic health records. Despite its potential to improve diagnostic accuracy and support personalized care, modeling such heterogeneous data presents significant technical challenges. This systematic review synthesizes findings from 69 studies to identify common obstacles, including missing modalities, limited sample sizes, dimensionality imbalance, interpretability issues, and finding the optimal fusion techniques. We highlight recent methodological advances, such as transfer learning, generative models, attention mechanisms, and neural architecture search that offer promising solutions. By mapping current trends and innovations, this review provides a comprehensive overview of the field and offers practical insights to guide future research and development in multimodal modeling for medical applications. }
}

@article{250509508v1,
  title={ Wearable Tracking of Eye and Body Movements During Breaching Training:   Towards Real-Time Blast Injury Monitoring },
  author={ Jeremy P. Kemmerer and James R. Williamson and Joseph Kim and Elizabeth Halford and Hrishikesh M. Rao and Christopher J. Smalt },
  journal={ arXiv preprint arXiv:2505.09508v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2505.09508v1 },
  abstract={ Repeated exposure to blast overpressure in occupational settings has been associated with changes in cognitive and psychological health, as well as deficits in neurosensory subsystems. In this work, we describe a wearable system to simultaneously monitor physiology and blast exposure levels and demonstrate how this system can identify individualized exposure levels corresponding to acute physiological response to blast exposure. Machine learning was used to develop a dose-response model that fused multiple physiological measures (electrooculuography, gait, and balance) into a single risk score by predicting the level of blast exposure on held-out subjects (Fused model, R = 0.60). We found that blast events with peak pressure levels as low as 0.25 psi could be related to physiological changes and hence may contribute to blast injury. We also identified an individual subject with deteriorating reaction time scores that consistently showed a rapid and anomalous change in physiology-based risk scores after exposure to low-level blast events. Our results suggest that the wearable approach to blast monitoring is viable in weapons training environments as a complement to more direct but sparsely administered brain health assessments, potentially viable in austere environments, and that fusing multiple physiological signals can improve sensitivity. }
}

@article{250503374v1,
  title={ Reducing Annotation Burden in Physical Activity Research Using   Vision-Language Models },
  author={ Abram Schonfeldt and Benjamin Maylor and Xiaofang Chen and Ronald Clark and Aiden Doherty },
  journal={ arXiv preprint arXiv:2505.03374v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2505.03374v1 },
  abstract={ Introduction: Data from wearable devices collected in free-living settings, and labelled with physical activity behaviours compatible with health research, are essential for both validating existing wearable-based measurement approaches and developing novel machine learning approaches. One common way of obtaining these labels relies on laborious annotation of sequences of images captured by cameras worn by participants through the course of a day. Methods: We compare the performance of three vision language models and two discriminative models on two free-living validation studies with 161 and 111 participants, collected in Oxfordshire, United Kingdom and Sichuan, China, respectively, using the Autographer (OMG Life, defunct) wearable camera. Results: We found that the best open-source vision-language model (VLM) and fine-tuned discriminative model (DM) achieved comparable performance when predicting sedentary behaviour from single images on unseen participants in the Oxfordshire study; median F1-scores: VLM = 0.89 (0.84, 0.92), DM = 0.91 (0.86, 0.95). Performance declined for light (VLM = 0.60 (0.56,0.67), DM = 0.70 (0.63, 0.79)), and moderate-to-vigorous intensity physical activity (VLM = 0.66 (0.53, 0.85); DM = 0.72 (0.58, 0.84)). When applied to the external Sichuan study, performance fell across all intensity categories, with median Cohen's kappa-scores falling from 0.54 (0.49, 0.64) to 0.26 (0.15, 0.37) for the VLM, and from 0.67 (0.60, 0.74) to 0.19 (0.10, 0.30) for the DM. Conclusion: Freely available computer vision models could help annotate sedentary behaviour, typically the most prevalent activity of daily living, from wearable camera images within similar populations to seen data, reducing the annotation burden. }
}

@article{250503039v1,
  title={ An Explainable Anomaly Detection Framework for Monitoring Depression and   Anxiety Using Consumer Wearable Devices },
  author={ Yuezhou Zhang and Amos A. Folarin and Callum Stewart and Heet Sankesara and Yatharth Ranjan and Pauline Conde and Akash Roy Choudhury and Shaoxiong Sun and Zulqarnain Rashid and Richard J. B. Dobson },
  journal={ arXiv preprint arXiv:2505.03039v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2505.03039v1 },
  abstract={ Continuous monitoring of behavior and physiology via wearable devices offers a novel, objective method for the early detection of worsening depression and anxiety. In this study, we present an explainable anomaly detection framework that identifies clinically meaningful increases in symptom severity using consumer-grade wearable data. Leveraging data from 2,023 participants with defined healthy baselines, our LSTM autoencoder model learned normal health patterns of sleep duration, step count, and resting heart rate. Anomalies were flagged when self-reported depression or anxiety scores increased by >=5 points (a threshold considered clinically significant). The model achieved an adjusted F1-score of 0.80 (precision = 0.73, recall = 0.88) in detecting 393 symptom-worsening episodes across 341 participants, with higher performance observed for episodes involving concurrent depression and anxiety escalation (F1 = 0.84) and for more pronounced symptom changes (>=10-point increases, F1 = 0.85). Model interpretability was supported by SHAP-based analysis, which identified resting heart rate as the most influential feature in 71.4 percentage of detected anomalies, followed by physical activity and sleep. Together, our findings highlight the potential of explainable anomaly detection to enable personalized, scalable, and proactive mental health monitoring in real-world settings. }
}

@article{250501305v1,
  title={ Early Detection of Patient Deterioration from Real-Time Wearable   Monitoring System },
  author={ Lo Pang-Yun Ting and Hong-Pei Chen and An-Shan Liu and Chun-Yin Yeh and Po-Lin Chen and Kun-Ta Chuang },
  journal={ arXiv preprint arXiv:2505.01305v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2505.01305v1 },
  abstract={ Early detection of patient deterioration is crucial for reducing mortality rates. Heart rate data has shown promise in assessing patient health, and wearable devices offer a cost-effective solution for real-time monitoring. However, extracting meaningful insights from diverse heart rate data and handling missing values in wearable device data remain key challenges. To address these challenges, we propose TARL, an innovative approach that models the structural relationships of representative subsequences, known as shapelets, in heart rate time series. TARL creates a shapelet-transition knowledge graph to model shapelet dynamics in heart rate time series, indicating illness progression and potential future changes. We further introduce a transition-aware knowledge embedding to reinforce relationships among shapelets and quantify the impact of missing values, enabling the formulation of comprehensive heart rate representations. These representations capture explanatory structures and predict future heart rate trends, aiding early illness detection. We collaborate with physicians and nurses to gather ICU patient heart rate data from wearables and diagnostic metrics assessing illness severity for evaluating deterioration. Experiments on real-world ICU data demonstrate that TARL achieves both high reliability and early detection. A case study further showcases TARL's explainable detection process, highlighting its potential as an AI-driven tool to assist clinicians in recognizing early signs of patient deterioration. }
}

@article{250500101v1,
  title={ From Lab to Wrist: Bridging Metabolic Monitoring and Consumer Wearables   for Heart Rate and Oxygen Consumption Modeling },
  author={ Barak Gahtan and Sanketh Vedula and Gil Samuelly Leichtag and Einat Kodesh and Alex M. Bronstein },
  journal={ arXiv preprint arXiv:2505.00101v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2505.00101v1 },
  abstract={ Understanding physiological responses during running is critical for performance optimization, tailored training prescriptions, and athlete health management. We introduce a comprehensive framework -- what we believe to be the first capable of predicting instantaneous oxygen consumption (VO\$\_\{2\}\$) trajectories exclusively from consumer-grade wearable data. Our approach employs two complementary physiological models: (1) accurate modeling of heart rate (HR) dynamics via a physiologically constrained ordinary differential equation (ODE) and neural Kalman filter, trained on over 3 million HR observations, achieving 1-second interval predictions with mean absolute errors as low as 2.81\\textbackslash{},bpm (correlation 0.87); and (2) leveraging the principles of precise HR modeling, a novel VO\$\_\{2\}\$ prediction architecture requiring only the initial second of VO\$\_\{2\}\$ data for calibration, enabling robust, sequence-to-sequence metabolic demand estimation. Despite relying solely on smartwatch and chest-strap data, our method achieves mean absolute percentage errors of approximately 13\\textbackslash{}\%, effectively capturing rapid physiological transitions and steady-state conditions across diverse running intensities. Our synchronized dataset, complemented by blood lactate measurements, further lays the foundation for future noninvasive metabolic zone identification. By embedding physiological constraints within modern machine learning, this framework democratizes advanced metabolic monitoring, bridging laboratory-grade accuracy and everyday accessibility, thus empowering both elite athletes and recreational fitness enthusiasts. }
}

@article{230201974v3,
  title={ Conic Sparsity: Estimation of Regression Parameters in Closed Convex   Polyhedral Cones },
  author={ Neha Agarwala and Arkaprava Roy and Anindya Roy },
  journal={ arXiv preprint arXiv:2302.01974v3 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2302.01974v3 },
  abstract={ Statistical problems often involve linear equality and inequality constraints on model parameters. Direct estimation of parameters restricted to general polyhedral cones, particularly when one is interested in estimating low dimensional features, may be challenging. We use a dual form parameterization to characterize parameter vectors restricted to lower dimensional faces of polyhedral cones and use the characterization to define a notion of 'sparsity' on such cones. We show that the proposed notion agrees with the usual notion of sparsity in the unrestricted case and prove the validity of the proposed definition as a measure of sparsity. The identifiable parameterization of the lower dimensional faces allows a generalization of popular spike-and-slab priors to a closed convex polyhedral cone. The prior measure utilizes the geometry of the cone by defining a Markov random field over the adjacency graph of the extreme rays of the cone. We describe an efficient way of computing the posterior of the parameters in the restricted case. We illustrate the usefulness of the proposed methodology for imposing linear equality and inequality constraints by using wearables data from the National Health and Nutrition Examination Survey (NHANES) actigraph study where the daily average activity profiles of participants exhibit patterns that seem to obey such constraints. }
}

@article{250212836v2,
  title={ An LLM-Powered Agent for Physiological Data Analysis: A Case Study on   PPG-based Heart Rate Estimation },
  author={ Mohammad Feli and Iman Azimi and Pasi Liljeberg and Amir M. Rahmani },
  journal={ arXiv preprint arXiv:2502.12836v2 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.12836v2 },
  abstract={ Large language models (LLMs) are revolutionizing healthcare by improving diagnosis, patient care, and decision support through interactive communication. More recently, they have been applied to analyzing physiological time-series like wearable data for health insight extraction. Existing methods embed raw numerical sequences directly into prompts, which exceeds token limits and increases computational costs. Additionally, some studies integrated features extracted from time-series in textual prompts or applied multimodal approaches. However, these methods often produce generic and unreliable outputs due to LLMs' limited analytical rigor and inefficiency in interpreting continuous waveforms. In this paper, we develop an LLM-powered agent for physiological time-series analysis aimed to bridge the gap in integrating LLMs with well-established analytical tools. Built on the OpenCHA, an open-source LLM-powered framework, our agent powered by OpenAI's GPT-3.5-turbo model features an orchestrator that integrates user interaction, data sources, and analytical tools to generate accurate health insights. To evaluate its effectiveness, we implement a case study on heart rate (HR) estimation from Photoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram (ECG) recordings in a remote health monitoring study. The agent's performance is benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the gold standard for HR estimation. Results demonstrate that our agent significantly outperforms benchmark models by achieving lower error rates and more reliable HR estimations. The agent implementation is publicly available on GitHub. }
}

@article{250207608v3,
  title={ Time2Lang: Bridging Time-Series Foundation Models and Large Language   Models for Health Sensing Beyond Prompting },
  author={ Arvind Pillai and Dimitris Spathis and Subigya Nepal and Amanda C Collins and Daniel M Mackin and Michael V Heinz and Tess Z Griffin and Nicholas C Jacobson and Andrew Campbell },
  journal={ arXiv preprint arXiv:2502.07608v3 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.07608v3 },
  abstract={ Large language models (LLMs) show promise for health applications when combined with behavioral sensing data. Traditional approaches convert sensor data into text prompts, but this process is prone to errors, computationally expensive, and requires domain expertise. These challenges are particularly acute when processing extended time series data. While time series foundation models (TFMs) have recently emerged as powerful tools for learning representations from temporal data, bridging TFMs and LLMs remains challenging. Here, we present Time2Lang, a framework that directly maps TFM outputs to LLM representations without intermediate text conversion. Our approach first trains on synthetic data using periodicity prediction as a pretext task, followed by evaluation on mental health classification tasks. We validate Time2Lang on two longitudinal wearable and mobile sensing datasets: daily depression prediction using step count data (17,251 days from 256 participants) and flourishing classification based on conversation duration (46 participants over 10 weeks). Time2Lang maintains near constant inference times regardless of input length, unlike traditional prompting methods. The generated embeddings preserve essential time-series characteristics such as auto-correlation. Our results demonstrate that TFMs and LLMs can be effectively integrated while minimizing information loss and enabling performance transfer across these distinct modeling paradigms. To our knowledge, we are the first to integrate a TFM and an LLM for health, thus establishing a foundation for future research combining general-purpose large models for complex healthcare tasks. }
}

@article{250402735v2,
  title={ Reliable Physiological Monitoring on the Wrist Using Generative Deep   Learning to Address Poor Skin-Sensor Contact },
  author={ Manh Pham Hung and Matthew Yiwen Ho and Yiming Zhang and Dimitris Spathis and Aaqib Saeed and Dong Ma },
  journal={ arXiv preprint arXiv:2504.02735v2 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2504.02735v2 },
  abstract={ Photoplethysmography (PPG) is a widely adopted, non-invasive technique for monitoring cardiovascular health and physiological parameters in both consumer and clinical settings. While motion artifacts in dynamic environments have been extensively studied, suboptimal skin-sensor contact in sedentary conditions - a critical yet underexplored issue - can distort PPG waveform morphology, leading to the loss or misalignment of key features and compromising sensing accuracy. In this work, we propose CP-PPG, a novel framework that transforms Contact Pressure-distorted PPG signals into high-fidelity waveforms with ideal morphology. CP-PPG integrates a custom data collection protocol, a carefully designed signal processing pipeline, and a novel deep adversarial model trained with a custom PPG-aware loss function. We validated CP-PPG through comprehensive evaluations, including 1) morphology transformation performance on our self-collected dataset, 2) downstream physiological monitoring performance on public datasets, and 3) in-the-wild study. Extensive experiments demonstrate substantial and consistent improvements in signal fidelity (Mean Absolute Error: 0.09, 40\% improvement over the original signal) as well as downstream performance across all evaluations in Heart Rate (HR), Heart Rate Variability (HRV), Respiration Rate (RR), and Blood Pressure (BP) estimation (on average, 21\% improvement in HR; 41-46\% in HRV; 6\% in RR; and 4-5\% in BP). These findings highlight the critical importance of addressing skin-sensor contact issues to enhance the reliability and effectiveness of PPG-based physiological monitoring. CP-PPG thus holds significant potential to improve the accuracy of wearable health technologies in clinical and consumer applications. }
}

@article{240506061v2,
  title={ GPTCoach: Towards LLM-Based Physical Activity Coaching },
  author={ Matthew Jörke and Shardul Sapkota and Lyndsea Warkenthien and Niklas Vainio and Paul Schmiedmayer and Emma Brunskill and James A. Landay },
  journal={ arXiv preprint arXiv:2405.06061v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2405.06061v2 },
  abstract={ Mobile health applications show promise for scalable physical activity promotion but are often insufficiently personalized. In contrast, health coaching offers highly personalized support but can be prohibitively expensive and inaccessible. This study draws inspiration from health coaching to explore how large language models (LLMs) might address personalization challenges in mobile health. We conduct formative interviews with 12 health professionals and 10 potential coaching recipients to develop design principles for an LLM-based health coach. We then built GPTCoach, a chatbot that implements the onboarding conversation from an evidence-based coaching program, uses conversational strategies from motivational interviewing, and incorporates wearable data to create personalized physical activity plans. In a lab study with 16 participants using three months of historical data, we find promising evidence that GPTCoach gathers rich qualitative information to offer personalized support, with users feeling comfortable sharing concerns. We conclude with implications for future research on LLM-based physical activity support. }
}

@article{250316091v1,
  title={ AIMI: Leveraging Future Knowledge and Personalization in Sparse Event   Forecasting for Treatment Adherence },
  author={ Abdullah Mamun and Diane J. Cook and Hassan Ghasemzadeh },
  journal={ arXiv preprint arXiv:2503.16091v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.16091v1 },
  abstract={ Adherence to prescribed treatments is crucial for individuals with chronic conditions to avoid costly or adverse health outcomes. For certain patient groups, intensive lifestyle interventions are vital for enhancing medication adherence. Accurate forecasting of treatment adherence can open pathways to developing an on-demand intervention tool, enabling timely and personalized support. With the increasing popularity of smartphones and wearables, it is now easier than ever to develop and deploy smart activity monitoring systems. However, effective forecasting systems for treatment adherence based on wearable sensors are still not widely available. We close this gap by proposing Adherence Forecasting and Intervention with Machine Intelligence (AIMI). AIMI is a knowledge-guided adherence forecasting system that leverages smartphone sensors and previous medication history to estimate the likelihood of forgetting to take a prescribed medication. A user study was conducted with 27 participants who took daily medications to manage their cardiovascular diseases. We designed and developed CNN and LSTM-based forecasting models with various combinations of input features and found that LSTM models can forecast medication adherence with an accuracy of 0.932 and an F-1 score of 0.936. Moreover, through a series of ablation studies involving convolutional and recurrent neural network architectures, we demonstrate that leveraging known knowledge about future and personalized training enhances the accuracy of medication adherence forecasting. Code available: https://github.com/ab9mamun/AIMI. }
}

@article{250212990v3,
  title={ Artificial Intelligence-derived Vascular Age from Photoplethysmography:   A Novel Digital Biomarker for Cardiovascular Health },
  author={ Guangkun Nie and Qinghao Zhao and Gongzheng Tang and Yaxin Li and Shenda Hong },
  journal={ arXiv preprint arXiv:2502.12990v3 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.12990v3 },
  abstract={ With the increasing availability of wearable devices, photoplethysmography (PPG) has emerged as a promising non-invasive tool for monitoring human hemodynamics. We propose a deep learning framework to estimate vascular age (AI-vascular age) from PPG signals, incorporating a distribution-aware loss to address biases caused by imbalanced data. The model was developed using data from the UK Biobank (UKB), with 98,672 participants in the development cohort and 113,559 participants (144,683 data pairs) for clinical evaluation. After adjusting for key confounders, individuals with a vascular age gap (AI-vascular age minus calendar age) exceeding 9 years had a significantly higher risk of major adverse cardiovascular and cerebrovascular events (MACCE) (HR = 2.37, p < 0.005) and secondary outcomes, including diabetes (HR = 2.69, p < 0.005), hypertension (HR = 2.88, p < 0.005), coronary heart disease (HR = 2.20, p < 0.005), heart failure (HR = 2.15, p < 0.005), myocardial infarction (HR = 2.51, p < 0.005), stroke (HR = 2.55, p < 0.005), and all-cause mortality (HR = 2.51, p < 0.005). Conversely, participants with a vascular age gap below -9 years exhibited a significantly lower incidence of these outcomes. We further evaluated the longitudinal applicability of AI-vascular age using serial PPG data from the UKB, demonstrating its value in risk stratification by leveraging AI-vascular age at two distinct time points to predict future MACCE incidence. External validation was performed on a MIMIC-III-derived cohort (n = 2,343), where each one-year increase in vascular age gap was significantly associated with elevated in-hospital mortality risk (OR = 1.02, p < 0.005). In conclusion, our study establishes AI-vascular age as a novel, non-invasive digital biomarker for cardiovascular health assessment. }
}

@article{250315821v1,
  title={ Temporal Point Process Modeling of Aggressive Behavior Onset in   Psychiatric Inpatient Youths with Autism },
  author={ Michael Potter and Michael Everett and Ashutosh Singh and Georgios Stratis and Yuna Watanabe and Ahmet Demirkaya and Deniz Erdogmus and Tales Imbiriba and Matthew S. Goodwin },
  journal={ arXiv preprint arXiv:2503.15821v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.15821v1 },
  abstract={ Aggressive behavior, including aggression towards others and self-injury, occurs in up to 80\% of children and adolescents with autism, making it a leading cause of behavioral health referrals and a major driver of healthcare costs. Predicting when autistic youth will exhibit aggression is challenging due to their communication difficulties. Many are minimally verbal or have poor emotional insight. Recent advances in Machine Learning and wearable biosensing enable short-term aggression predictions within a limited future window (typically one to three minutes). However, existing models do not estimate aggression probability within longer future windows nor the expected number of aggression onsets over such a period. To address these limitations, we employ Temporal Point Processes (TPPs) to model the generative process of aggressive behavior onsets in inpatient youths with autism. We hypothesize that aggressive behavior onsets follow a self-exciting process driven by short-term history, making them well-suited for Hawkes Point Process modeling. We establish a benchmark and demonstrate through Goodness-of-Fit statistics and predictive metrics that TPPs perform well modeling aggressive behavior onsets in inpatient youths with autism. Additionally, we gain insights into the onset generative process, like the branching factor near criticality, and suggest TPPs may enhance future clinical decision-making and preemptive interventions. }
}

@article{250315637v1,
  title={ Understanding State Social Anxiety in Virtual Social Interactions using   Multimodal Wearable Sensing Indicators },
  author={ Maria A. Larrazabal and Zhiyuan Wang and Mark Rucker and Emma R. Toner and Mehdi Boukhechba and Bethany A. Teachman and Laura E. Barnes },
  journal={ arXiv preprint arXiv:2503.15637v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.15637v1 },
  abstract={ Mobile sensing is ubiquitous and offers opportunities to gain insight into state mental health functioning. Detecting state elevations in social anxiety would be especially useful given this phenomenon is highly prevalent and impairing, but often not disclosed. Although anxiety is highly dynamic, fluctuating rapidly over the course of minutes, most work to date has examined anxiety at a scale of hours, days, or longer. In the present work, we explore the feasibility of detecting fluctuations in state social anxiety among N = 46 undergraduate students with elevated symptoms of trait social anxiety. Participants engaged in two dyadic and two group social interactions via Zoom. We evaluated participants' state anxiety levels as they anticipated, immediately after experiencing, and upon reflecting on each social interaction, spanning a time frame of 2-6 minutes. We collected biobehavioral features (i.e., PPG, EDA, skin temperature, and accelerometer) via Empatica E4 devices as they participated in the varied social contexts (e.g., dyadic vs. group; anticipating vs. experiencing the interaction; experiencing varying levels of social evaluation). We additionally measured their trait mental health functioning. Mixed-effect logistic regression and leave-one-subject-out machine learning modeling indicated biobehavioral features significantly predict state fluctuations in anxiety, though balanced accuracy tended to be modest (59\%). However, our capacity to identify instances of heightened versus low state anxiety significantly increased (with balanced accuracy ranging from 69\% to 84\% across different operationalizations of state anxiety) when we integrated contextual data alongside trait mental health functioning into our predictive models.. We discuss these and other findings in the context of the broader anxiety detection literature. }
}

@article{250315221v1,
  title={ A Foundation Model for Patient Behavior Monitoring and Suicide Detection },
  author={ Rodrigo Oliver and Josué Pérez-Sabater and Leire Paz-Arbaizar and Alejandro Lancho and Antonio Artés and Pablo M. Olmos },
  journal={ arXiv preprint arXiv:2503.15221v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.15221v1 },
  abstract={ Foundation models (FMs) have achieved remarkable success across various domains, yet their adoption in healthcare remains limited. While significant advances have been made in medical imaging, genetic biomarkers, and time series from electronic health records, the potential of FMs for patient behavior monitoring through wearable devices remains underexplored. These datasets are inherently heterogeneous, multisource, and often exhibit high rates of missing data, posing unique challenges. This paper introduces a novel FM based on a modified vector quantized variational autoencoder (VQ-VAE), specifically designed to process real-world data from wearable devices. We demonstrate that our pretrained FM, trained on a broad cohort of psychiatric patients, performs downstream tasks via its latent representation without fine-tuning on a held-out cohort of suicidal patients. To illustrate this, we develop a probabilistic change-point detection algorithm for suicide detection and demonstrate the FM's effectiveness in predicting emotional states. Our results show that the discrete latent structure of the VQ-VAE outperforms a state-of-the-art Informer architecture in unsupervised suicide detection, while matching its performance in supervised emotion prediction when the latent dimensionality is increased, though at the cost of reduced unsupervised accuracy. This trade-off highlights the need for future FMs to integrate hybrid discrete-continuous structures for balanced performance across tasks. }
}

@article{250313425v1,
  title={ Movement Sequencing: A Novel Approach to Quantifying the Building Blocks   of Human Gait },
  author={ Alexandra Hammerberg and Samuel Grunblatt and Patricia Kramer },
  journal={ arXiv preprint arXiv:2503.13425v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.13425v1 },
  abstract={ By 2050, a quarter of the US population will be over the age of 65 with greater than a 40\% risk of developing life-altering neuromusculoskeletal pathologies. The potential of wearables, such as Apple AirPods and hearing aids, to provide personalized preventative and predictive health monitoring outside of the clinic is nascent, but large quantities of open-ended data that capture movement in the physical world now exist. Algorithms that leverage existing wearable technology to detect subtle changes to walking mechanics, an early indicator of neuromusculoskeletal pathology, have successfully been developed to determine population-level statistics, but individual-level variability is more difficult to parse from population-level data. Like genetic sequencing, the individual's gait pattern can be discerned by decomposing the movement signal into its fundamental features from which we can detect ''mutations'' or changes to the pattern that are early indicators of pathology - movement-based biomarkers. We have developed a novel approach to quantify ''normal baseline movement'' at an individual level, combining methods from gait laboratories with methods used to characterize stellar oscillations. We tested our approach by asking participants to complete an outdoor circuit while wearing a pair of AirPods, using orthopaedic braces to simulate pathology. We found that the novel features we propose are sensitive enough to distinguish between normal walking and brace walking at the population level and at the individual level in all sensor directions (both p \$<\$ 0.05). We also perform principal component analysis on our population-level and individual-level models, and find significant differences between individuals as well as between the overall population model and most individuals. We also demonstrate the potential of these gait features in deep learning applications. }
}

@article{250312657v1,
  title={ TEANet: A Transpose-Enhanced Autoencoder Network for Wearable Stress   Monitoring },
  author={ Md Santo Ali and Sapnil Sarker Bipro and Mohammod Abdul Motin and Sumaiya Kabir and Manish Sharma and M. E. H. Chowdhury },
  journal={ arXiv preprint arXiv:2503.12657v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.12657v1 },
  abstract={ Mental stress poses a significant public health concern due to its detrimental effects on physical and mental well-being, necessitating the development of continuous stress monitoring tools for wearable devices. Blood volume pulse (BVP) sensors, readily available in many smartwatches, offer a convenient and cost-effective solution for stress monitoring. This study proposes a deep learning approach, a Transpose-Enhanced Autoencoder Network (TEANet), for stress detection using BVP signals. The proposed TEANet model was trained and validated utilizing a self-collected RUET SPML dataset, comprising 19 healthy subjects, and the publicly available wearable stress and affect detection (WESAD) dataset, comprising 15 healthy subjects. It achieves the highest accuracy of 92.51\% and 96.94\%, F1 scores of 95.03\% and 95.95\%, and kappa of 0.7915 and 0.9350 for RUET SPML, and WESAD datasets respectively. The proposed TEANet effectively detects mental stress through BVP signals with high accuracy, making it a promising tool for continuous stress monitoring. Furthermore, the proposed model effectively addresses class imbalances and demonstrates high accuracy, underscoring its potential for reliable real-time stress monitoring using wearable devices. }
}

@article{250311466v1,
  title={ In Shift and In Variance: Assessing the Robustness of HAR Deep Learning   Models against Variability },
  author={ Azhar Ali Khaked and Nobuyuki Oishi and Daniel Roggen and Paula Lago },
  journal={ arXiv preprint arXiv:2503.11466v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.11466v1 },
  abstract={ Human Activity Recognition (HAR) using wearable inertial measurement unit (IMU) sensors can revolutionize healthcare by enabling continual health monitoring, disease prediction, and routine recognition. Despite the high accuracy of Deep Learning (DL) HAR models, their robustness to real-world variabilities remains untested, as they have primarily been trained and tested on limited lab-confined data. In this study, we isolate subject, device, position, and orientation variability to determine their effect on DL HAR models and assess the robustness of these models in real-world conditions. We evaluated the DL HAR models using the HARVAR and REALDISP datasets, providing a comprehensive discussion on the impact of variability on data distribution shifts and changes in model performance. Our experiments measured shifts in data distribution using Maximum Mean Discrepancy (MMD) and observed DL model performance drops due to variability. We concur that studied variabilities affect DL HAR models differently, and there is an inverse relationship between data distribution shifts and model performance. The compounding effect of variability was analyzed, and the implications of variabilities in real-world scenarios were highlighted. MMD proved an effective metric for calculating data distribution shifts and explained the drop in performance due to variabilities in HARVAR and REALDISP datasets. Combining our understanding of variability with evaluating its effects will facilitate the development of more robust DL HAR models and optimal training techniques. Allowing Future models to not only be assessed based on their maximum F1 score but also on their ability to generalize effectively }
}

@article{250309661v1,
  title={ Towards Hardware Supported Domain Generalization in DNN-Based Edge   Computing Devices for Health Monitoring },
  author={ Johnson Loh and Lyubov Dudchenko and Justus Viga and Tobias Gemmeke },
  journal={ arXiv preprint arXiv:2503.09661v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.09661v1 },
  abstract={ Deep neural network (DNN) models have shown remarkable success in many real-world scenarios, such as object detection and classification. Unfortunately, these models are not yet widely adopted in health monitoring due to exceptionally high requirements for model robustness and deployment in highly resource-constrained devices. In particular, the acquisition of biosignals, such as electrocardiogram (ECG), is subject to large variations between training and deployment, necessitating domain generalization (DG) for robust classification quality across sensors and patients. The continuous monitoring of ECG also requires the execution of DNN models in convenient wearable devices, which is achieved by specialized ECG accelerators with small form factor and ultra-low power consumption. However, combining DG capabilities with ECG accelerators remains a challenge. This article provides a comprehensive overview of ECG accelerators and DG methods and discusses the implication of the combination of both domains, such that multi-domain ECG monitoring is enabled with emerging algorithm-hardware co-optimized systems. Within this context, an approach based on correction layers is proposed to deploy DG capabilities on the edge. Here, the DNN fine-tuning for unknown domains is limited to a single layer, while the remaining DNN model remains unmodified. Thus, computational complexity (CC) for DG is reduced with minimal memory overhead compared to conventional fine-tuning of the whole DNN model. The DNN model-dependent CC is reduced by more than 2.5x compared to DNN fine-tuning at an average increase of F1 score by more than 20\% on the generalized target domain. In summary, this article provides a novel perspective on robust DNN classification on the edge for health monitoring applications. }
}

@article{250307008v1,
  title={ SDFA: Structure Aware Discriminative Feature Aggregation for Efficient   Human Fall Detection in Video },
  author={ Sania Zahan and Ghulam Mubashar Hassan and Ajmal Mian },
  journal={ arXiv preprint arXiv:2503.07008v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.07008v1 },
  abstract={ Older people are susceptible to fall due to instability in posture and deteriorating health. Immediate access to medical support can greatly reduce repercussions. Hence, there is an increasing interest in automated fall detection, often incorporated into a smart healthcare system to provide better monitoring. Existing systems focus on wearable devices which are inconvenient or video monitoring which has privacy concerns. Moreover, these systems provide a limited perspective of their generalization ability as they are tested on datasets containing few activities that have wide disparity in the action space and are easy to differentiate. Complex daily life scenarios pose much greater challenges with activities that overlap in action spaces due to similar posture or motion. To overcome these limitations, we propose a fall detection model, coined SDFA, based on human skeletons extracted from low-resolution videos. The use of skeleton data ensures privacy and low-resolution videos ensures low hardware and computational cost. Our model captures discriminative structural displacements and motion trends using unified joint and motion features projected onto a shared high dimensional space. Particularly, the use of separable convolution combined with a powerful GCN architecture provides improved performance. Extensive experiments on five large-scale datasets with a wide range of evaluation settings show that our model achieves competitive performance with extremely low computational complexity and runs faster than existing models. }
}

@article{241215947v2,
  title={ Mamba-based Deep Learning Approaches for Sleep Staging on a Wireless   Multimodal Wearable System without Electroencephalography },
  author={ Andrew H. Zhang and Alex He-Mo and Richard Fei Yin and Chunlin Li and Yuzhi Tang and Dharmendra Gurve and Veronique van der Horst and Aron S. Buchman and Nasim Montazeri Ghahjaverestan and Maged Goubran and Bo Wang and Andrew S. P. Lim },
  journal={ arXiv preprint arXiv:2412.15947v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.15947v2 },
  abstract={ Study Objectives: We investigate Mamba-based deep learning approaches for sleep staging on signals from ANNE One (Sibel Health, Evanston, IL), a non-intrusive dual-module wireless wearable system measuring chest electrocardiography (ECG), accelerometry, and temperature, and finger photoplethysmography (PPG) and temperature.   Methods: We obtained wearable sensor recordings from 360 adults undergoing concurrent polysomnography (PSG) at a tertiary care sleep lab. Each PSG recording was manually scored and these annotations served as ground truth labels for training and evaluation of our models. PSG and wearable sensor data were automatically aligned using their ECG channels with manual confirmation by visual inspection. We trained Mamba-based models with convolutional-recurrent neural network (CRNN) and the recurrent neural network (RNN) architectures on these recordings. Ensembling of model variants with similar architectures was performed.   Results: Our best approach, after ensembling, attains a 3-class (wake, non rapid eye movement [NREM] sleep, rapid eye movement [REM] sleep) balanced accuracy of 84.02\%, F1 score of 84.23\%, Cohen's \$\\textbackslash{}kappa\$ of 72.89\%, and a Matthews correlation coefficient (MCC) score of 73.00\%; a 4-class (wake, NREM stage 1/2 [N1/N2], NREM stage 3 [N3], REM) balanced accuracy of 75.30\%, F1 score of 74.10\%, Cohen's \$\\textbackslash{}kappa\$ of 61.51\%, and MCC score of 61.95\%; a 5-class (wake, N1, N2, N3, REM) balanced accuracy of 65.11\%, F1 score of 66.15\%, Cohen's \$\\textbackslash{}kappa\$ of 53.23\%, MCC score of 54.38\%.   Conclusions: Deep learning models can infer major sleep stages from the ANNE One, a wearable system without electroencephalography (EEG), and can be successfully applied to data from adults attending a tertiary care sleep clinic. }
}

@article{250303803v1,
  title={ EgoLife: Towards Egocentric Life Assistant },
  author={ Jingkang Yang and Shuai Liu and Hongming Guo and Yuhao Dong and Xiamengwei Zhang and Sicheng Zhang and Pengyun Wang and Zitang Zhou and Binzhu Xie and Ziyue Wang and Bei Ouyang and Zhengyu Lin and Marco Cominelli and Zhongang Cai and Yuanhan Zhang and Peiyuan Zhang and Fangzhou Hong and Joerg Widmer and Francesco Gringoli and Lei Yang and Bo Li and Ziwei Liu },
  journal={ arXiv preprint arXiv:2503.03803v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.03803v1 },
  abstract={ We introduce EgoLife, a project to develop an egocentric life assistant that accompanies and enhances personal efficiency through AI-powered wearable glasses. To lay the foundation for this assistant, we conducted a comprehensive data collection study where six participants lived together for one week, continuously recording their daily activities - including discussions, shopping, cooking, socializing, and entertainment - using AI glasses for multimodal egocentric video capture, along with synchronized third-person-view video references. This effort resulted in the EgoLife Dataset, a comprehensive 300-hour egocentric, interpersonal, multiview, and multimodal daily life dataset with intensive annotation. Leveraging this dataset, we introduce EgoLifeQA, a suite of long-context, life-oriented question-answering tasks designed to provide meaningful assistance in daily life by addressing practical questions such as recalling past relevant events, monitoring health habits, and offering personalized recommendations. To address the key technical challenges of (1) developing robust visual-audio models for egocentric data, (2) enabling identity recognition, and (3) facilitating long-context question answering over extensive temporal information, we introduce EgoButler, an integrated system comprising EgoGPT and EgoRAG. EgoGPT is an omni-modal model trained on egocentric datasets, achieving state-of-the-art performance on egocentric video understanding. EgoRAG is a retrieval-based component that supports answering ultra-long-context questions. Our experimental studies verify their working mechanisms and reveal critical factors and bottlenecks, guiding future improvements. By releasing our datasets, models, and benchmarks, we aim to stimulate further research in egocentric AI assistants. }
}

@article{250311683v1,
  title={ MealMeter: Using Multimodal Sensing and Machine Learning for   Automatically Estimating Nutrition Intake },
  author={ Asiful Arefeen and Samantha Fessler and Sayyed Mostafa Mostafavi and Carol S Johnston and Hassan Ghasemzadeh },
  journal={ arXiv preprint arXiv:2503.11683v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.11683v1 },
  abstract={ Accurate estimation of meal macronutrient composition is a pre-perquisite for precision nutrition, metabolic health monitoring, and glycemic management. Traditional dietary assessment methods, such as self-reported food logs or diet recalls are time-intensive and prone to inaccuracies and biases. Several existing AI-driven frameworks are data intensive. In this study, we propose MealMeter, a machine learning driven method that leverages multimodal sensor data of wearable and mobile devices. Data are collected from 12 participants to estimate macronutrient intake. Our approach integrates physiological signals (e.g., continuous glucose, heart rate variability), inertial motion data, and environmental cues to model the relationship between meal intake and metabolic responses. Using lightweight machine learning models trained on a diverse dataset of labeled meal events, MealMeter predicts the composition of carbohydrates, proteins, and fats with high accuracy. Our results demonstrate that multimodal sensing combined with machine learning significantly improves meal macronutrient estimation compared to the baselines including foundation model and achieves average mean absolute errors (MAE) and average root mean squared relative errors (RMSRE) as low as 13.2 grams and 0.37, respectively, for carbohydrates. Therefore, our developed system has the potential to automate meal tracking, enhance dietary interventions, and support personalized nutrition strategies for individuals managing metabolic disorders such as diabetes and obesity. }
}

@article{250301918v1,
  title={ Multi-models with averaging in feature domain for non-invasive blood   glucose estimation },
  author={ Yiting Wei and Bingo Wing-Kuen Ling and Qing Liu and Jiaxin Liu },
  journal={ arXiv preprint arXiv:2503.01918v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.01918v1 },
  abstract={ Diabetes is a serious chronic metabolic disease. In the recent years, more and more consumer technology enterprises focusing on human health are committed to implementing accurate and non-invasive blood glucose algorithm in their products. However, due to the interference from the external environment, these wearable non-invasive methods yield the low estimation accuracy. To address this issue, this paper employs different models based on different ranges of the blood glucose values for performing the blood glucose estimation. First the photoplethysmograms (PPGs) are acquired and they are denoised via the bit plane singular spectrum analysis (SSA) method. Second, the features are extracted. For the data in the training set, first the features are averaged across the measurements in the feature domain via the optimization approach. Second, the random forest is employed to sort the importance of each feature. Third, the training set is divided into three subsets according to the reference blood glucose values. Fourth, the feature vectors and the corresponding blood glucose values in the same group are employed to build an individual model. Fifth, for each feature, the average of the feature values for all the measurements in the same subset is computed. For the data in the test set, first, the sum of the weighted distances between the test feature values and the average values obtained in the above is computed for each model. Here, the weights are defined based on the importance sorted by the random forest obtained in the above. The model corresponding to the smallest sum is assigned. Finally, the blood glucose value is estimated based on the corresponding model. Compared to the state of arts methods, our proposed method can effectively improve the estimation accuracy. }
}

@article{250221162v1,
  title={ Parallel-Learning of Invariant and Tempo-variant Attributes of   Single-Lead Cardiac Signals: PLITA },
  author={ Adtian Atienza and Jakob E. Bardram and Sadasivan Puthusserypady },
  journal={ arXiv preprint arXiv:2502.21162v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.21162v1 },
  abstract={ Wearable sensing devices, such as Holter monitors, will play a crucial role in the future of digital health. Unsupervised learning frameworks such as Self-Supervised Learning (SSL) are essential to map these single-lead electrocardiogram (ECG) signals with their anticipated clinical outcomes. These signals are characterized by a tempo-variant component whose patterns evolve through the recording and an invariant component with patterns that remain unchanged. However, existing SSL methods only drive the model to encode the invariant attributes, leading the model to neglect tempo-variant information which reflects subject-state changes through time. In this paper, we present Parallel-Learning of Invariant and Tempo-variant Attributes (PLITA), a novel SSL method designed for capturing both invariant and tempo-variant ECG attributes. The latter are captured by mandating closer representations in space for closer inputs on time. We evaluate both the capability of the method to learn the attributes of these two distinct kinds, as well as PLITA's performance compared to existing SSL methods for ECG analysis. PLITA performs significantly better in the set-ups where tempo-variant attributes play a major role. }
}

@article{250221127v1,
  title={ CuPID: Leveraging Masked Single-Lead ECG Modelling for Enhancing the   Representations },
  author={ Adtian Atienza and Gouthamaan Manimaran and Jakob E. Bardram and Sadasivan Puthusserypady },
  journal={ arXiv preprint arXiv:2502.21127v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.21127v1 },
  abstract={ Wearable sensing devices, such as Electrocardiogram (ECG) heart-rate monitors, will play a crucial role in the future of digital health. This continuous monitoring leads to massive unlabeled data, incentivizing the development of unsupervised learning frameworks. While Masked Data Modelling (MDM) techniques have enjoyed wide use, their direct application to single-lead ECG data is suboptimal due to the decoder's difficulty handling irregular heartbeat intervals when no contextual information is provided. In this paper, we present Cueing the Predictor Increments the Detailing (CuPID), a novel MDM method tailored to single-lead ECGs. CuPID enhances existing MDM techniques by cueing spectrogram-derived context to the decoder, thus incentivizing the encoder to produce more detailed representations. This has a significant impact on the encoder's performance across a wide range of different configurations, leading CuPID to outperform state-of-the-art methods in a variety of downstream tasks. }
}

@article{241014879v2,
  title={ Vital Insight: Assisting Experts' Context-Driven Sensemaking of   Multi-modal Personal Tracking Data Using Visualization and Human-In-The-Loop   LLM Agents },
  author={ Jiachen Li and Xiwen Li and Justin Steinberg and Akshat Choube and Bingsheng Yao and Xuhai Xu and Dakuo Wang and Elizabeth Mynatt and Varun Mishra },
  journal={ arXiv preprint arXiv:2410.14879v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.14879v2 },
  abstract={ Passive tracking methods, such as phone and wearable sensing, have become dominant in monitoring human behaviors in modern ubiquitous computing studies. While there have been significant advances in machine-learning approaches to translate periods of raw sensor data to model momentary behaviors, (e.g., physical activity recognition), there still remains a significant gap in the translation of these sensing streams into meaningful, high-level, context-aware insights that are required for various applications (e.g., summarizing an individual's daily routine). To bridge this gap, experts often need to employ a context-driven sensemaking process in real-world studies to derive insights. This process often requires manual effort and can be challenging even for experienced researchers due to the complexity of human behaviors.   We conducted three rounds of user studies with 21 experts to explore solutions to address challenges with sensemaking. We follow a human-centered design process to identify needs and design, iterate, build, and evaluate Vital Insight (VI), a novel, LLM-assisted, prototype system to enable human-in-the-loop inference (sensemaking) and visualizations of multi-modal passive sensing data from smartphones and wearables. Using the prototype as a technology probe, we observe experts' interactions with it and develop an expert sensemaking model that explains how experts move between direct data representations and AI-supported inferences to explore, question, and validate insights. Through this iterative process, we also synthesize and discuss a list of design implications for the design of future AI-augmented visualization systems to better assist experts' sensemaking processes in multi-modal health sensing data. }
}

@article{250218733v1,
  title={ Cross-Modality Investigation on WESAD Stress Classification },
  author={ Eric Oliver and Sagnik Dakshit },
  journal={ arXiv preprint arXiv:2502.18733v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.18733v1 },
  abstract={ Deep learning's growing prevalence has driven its widespread use in healthcare, where AI and sensor advancements enhance diagnosis, treatment, and monitoring. In mobile health, AI-powered tools enable early diagnosis and continuous monitoring of conditions like stress. Wearable technologies and multimodal physiological data have made stress detection increasingly viable, but model efficacy depends on data quality, quantity, and modality. This study develops transformer models for stress detection using the WESAD dataset, training on electrocardiograms (ECG), electrodermal activity (EDA), electromyography (EMG), respiration rate (RESP), temperature (TEMP), and 3-axis accelerometer (ACC) signals. The results demonstrate the effectiveness of single-modality transformers in analyzing physiological signals, achieving state-of-the-art performance with accuracy, precision and recall values in the range of \$99.73\\textbackslash{}\%\$ to \$99.95\\textbackslash{}\%\$ for stress detection. Furthermore, this study explores cross-modal performance and also explains the same using 2D visualization of the learned embedding space and quantitative analysis based on data variance. Despite the large body of work on stress detection and monitoring, the robustness and generalization of these models across different modalities has not been explored. This research represents one of the initial efforts to interpret embedding spaces for stress detection, providing valuable information on cross-modal performance. }
}

@article{250316455v1,
  title={ Bridging Structural Dynamics and Biomechanics: Human Motion Estimation   through Footstep-Induced Floor Vibrations },
  author={ Yiwen Dong and Jessica Rose and Hae Young Noh },
  journal={ arXiv preprint arXiv:2503.16455v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.16455v1 },
  abstract={ Quantitative estimation of human joint motion in daily living spaces is essential for early detection and rehabilitation tracking of neuromusculoskeletal disorders (e.g., Parkinson's) and mitigating trip and fall risks for older adults. Existing approaches involve monitoring devices such as cameras, wearables, and pressure mats, but have operational constraints such as direct line-of-sight, carrying devices, and dense deployment. To overcome these limitations, we leverage gait-induced floor vibration to estimate lower-limb joint motion (e.g., ankle, knee, and hip flexion angles), allowing non-intrusive and contactless gait health monitoring in people's living spaces. To overcome the high uncertainty in lower-limb movement given the limited information provided by the gait-induced floor vibrations, we formulate a physics-informed graph to integrate domain knowledge of gait biomechanics and structural dynamics into the model. Specifically, different types of nodes represent heterogeneous information from joint motions and floor vibrations; Their connecting edges represent the physiological relationships between joints and forces governed by gait biomechanics, as well as the relationships between forces and floor responses governed by the structural dynamics. As a result, our model poses physical constraints to reduce uncertainty while allowing information sharing between the body and the floor to make more accurate predictions. We evaluate our approach with 20 participants through a real-world walking experiment. We achieved an average of 3.7 degrees of mean absolute error in estimating 12 joint flexion angles (38\% error reduction from baseline), which is comparable to the performance of cameras and wearables in current medical practices. }
}

@article{250214522v1,
  title={ Investigating the Generalizability of ECG Noise Detection Across Diverse   Data Sources and Noise Types },
  author={ Sharmad Kalpande and Nilesh Kumar Sahu and Haroon Lone },
  journal={ arXiv preprint arXiv:2502.14522v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.14522v1 },
  abstract={ Electrocardiograms (ECGs) are essential for monitoring cardiac health, allowing clinicians to analyze heart rate variability (HRV), detect abnormal rhythms, and diagnose cardiovascular diseases. However, ECG signals, especially those from wearable devices, are often affected by noise artifacts caused by motion, muscle activity, or device-related interference. These artifacts distort R-peaks and the characteristic QRS complex, making HRV analysis unreliable and increasing the risk of misdiagnosis.   Despite this, the few existing studies on ECG noise detection have primarily focused on a single dataset, limiting the understanding of how well noise detection models generalize across different datasets. In this paper, we investigate the generalizability of noise detection in ECG using a novel HRV-based approach through cross-dataset experiments on four datasets. Our results show that machine learning achieves an average accuracy of over 90\\textbackslash{}\% and an AUPRC of more than 0.9. These findings suggest that regardless of the ECG data source or the type of noise, the proposed method maintains high accuracy even on unseen datasets, demonstrating the feasibility of generalizability. }
}

@article{250213920v1,
  title={ Exploring Personalized Health Support through Data-Driven, Theory-Guided   LLMs: A Case Study in Sleep Health },
  author={ Xingbo Wang and Janessa Griffith and Daniel A. Adler and Joey Castillo and Tanzeem Choudhury and Fei Wang },
  journal={ arXiv preprint arXiv:2502.13920v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.13920v1 },
  abstract={ Despite the prevalence of sleep-tracking devices, many individuals struggle to translate data into actionable improvements in sleep health. Current methods often provide data-driven suggestions but may not be feasible and adaptive to real-life constraints and individual contexts. We present HealthGuru, a novel large language model-powered chatbot to enhance sleep health through data-driven, theory-guided, and adaptive recommendations with conversational behavior change support. HealthGuru's multi-agent framework integrates wearable device data, contextual information, and a contextual multi-armed bandit model to suggest tailored sleep-enhancing activities. The system facilitates natural conversations while incorporating data-driven insights and theoretical behavior change techniques. Our eight-week in-the-wild deployment study with 16 participants compared HealthGuru to a baseline chatbot. Results show improved metrics like sleep duration and activity scores, higher quality responses, and increased user motivation for behavior change with HealthGuru. We also identify challenges and design considerations for personalization and user engagement in health chatbots. }
}

@article{230705333v2,
  title={ Wearable-based Fair and Accurate Pain Assessment Using Multi-Attribute   Fairness Loss in Convolutional Neural Networks },
  author={ Yidong Zhu and Shao-Hsien Liu and Mohammad Arif Ul Alam },
  journal={ arXiv preprint arXiv:2307.05333v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2307.05333v2 },
  abstract={ The integration of diverse health data, such as IoT (Internet of Things), EHR (Electronic Health Record), and clinical surveys, with scalable AI(Artificial Intelligence) has enabled the identification of physical, behavioral, and psycho-social indicators of pain. However, the adoption of AI in clinical pain evaluation is hindered by challenges like personalization and fairness. Many AI models, including machine and deep learning, exhibit biases, discriminating against specific groups based on gender or ethnicity, causing skepticism among medical professionals about their reliability. This paper proposes a Multi-attribute Fairness Loss (MAFL) based Convolutional Neural Network (CNN) model designed to account for protected attributes in data, ensuring fair pain status predictions while minimizing disparities between privileged and unprivileged groups. We evaluate whether a balance between accuracy and fairness is achievable by comparing the proposed model with existing mitigation methods. Our findings indicate that the model performs favorably against state-of-the-art techniques. Using the NIH All-Of-US dataset, comprising data from 868 individuals over 1500 days, we demonstrate our model's effectiveness, achieving accuracy rates between 75\% and 85\%. }
}

@article{250217460v1,
  title={ Finetuning and Quantization of EEG-Based Foundational BioSignal Models   on ECG and PPG Data for Blood Pressure Estimation },
  author={ Bálint Tóth and Dominik Senti and Thorir Mar Ingolfsson and Jeffrey Zweidler and Alexandre Elsig and Luca Benini and Yawei Li },
  journal={ arXiv preprint arXiv:2502.17460v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.17460v1 },
  abstract={ Blood pressure (BP) is a key indicator of cardiovascular health. As hypertension remains a global cause of morbidity and mortality, accurate, continuous, and non-invasive BP monitoring is therefore of paramount importance. Photoplethysmography (PPG) and electrocardiography (ECG) can potentially enable continuous BP monitoring, yet training accurate and robust machine learning (ML) models remains challenging due to variability in data quality and patient-specific factors. Recently, multiple research groups explored Electroencephalographic (EEG)--based foundation models and demonstrated their exceptional ability to learn rich temporal resolution. Considering the morphological similarities between different biosignals, the question arises of whether a model pre-trained on one modality can effectively be exploited to improve the accuracy of a different signal type. In this work, we take an initial step towards generalized biosignal foundation models by investigating whether model representations learned from abundant EEG data can effectively be transferred to ECG/PPG data solely with fine-tuning, without the need for large-scale additional pre-training, for the BP estimation task. Evaluations on the MIMIC-III and VitalDB datasets demonstrate that our approach achieves near state-of-the-art accuracy for diastolic BP (mean absolute error of 1.57 mmHg) and surpasses by 1.5x the accuracy of prior works for systolic BP (mean absolute error 2.72 mmHg). Additionally, we perform dynamic INT8 quantization, reducing the smallest model size by over 3.5x (from 13.73 MB down to 3.83 MB) while preserving performance, thereby enabling unobtrusive, real-time BP monitoring on resource-constrained wearable devices. }
}

@article{250206438v1,
  title={ FEMBA: Efficient and Scalable EEG Analysis with a Bidirectional Mamba   Foundation Model },
  author={ Anna Tegon and Thorir Mar Ingolfsson and Xiaying Wang and Luca Benini and Yawei Li },
  journal={ arXiv preprint arXiv:2502.06438v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.06438v1 },
  abstract={ Accurate and efficient electroencephalography (EEG) analysis is essential for detecting seizures and artifacts in long-term monitoring, with applications spanning hospital diagnostics to wearable health devices. Robust EEG analytics have the potential to greatly improve patient care. However, traditional deep learning models, especially Transformer-based architectures, are hindered by their quadratic time and memory complexity, making them less suitable for resource-constrained environments. To address these challenges, we present FEMBA (Foundational EEG Mamba + Bidirectional Architecture), a novel self-supervised framework that establishes new efficiency benchmarks for EEG analysis through bidirectional state-space modeling. Unlike Transformer-based models, which incur quadratic time and memory complexity, FEMBA scales linearly with sequence length, enabling more scalable and efficient processing of extended EEG recordings. Trained on over 21,000 hours of unlabeled EEG and fine-tuned on three downstream tasks, FEMBA achieves competitive performance in comparison with transformer models, with significantly lower computational cost. Specifically, it reaches 81.82\% balanced accuracy (0.8921 AUROC) on TUAB and 0.949 AUROC on TUAR, while a tiny 7.8M-parameter variant demonstrates viability for resource-constrained devices. These results pave the way for scalable, general-purpose EEG analytics in both clinical and highlight FEMBA as a promising candidate for wearable applications. }
}

@article{241020542v2,
  title={ PaPaGei: Open Foundation Models for Optical Physiological Signals },
  author={ Arvind Pillai and Dimitris Spathis and Fahim Kawsar and Mohammad Malekzadeh },
  journal={ arXiv preprint arXiv:2410.20542v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.20542v2 },
  abstract={ Photoplethysmography (PPG) is the leading non-invasive technique for monitoring biosignals and cardiovascular health, with widespread adoption in both clinical settings and consumer wearable devices. While machine learning models trained on PPG signals have shown promise, they tend to be task-specific and struggle with generalization. Current research is limited by the use of single-device datasets, insufficient exploration of out-of-domain generalization, and a lack of publicly available models, which hampers reproducibility. To address these limitations, we present PaPaGei, the first open foundation model for PPG signals. The model is pre-trained on over 57,000 hours of data, comprising 20 million unlabeled PPG segments from publicly available datasets. We introduce a novel representation learning approach that leverages domain knowledge of PPG signal morphology across individuals, enabling the capture of richer representations compared to traditional contrastive learning methods. We evaluate PaPaGei against state-of-the-art time-series foundation models and self-supervised learning benchmarks across 20 tasks from 10 diverse datasets, spanning cardiovascular health, sleep disorders, pregnancy monitoring, and wellbeing assessment. Our model demonstrates superior performance, improving classification and regression metrics by 6.3\% and 2.9\% respectively in at least 14 tasks. Notably, PaPaGei achieves these results while being more data- and parameter-efficient, outperforming models that are 70x larger. Beyond accuracy, we examine model robustness across different skin tones, establishing a benchmark for bias evaluation in future models. PaPaGei can serve as both a feature extractor and an encoder for multimodal models, opening up new opportunities for multimodal health monitoring. }
}

@article{240807795v2,
  title={ Exoskeleton-Assisted Balance and Task Evaluation During Quiet Stance and   Kneeling in Construction },
  author={ Gayatri Sreenivasan and Chunchu Zhu and Jingang Yi },
  journal={ arXiv preprint arXiv:2408.07795v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.07795v2 },
  abstract={ Construction workers exert intense physical effort and experience serious safety and health risks in hazardous working environments. Quiet stance and kneeling are among the most common postures performed by construction workers during their daily work. This paper analyzes lower-limb joint influence on neural balance control strategies using the frequency behavior of the intersection point of ground reaction forces. To evaluate the impact of elevation and wearable knee exoskeletons on postural balance and welding task performance, we design and integrate virtual- and mixed-reality (VR/MR) to simulate elevated environments and welding tasks. A linear quadratic regulator-controlled triple- and double-link inverted pendulum model is used for balance strategy quantification in quiet stance and kneeling, respectively. Extensive multi-subject experiments are conducted to evaluate the usability of occupational exoskeletons in destabilizing construction environments. The quantified balance strategies capture the significance of knee joint during balance control of quiet stance and kneeling gaits. Results show that center of pressure sway area reduced up to 62\% in quiet stance and 39\% in kneeling for subjects tested in high-elevation VR/MR worksites when provided knee exoskeleton assistance. The comprehensive balance and multitask evaluation methodology developed aims to reveal exoskeleton design considerations to mitigate the fall risk in construction. }
}

@article{250201108v1,
  title={ Pulse-PPG: An Open-Source Field-Trained PPG Foundation Model for   Wearable Applications Across Lab and Field Settings },
  author={ Mithun Saha and Maxwell A. Xu and Wanting Mao and Sameer Neupane and James M. Rehg and Santosh Kumar },
  journal={ arXiv preprint arXiv:2502.01108v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.01108v1 },
  abstract={ Photoplethysmography (PPG)-based foundation models are gaining traction due to the widespread use of PPG in biosignal monitoring and their potential to generalize across diverse health applications. In this paper, we introduce Pulse-PPG, the first open-source PPG foundation model trained exclusively on raw PPG data collected over a 100-day field study with 120 participants. Existing PPG foundation models are either open-source but trained on clinical data or closed-source, limiting their applicability in real-world settings. We evaluate Pulse-PPG across multiple datasets and downstream tasks, comparing its performance against a state-of-the-art foundation model trained on clinical data. Our results demonstrate that Pulse-PPG, trained on uncurated field data, exhibits superior generalization across clinical and mobile health applications in both lab and field settings. This suggests that exposure to real-world variability enables the model to learn fine-grained representations, making it more adaptable across tasks. Furthermore, pre-training on field data surprisingly outperforms its pre-training on clinical data in many tasks, reinforcing the importance of training on real-world, diverse datasets. To encourage further advancements in robust foundation models leveraging field data, we plan to release Pulse-PPG, providing researchers with a powerful resource for developing more generalizable PPG-based models. }
}

@article{250200973v1,
  title={ A Wearable Device Dataset for Mental Health Assessment Using Laser   Doppler Flowmetry and Fluorescence Spectroscopy Sensors },
  author={ Minh Ngoc Nguyen and Khai Le-Duc and Tan-Hanh Pham and Trang Nguyen and Quang Minh Luu and Ba Kien Tran and Truong-Son Hy and Viktor Dremin and Sergei Sokolovsky and Edik Rafailov },
  journal={ arXiv preprint arXiv:2502.00973v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.00973v1 },
  abstract={ In this study, we introduce a novel method to predict mental health by building machine learning models for a non-invasive wearable device equipped with Laser Doppler Flowmetry (LDF) and Fluorescence Spectroscopy (FS) sensors. Besides, we present the corresponding dataset to predict mental health, e.g. depression, anxiety, and stress levels via the DAS-21 questionnaire. To our best knowledge, this is the world's largest and the most generalized dataset ever collected for both LDF and FS studies. The device captures cutaneous blood microcirculation parameters, and wavelet analysis of the LDF signal extracts key rhythmic oscillations. The dataset, collected from 132 volunteers aged 18-94 from 19 countries, explores relationships between physiological features, demographics, lifestyle habits, and health conditions. We employed a variety of machine learning methods to classify stress detection, in which LightGBM is identified as the most effective model for stress detection, achieving a ROC AUC of 0.7168 and a PR AUC of 0.8852. In addition, we also incorporated Explainable Artificial Intelligence (XAI) techniques into our analysis to investigate deeper insights into the model's predictions. Our results suggest that females, younger individuals and those with a higher Body Mass Index (BMI) or heart rate have a greater likelihood of experiencing mental health conditions like stress and anxiety. All related code and data are published online: https://github.com/leduckhai/Wearable\_LDF-FS. }
}

@article{241211276v2,
  title={ Wearable Accelerometer Foundation Models for Health via Knowledge   Distillation },
  author={ Salar Abbaspourazad and Anshuman Mishra and Joseph Futoma and Andrew C. Miller and Ian Shapiro },
  journal={ arXiv preprint arXiv:2412.11276v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.11276v2 },
  abstract={ Modern wearable devices can conveniently record various biosignals in the many different environments of daily living, enabling a rich view of individual health. However, not all biosignals are the same: high-fidelity biosignals, such as photoplethysmogram (PPG), contain more physiological information, but require optical sensors with a high power footprint. Alternatively, a lower-fidelity biosignal such as accelerometry has a significantly smaller power footprint and is available in almost any wearable device. While accelerometry is widely used for activity recognition and fitness, it is less explored for health biomarkers and diagnosis. Here, we show that an accelerometry foundation model can predict a wide variety of health targets. To achieve improved performance, we distill representational knowledge from PPG encoders to accelerometery encoders using 20 million minutes of unlabeled data, collected from \~{}172K participants in the Apple Heart and Movement Study under informed consent. We observe strong cross-modal alignment on unseen data, e.g., 99.2\% top-1 accuracy for retrieving PPG embeddings from accelerometry embeddings. We show that distilled accelerometry encoders have significantly more informative representations compared to self-supervised or supervised encoders trained directly on accelerometry data, observed by at least 23\%-49\% improved performance for predicting heart rate and heart rate variability. We also show that distilled accelerometry encoders are readily predictive of a wide array of downstream health targets, i.e., they are generalist foundation models. We believe accelerometry foundation models for health may unlock new opportunities for developing digital biomarkers from any wearable device. }
}

@article{250114919v2,
  title={ Clustering of functional data prone to complex heteroscedastic   measurement error },
  author={ Andi Mai and Lan Xue and Roger Zoh and Carmen Tekwe },
  journal={ arXiv preprint arXiv:2501.14919v2 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2501.14919v2 },
  abstract={ Several factors make clustering of functional data challenging, including the infinite-dimensional space to which observations belong and the lack of a defined probability density function for the functional random variable. To overcome these barriers, researchers either assume that observations belong to a finite-dimensional space spanned by basis functions or apply nonparametric smoothing methods to the functions prior to clustering. Although extensive literature describes clustering methods for functional data, few studies have explored the clustering of measurement error--prone function-valued data. In this work, we consider clustering methods for functional data prone to complex, heteroscedastic measurement errors. Two stage-based methods using mixed-effects models are first applied to adjust for measurement error bias, followed by cluster analysis of the measurement error--adjusted curves. Through simulations, we investigate how varying sample size, the magnitude of measurement error, and the presence of complex heteroscedastic measurement errors influence the cluster analysis of functional data. Our results indicate that failing to account for measurement errors and the correlation structures associated with frequently collected functional data reduces the accuracy of identifying the true latent groups or clusters. The method consistently produces better results regardless of the initial clustering values used. Moreover, it is flexible and can be applied to various clustering approaches, based on the specific distribution of the data. The developed methods are applied to two data sets: a school-based study of energy expenditure among elementary school-aged children in Texas and data from the National Health and Nutrition Examination Survey on participants' physical activity monitored by wearable devices at frequent intervals. }
}

@article{250209626v1,
  title={ On the Bias, Fairness, and Bias Mitigation for a Wearable-based Freezing   of Gait Detection in Parkinson's Disease },
  author={ Timothy Odonga and Christine D. Esper and Stewart A. Factor and J. Lucas McKay and Hyeokhyen Kwon },
  journal={ arXiv preprint arXiv:2502.09626v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.09626v1 },
  abstract={ Freezing of gait (FOG) is a debilitating feature of Parkinson's disease (PD), which is a cause of injurious falls among PD patients. Recent advances in wearable-based human activity recognition (HAR) technology have enabled the detection of FOG subtypes across benchmark datasets. Since FOG manifestation is heterogeneous, developing models that quantify FOG consistently across patients with varying demographics, FOG types, and PD conditions is important. Bias and fairness in FOG models remain understudied in HAR, with research focused mainly on FOG detection using single benchmark datasets. We evaluated the bias and fairness of HAR models for wearable-based FOG detection across demographics and PD conditions using multiple datasets and the effectiveness of transfer learning as a potential bias mitigation approach. Our evaluation using demographic parity ratio (DPR) and equalized odds ratio (EOR) showed model bias (DPR \& EOR < 0.8) for all stratified demographic variables, including age, sex, and disease duration. Our experiments demonstrated that transfer learning from multi-site datasets and generic human activity representations significantly improved fairness (average change in DPR +0.027, +0.039, respectively) and performance (average change in F1-score +0.026, +0.018, respectively) across attributes, supporting the hypothesis that generic human activity representations learn fairer representations applicable to health analytics. }
}

@article{250114549v1,
  title={ Wearable slot antenna at 2.45 GHz for off-body radiation: analysis of   efficiency, frequency shift and body absorption },
  author={ Marta Fernandez and Hugo G. Espinosa and David V. Thiel and Amaia Arrinda },
  journal={ arXiv preprint arXiv:2501.14549v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2501.14549v1 },
  abstract={ The interaction of body worn antennas with the human body causes a significant decrease in the antenna efficiency and a shift in the resonant frequency. A resonant slot in a small conductive box placed on the body has been shown to reduce these effects. The specific absorption rate (SAR) is less than international health standards for most wearable antennas due to the small transmitter power. This paper reports the linear relationship between the power absorbed by biological tissues at different locations on the body, and the radiation efficiency based on numerical modeling (r = 0.99). While the -10 dB bandwidth of the antenna remains constant and equal to 12.5\%, the maximum frequency shift occurs when the antenna is close to the elbow (6.61\%) and on the thigh (5.86\%). The smallest change was found on the torso (4.21\%). Participants with body-mass index (BMI) between 17 and 29 kg/m2 took part in experimental measurements, where the maximum frequency shift was 2.51\%. Measurements show better agreement with simulations on the upper arm. These experimental results demonstrate that the BMI for each individual has little effect on the performance of the antenna. }
}

@article{241115240v3,
  title={ AI Foundation Models for Wearable Movement Data in Mental Health   Research },
  author={ Franklin Y. Ruan and Aiwei Zhang and Jenny Y. Oh and SouYoung Jin and Nicholas C. Jacobson },
  journal={ arXiv preprint arXiv:2411.15240v3 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2411.15240v3 },
  abstract={ Pretrained foundation models and transformer architectures have driven the success of large language models (LLMs) and other modern AI breakthroughs. However, similar advancements in health data modeling remain limited due to the need for innovative adaptations. Wearable movement data offers a valuable avenue for exploration, as it's a core feature in nearly all commercial smartwatches, well established in clinical and mental health research, and the sequential nature of the data shares similarities to language. We introduce the Pretrained Actigraphy Transformer (PAT), the first open source foundation model designed for time-series wearable movement data. Leveraging transformer-based architectures and novel techniques, such as patch embeddings, and pretraining on data from 29,307 participants in a national U.S. sample, PAT achieves state-of-the-art performance in several mental health prediction tasks. PAT is also lightweight and easily interpretable, making it a robust tool for mental health research.   GitHub: https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/ }
}

@article{241219967v2,
  title={ MobileNetV2: A lightweight classification model for home-based sleep   apnea screening },
  author={ Hui Pan and Yanxuan Yu and Jilun Ye and Xu Zhang },
  journal={ arXiv preprint arXiv:2412.19967v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.19967v2 },
  abstract={ This study proposes a novel lightweight neural network model leveraging features extracted from electrocardiogram (ECG) and respiratory signals for early OSA screening. ECG signals are used to generate feature spectrograms to predict sleep stages, while respiratory signals are employed to detect sleep-related breathing abnormalities. By integrating these predictions, the method calculates the apnea-hypopnea index (AHI) with enhanced accuracy, facilitating precise OSA diagnosis.   The method was validated on three publicly available sleep apnea databases: the Apnea-ECG database, the UCDDB dataset, and the MIT-BIH Polysomnographic database. Results showed an overall OSA detection accuracy of 0.978, highlighting the model's robustness. Respiratory event classification achieved an accuracy of 0.969 and an area under the receiver operating characteristic curve (ROC-AUC) of 0.98. For sleep stage classification, in UCDDB dataset, the ROC-AUC exceeded 0.85 across all stages, with recall for Sleep reaching 0.906 and specificity for REM and Wake states at 0.956 and 0.937, respectively.   This study underscores the potential of integrating lightweight neural networks with multi-signal analysis for accurate, portable, and cost-effective OSA screening, paving the way for broader adoption in home-based and wearable health monitoring systems. }
}

@article{241217860v1,
  title={ EnhancePPG: Improving PPG-based Heart Rate Estimation with   Self-Supervision and Augmentation },
  author={ Luca Benfenati and Sofia Belloni and Alessio Burrello and Panagiotis Kasnesis and Xiaying Wang and Luca Benini and Massimo Poncino and Enrico Macii and Daniele Jahier Pagliari },
  journal={ arXiv preprint arXiv:2412.17860v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.17860v1 },
  abstract={ Heart rate (HR) estimation from photoplethysmography (PPG) signals is a key feature of modern wearable devices for health and wellness monitoring. While deep learning models show promise, their performance relies on the availability of large datasets. We present EnhancePPG, a method that enhances state-of-the-art models by integrating self-supervised learning with data augmentation (DA). Our approach combines self-supervised pre-training with DA, allowing the model to learn more generalizable features, without needing more labelled data. Inspired by a U-Net-like autoencoder architecture, we utilize unsupervised PPG signal reconstruction, taking advantage of large amounts of unlabeled data during the pre-training phase combined with data augmentation, to improve state-of-the-art models' performance. Thanks to our approach and minimal modification to the state-of-the-art model, we improve the best HR estimation by 12.2\%, lowering from 4.03 Beats-Per-Minute (BPM) to 3.54 BPM the error on PPG-DaLiA. Importantly, our EnhancePPG approach focuses exclusively on the training of the selected deep learning model, without significantly increasing its inference latency }
}

@article{241217832v1,
  title={ MANGO: Multimodal Acuity traNsformer for intelliGent ICU Outcomes },
  author={ Jiaqing Zhang and Miguel Contreras and Sabyasachi Bandyopadhyay and Andrea Davidson and Jessica Sena and Yuanfang Ren and Ziyuan Guan and Tezcan Ozrazgat-Baslanti and Tyler J. Loftus and Subhash Nerella and Azra Bihorac and Parisa Rashidi },
  journal={ arXiv preprint arXiv:2412.17832v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.17832v1 },
  abstract={ Estimation of patient acuity in the Intensive Care Unit (ICU) is vital to ensure timely and appropriate interventions. Advances in artificial intelligence (AI) technologies have significantly improved the accuracy of acuity predictions. However, prior studies using machine learning for acuity prediction have predominantly relied on electronic health records (EHR) data, often overlooking other critical aspects of ICU stay, such as patient mobility, environmental factors, and facial cues indicating pain or agitation. To address this gap, we present MANGO: the Multimodal Acuity traNsformer for intelliGent ICU Outcomes, designed to enhance the prediction of patient acuity states, transitions, and the need for life-sustaining therapy. We collected a multimodal dataset ICU-Multimodal, incorporating four key modalities, EHR data, wearable sensor data, video of patient's facial cues, and ambient sensor data, which we utilized to train MANGO. The MANGO model employs a multimodal feature fusion network powered by Transformer masked self-attention method, enabling it to capture and learn complex interactions across these diverse data modalities even when some modalities are absent. Our results demonstrated that integrating multiple modalities significantly improved the model's ability to predict acuity status, transitions, and the need for life-sustaining therapy. The best-performing models achieved an area under the receiver operating characteristic curve (AUROC) of 0.76 (95\% CI: 0.72-0.79) for predicting transitions in acuity status and the need for life-sustaining therapy, while 0.82 (95\% CI: 0.69-0.89) for acuity status prediction... }
}

@article{241209289v1,
  title={ Optimising TinyML with Quantization and Distillation of Transformer and   Mamba Models for Indoor Localisation on Edge Devices },
  author={ Thanaphon Suwannaphong and Ferdian Jovan and Ian Craddock and Ryan McConville },
  journal={ arXiv preprint arXiv:2412.09289v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.09289v1 },
  abstract={ This paper proposes small and efficient machine learning models (TinyML) for resource-constrained edge devices, specifically for on-device indoor localisation. Typical approaches for indoor localisation rely on centralised remote processing of data transmitted from lower powered devices such as wearables. However, there are several benefits for moving this to the edge device itself, including increased battery life, enhanced privacy, reduced latency and lowered operational costs, all of which are key for common applications such as health monitoring. The work focuses on model compression techniques, including quantization and knowledge distillation, to significantly reduce the model size while maintaining high predictive performance. We base our work on a large state-of-the-art transformer-based model and seek to deploy it within low-power MCUs. We also propose a state-space-based architecture using Mamba as a more compact alternative to the transformer. Our results show that the quantized transformer model performs well within a 64 KB RAM constraint, achieving an effective balance between model size and localisation precision. Additionally, the compact Mamba model has strong performance under even tighter constraints, such as a 32 KB of RAM, without the need for model compression, making it a viable option for more resource-limited environments. We demonstrate that, through our framework, it is feasible to deploy advanced indoor localisation models onto low-power MCUs with restricted memory limitations. The application of these TinyML models in healthcare has the potential to revolutionize patient monitoring by providing accurate, real-time location data while minimizing power consumption, increasing data privacy, improving latency and reducing infrastructure costs. }
}

@article{241205895v1,
  title={ A Review on Multisensor Data Fusion for Wearable Health Monitoring },
  author={ Arlene John and Barry Cardiff and Deepu John },
  journal={ arXiv preprint arXiv:2412.05895v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.05895v1 },
  abstract={ The growing demand for accurate, continuous, and non-invasive health monitoring has propelled multi-sensor data fusion to the forefront of healthcare technology. This review aims to provide an overview of the development of fusion frameworks in the literature and common terminology used in fusion literature. The review introduces the fusion classification standards and methods that are most relevant from an algorithm development perspective. Applications of the reviewed fusion frameworks in fields such as defense, autonomous driving, robotics, and image fusion are also discussed to provide contextual information on the various fusion methodologies that have been developed in this field. This review provides a comprehensive analysis of multi-sensor data fusion methods applied to health monitoring systems, focusing on key algorithms, applications, challenges, and future directions. We examine commonly used fusion techniques, including Kalman filters, Bayesian networks, and machine learning models. By integrating data from various sources, these fusion approaches enhance the reliability, accuracy, and resilience of health monitoring systems. However, challenges such as data quality and differences in acquisition systems exist, calling for intelligent fusion algorithms in recent years. The review finally converges on applications of fusion algorithms in biomedical inference tasks like heartbeat detection, respiration rate estimation, sleep apnea detection, arrhythmia detection, and atrial fibrillation detection. }
}

@article{241204950v1,
  title={ Bed-Attached Vibration Sensor System: A Machine Learning Approach for   Fall Detection in Nursing Homes },
  author={ Thomas Bartz-Beielstein and Axel Wellendorf and Noah Pütz and Jens Brandt and Alexander Hinterleitner and Richard Schulz and Richard Scholz and Olaf Mersmann and Robin Knabe },
  journal={ arXiv preprint arXiv:2412.04950v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.04950v1 },
  abstract={ The increasing shortage of nursing staff and the acute risk of falls in nursing homes pose significant challenges for the healthcare system. This study presents the development of an automated fall detection system integrated into care beds, aimed at enhancing patient safety without compromising privacy through wearables or video monitoring. Mechanical vibrations transmitted through the bed frame are processed using a short-time Fourier transform, enabling robust classification of distinct human fall patterns with a convolutional neural network. Challenges pertaining to the quantity and diversity of the data are addressed, proposing the generation of additional data with a specific emphasis on enhancing variation. While the model shows promising results in distinguishing fall events from noise using lab data, further testing in real-world environments is recommended for validation and improvement. Despite limited available data, the proposed system shows the potential for an accurate and rapid response to falls, mitigating health implications, and addressing the needs of an aging population. This case study was performed as part of the ZIM Project. Further research on sensors enhanced by artificial intelligence will be continued in the ShapeFuture Project. }
}

@article{240801580v2,
  title={ Controlling Dataflows with a Bolt-on Data Escrow },
  author={ Zhiru Zhu and Raul Castro Fernandez },
  journal={ arXiv preprint arXiv:2408.01580v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.01580v2 },
  abstract={ In today's data-driven economy, individuals share their data with platforms in exchange for services such as search, social networks, and health recommendations, platforms use the data to provide those services and create other revenue-generating opportunities, e.g., selling the data to data brokers, all of which generate tremendous value. With the ever-expanding data economy comes the growing concern about potential data misuse. While most platforms give individuals specific control over their data (i.e., what data is being shared), individuals cannot limit the purposes of sharing their data since they cannot control how their data is used once it is shared.   In this paper, we introduce a data management solution to this socio-technical problem. We present a data escrow design that permits individuals to observe all dataflows -- not just what data is shared but also for what purpose it will be used. Rather than having individuals' data flowing to the platform, the platform delegates their computation to the escrow, where individuals can observe and manage their data. We propose a minimally invasive programming interface to enable the escrow's delegated computation model; developers specify dataflows via the interface and the escrow runs the computation based on developers' specifications. In addition to proposing the escrow design, which is general and applies to different ecosystems such as web browsers, wearables, and mobile platforms, we also contribute a concrete escrow implementation in the Apple ecosystem. In our evaluation, we analyze the dataflows in real-world applications and show that the escrow's programming interface supports implementing a wide range of dataflows, and thus applications. We show that our escrow-based solution is a feasible and practical alternative to today's data governance and has minimum overhead. }
}

@article{241117935v1,
  title={ State Anxiety Biomarker Discovery: Electrooculography and Electrodermal   Activity in Stress Monitoring },
  author={ Jadelynn Dao and Ruixiao Liu and Sarah Solomon and Samuel Solomon },
  journal={ arXiv preprint arXiv:2411.17935v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2411.17935v1 },
  abstract={ Anxiety has become a significant health concern affecting mental and physical well-being, with state anxiety, a transient emotional response, linked to adverse cardiovascular and long-term health outcomes. This research explores the potential of non-invasive wearable technology to enhance the real-time monitoring of physiological responses associated with state anxiety. Using electrooculography (EOG) and electrodermal activity (EDA), we have reviewed novel biomarkers that reveal nuanced emotional and stress responses. Our study presents two datasets: 1) EOG signal blink identification dataset BLINKEO, containing both true blink events and motion artifacts, and 2) EOG and EDA signals dataset EMOCOLD, capturing physiological responses from a Cold Pressor Test (CPT). From analyzing blink rate variability, skin conductance peaks, and associated arousal metrics, we identified multiple new anxiety-specific biomarkers. SHapley Additive exPlanations (SHAP) were used to interpret and refine our model, enabling a robust understanding of the biomarkers that correlate strongly with state anxiety. These results suggest that a combined analysis of EOG and EDA data offers significant improvements in detecting real-time anxiety markers, underscoring the potential of wearables in personalized health monitoring and mental health intervention strategies. This work contributes to the development of context-sensitive models for anxiety assessment, promoting more effective applications of wearable technology in healthcare. }
}

@article{241112689v1,
  title={ IMUVIE: Pickup Timeline Action Localization via Motion Movies },
  author={ John Clapham and Kenneth Koltermann and Yanfu Zhang and Yuming Sun and Evie N Burnet and Gang Zhou },
  journal={ arXiv preprint arXiv:2411.12689v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2411.12689v1 },
  abstract={ Falls among seniors due to difficulties with tasks such as picking up objects pose significant health and safety risks, impacting quality of life and independence. Reliable, accessible assessment tools are critical for early intervention but often require costly clinic-based equipment and trained personnel, limiting their use in daily life. Existing wearable-based pickup measurement solutions address some needs but face limitations in generalizability.   We present IMUVIE, a wearable system that uses motion movies and a machine-learning model to automatically detect and measure pickup events, providing a practical solution for frequent monitoring. IMUVIE's design principles-data normalization, occlusion handling, and streamlined visuals-enhance model performance and are adaptable to tasks beyond pickup classification.   In rigorous leave one subject out cross validation evaluations, IMUVIE achieves exceptional window level localization accuracy of 91-92\% for pickup action classification on 256,291 motion movie frame candidates while maintaining an event level recall of 97\% when evaluated on 129 pickup events. IMUVIE has strong generalization and performs well on unseen subjects. In an interview survey, IMUVIE demonstrated strong user interest and trust, with ease of use identified as the most critical factor for adoption. IMUVIE offers a practical, at-home solution for fall risk assessment, facilitating early detection of movement deterioration, and supporting safer, independent living for seniors. }
}

@article{241112585v1,
  title={ Semiparametric quantile functional regression analysis of adolescent   physical activity distributions in the presence of missing data },
  author={ Benny Ren and Ian Barnett and Haochang Shou and Jeremy Rubin and Hongxiao Zhu and Terry Conway and Kelli Cain and Brian Saelens and Karen Glanz and James Sallis and Jeffrey S. Morris },
  journal={ arXiv preprint arXiv:2411.12585v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2411.12585v1 },
  abstract={ In the age of digital healthcare, passively collected physical activity profiles from wearable sensors are a preeminent tool for evaluating health outcomes. In order to fully leverage the vast amounts of data collected through wearable accelerometers, we propose to use quantile functional regression to model activity profiles as distributional outcomes through quantile responses, which can be used to evaluate activity level differences across covariates based on any desired distributional summary. Our proposed framework addresses two key problems not handled in existing distributional regression literature. First, we use spline mixed model formulations in the basis space to model nonparametric effects of continuous predictors on the distributional response. Second, we address the underlying missingness problem that is common in these types of wearable data but typically not addressed. We show that the missingness can induce bias in the subject-specific distributional summaries that leads to biased distributional regression estimates and even bias the frequently used scalar summary measures, and introduce a nonparametric function-on-function modeling approach that adjusts for each subject's missingness profile to address this problem. We evaluate our nonparametric modeling and missing data adjustment using simulation studies based on realistically simulated activity profiles and use it to gain insights into adolescent activity profiles from the Teen Environment and Neighborhood study. }
}

@article{241201829v1,
  title={ Explainable Artificial Intelligence for Medical Applications: A Review },
  author={ Qiyang Sun and Alican Akman and Björn W. Schuller },
  journal={ arXiv preprint arXiv:2412.01829v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.01829v1 },
  abstract={ The continuous development of artificial intelligence (AI) theory has propelled this field to unprecedented heights, owing to the relentless efforts of scholars and researchers. In the medical realm, AI takes a pivotal role, leveraging robust machine learning (ML) algorithms. AI technology in medical imaging aids physicians in X-ray, computed tomography (CT) scans, and magnetic resonance imaging (MRI) diagnoses, conducts pattern recognition and disease prediction based on acoustic data, delivers prognoses on disease types and developmental trends for patients, and employs intelligent health management wearable devices with human-computer interaction technology to name but a few. While these well-established applications have significantly assisted in medical field diagnoses, clinical decision-making, and management, collaboration between the medical and AI sectors faces an urgent challenge: How to substantiate the reliability of decision-making? The underlying issue stems from the conflict between the demand for accountability and result transparency in medical scenarios and the black-box model traits of AI. This article reviews recent research grounded in explainable artificial intelligence (XAI), with an emphasis on medical practices within the visual, audio, and multimodal perspectives. We endeavour to categorise and synthesise these practices, aiming to provide support and guidance for future researchers and healthcare professionals. }
}

@article{241108699v1,
  title={ FedSub: Introducing class-aware Subnetworks Fusion to Enhance   Personalized Federated Learning in Ubiquitous Systems },
  author={ Mattia Giovanni Campana and Franca Delmastro },
  journal={ arXiv preprint arXiv:2411.08699v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2411.08699v1 },
  abstract={ Personalized Federated Learning is essential in AI-driven ubiquitous systems, supporting the distributed development of models able to adapt to diverse and evolving user behaviors while safeguarding privacy. Despite addressing heterogeneous user data distributions in collaborative model training, existing methods often face limitations balancing personalization and generalization, oversimplifying user similarities, or relying heavily on global models. In this paper, we propose FedSub, a novel federated approach designed to enhance personalization through the use of class-aware prototypes and model subnetworks. Prototypes serve as compact representations of user data, clustered on the server to identify similarities based on specific label patterns. Concurrently, subnetworks -- model components necessary to process each class -- are extracted locally and fused by the server according to these clusters, producing highly tailored model updates for each user. This fine-grained, class-specific aggregation of clients' models allows FedSub to capture the unique characteristics of individual user data patterns. The effectiveness of FedSub is validated in three real-world scenarios characterized by high data heterogeneity, derived from human activity recognition and mobile health applications. Experimental evaluations demonstrate FedSub's performance improvements with respect to the state-of-the-art and significant advancements in personalization for ubiquitous systems based on personal mobile and wearable devices. }
}

@article{241022950v1,
  title={ SpiroActive: Active Learning for Efficient Data Acquisition for   Spirometry },
  author={ Ankita Kumari Jain and Nitish Sharma and Madhav Kanda and Nipun Batra },
  journal={ arXiv preprint arXiv:2410.22950v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.22950v1 },
  abstract={ Respiratory illnesses are a significant global health burden. Respiratory illnesses, primarily Chronic obstructive pulmonary disease (COPD), is the seventh leading cause of poor health worldwide and the third leading cause of death worldwide, causing 3.23 million deaths in 2019, necessitating early identification and diagnosis for effective mitigation. Among the diagnostic tools employed, spirometry plays a crucial role in detecting respiratory abnormalities. However, conventional clinical spirometry methods often entail considerable costs and practical limitations like the need for specialized equipment, trained personnel, and a dedicated clinical setting, making them less accessible. To address these challenges, wearable spirometry technologies have emerged as promising alternatives, offering accurate, cost-effective, and convenient solutions. The development of machine learning models for wearable spirometry heavily relies on the availability of high-quality ground truth spirometry data, which is a laborious and expensive endeavor. In this research, we propose using active learning, a sub-field of machine learning, to mitigate the challenges associated with data collection and labeling. By strategically selecting samples from the ground truth spirometer, we can mitigate the need for resource-intensive data collection. We present evidence that models trained on small subsets obtained through active learning achieve comparable/better results than models trained on the complete dataset. }
}

@article{241021319v1,
  title={ Towards Continuous Skin Sympathetic Nerve Activity Monitoring: Removing   Muscle Noise },
  author={ Farnoush Baghestani and Mahdi Pirayesh Shirazi Nejad and Youngsun Kong and Ki H. Chon },
  journal={ arXiv preprint arXiv:2410.21319v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.21319v1 },
  abstract={ Continuous monitoring of non-invasive skin sympathetic nerve activity (SKNA) holds promise for understanding the sympathetic nervous system (SNS) dynamics in various physiological and pathological conditions. However, muscle noise artifacts present a challenge in accurate SKNA analysis, particularly in real-life scenarios. This study proposes a deep convolutional neural network (CNN) approach to detect and remove muscle noise from SKNA recordings obtained via ECG electrodes. Twelve healthy participants underwent controlled experimental protocols involving cognitive stress induction and voluntary muscle movements, while collecting SKNA data. Power spectral analysis revealed significant muscle noise interference within the SKNA frequency band (500-1000 Hz). A 2D CNN model was trained on the spectrograms of the data segments to classify them into baseline, stress-induced SKNA, and muscle noise-contaminated periods, achieving an average accuracy of 89.85\% across all subjects. Our findings underscore the importance of addressing muscle noise for accurate SKNA monitoring, advancing towards wearable SKNA sensors for real-world applications. }
}

@article{241016924v1,
  title={ SleepCoT: A Lightweight Personalized Sleep Health Model via   Chain-of-Thought Distillation },
  author={ Huimin Zheng and Xiaofeng Xing and Xiangmin Xu },
  journal={ arXiv preprint arXiv:2410.16924v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.16924v1 },
  abstract={ We present a novel approach to personalized sleep health management using few-shot Chain-of-Thought (CoT) distillation, enabling small-scale language models (> 2B parameters) to rival the performance of large language models (LLMs) in specialized health domains. Our method simultaneously distills problem-solving strategies, long-tail expert knowledge, and personalized recommendation capabilities from larger models into more efficient, compact models. Unlike existing systems, our approach offers three key functionalities: generating personalized sleep health recommendations, supporting user-specific follow-up inquiries, and providing responses to domain-specific knowledge questions. We focus on sleep health due to its measurability via wearable devices and its impact on overall well-being. Our experimental setup, involving GPT-4o for data synthesis, Qwen-max for instruction set creation, and Qwen2.5 1.5B for model distillation, demonstrates significant improvements over baseline small-scale models in penalization, reasoning, and knowledge application. Experiments using 100 simulated sleep reports and 1,000 domain-specific questions shows our model achieves comparable performance to larger models while maintaining efficiency for real-world deployment. This research not only advances AI-driven health management but also provides a novel approach to leveraging LLM capabilities in resource-constrained environments, potentially enhancing the accessibility of personalized healthcare solutions. }
}

@article{241013638v1,
  title={ Scaling Wearable Foundation Models },
  author={ Girish Narayanswamy and Xin Liu and Kumar Ayush and Yuzhe Yang and Xuhai Xu and Shun Liao and Jake Garrison and Shyam Tailor and Jake Sunshine and Yun Liu and Tim Althoff and Shrikanth Narayanan and Pushmeet Kohli and Jiening Zhan and Mark Malhotra and Shwetak Patel and Samy Abdel-Ghaffar and Daniel McDuff },
  journal={ arXiv preprint arXiv:2410.13638v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.13638v1 },
  abstract={ Wearable sensors have become ubiquitous thanks to a variety of health tracking features. The resulting continuous and longitudinal measurements from everyday life generate large volumes of data; however, making sense of these observations for scientific and actionable insights is non-trivial. Inspired by the empirical success of generative modeling, where large neural networks learn powerful representations from vast amounts of text, image, video, or audio data, we investigate the scaling properties of sensor foundation models across compute, data, and model size. Using a dataset of up to 40 million hours of in-situ heart rate, heart rate variability, electrodermal activity, accelerometer, skin temperature, and altimeter per-minute data from over 165,000 people, we create LSM, a multimodal foundation model built on the largest wearable-signals dataset with the most extensive range of sensor modalities to date. Our results establish the scaling laws of LSM for tasks such as imputation, interpolation and extrapolation, both across time and sensor modalities. Moreover, we highlight how LSM enables sample-efficient downstream learning for tasks like exercise and activity recognition. }
}

@article{241011631v1,
  title={ 3D printing by two-photon polymerization of hollow microneedles for   interstitial fluid extraction },
  author={ Tiago Elias Abi-Ramia Silva and Stephan Kohler and Nicolas Bartzsch and Felix Beuschlein and Andreas T. Guentner },
  journal={ arXiv preprint arXiv:2410.11631v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.11631v1 },
  abstract={ Dermal interstitial fluid (ISF) is a rich source of biomarkers (e.g., glucose) that can be used for continuous health monitoring with wearable sensors. Hollow microneedle devices are a promising solution to extract ISF on demand by penetrating the skin with minimal pain. However, they rely on inserting bio-incompatible materials (e.g., silicon) into individuals, limiting the application time. Here, the direct 3D printing of polymer hollow microneedles on silicon-based microfluidic devices and the successful in-vivo extraction of ISF are demonstrated. Our additive manufacturing approach enables the versatile combination of materials and rapid prototyping of microneedle geometry. After improving the design through finite element modeling, a hollow microneedle geometry was printed by two-photon polymerization and experimentally characterized with mechanical and fluidic tests. Microneedles were fabricated with high accuracy (i.e., 997 +/- 2 um) and reliably interfaced with the microfluidic chip (i.e., centerline alignment within 5\% of diameter). The needles demonstrated sufficient mechanical strength (i.e., 411 +/- 3 mN per needle) to endure at least 10 consecutive insertions into simulated skin. Biocompatibility and ISF extraction were demonstrated in an in-vivo 72-hour test, showing the safety and reliability of our approach. Such a platform is promising for minimally invasive, continuous monitoring of biomarkers in ISF, aiding in medical diagnoses and personalized health treatments. }
}

@article{241004366v1,
  title={ RespDiff: An End-to-End Multi-scale RNN Diffusion Model for Respiratory   Waveform Estimation from PPG Signals },
  author={ Yuyang Miao and Zehua Chen and Chang Li and Danilo Mandic },
  journal={ arXiv preprint arXiv:2410.04366v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.04366v1 },
  abstract={ Respiratory rate (RR) is a critical health indicator often monitored under inconvenient scenarios, limiting its practicality for continuous monitoring. Photoplethysmography (PPG) sensors, increasingly integrated into wearable devices, offer a chance to continuously estimate RR in a portable manner. In this paper, we propose RespDiff, an end-to-end multi-scale RNN diffusion model for respiratory waveform estimation from PPG signals. RespDiff does not require hand-crafted features or the exclusion of low-quality signal segments, making it suitable for real-world scenarios. The model employs multi-scale encoders, to extract features at different resolutions, and a bidirectional RNN to process PPG signals and extract respiratory waveform. Additionally, a spectral loss term is introduced to optimize the model further. Experiments conducted on the BIDMC dataset demonstrate that RespDiff outperforms notable previous works, achieving a mean absolute error (MAE) of 1.18 bpm for RR estimation while others range from 1.66 to 2.15 bpm, showing its potential for robust and accurate respiratory monitoring in real-world applications. }
}

@article{241003211v1,
  title={ CUDLE: Learning Under Label Scarcity to Detect Cannabis Use in   Uncontrolled Environments },
  author={ Reza Rahimi Azghan and Nicholas C. Glodosky and Ramesh Kumar Sah and Carrie Cuttler and Ryan McLaughlin and Michael J. Cleveland and Hassan Ghasemzadeh },
  journal={ arXiv preprint arXiv:2410.03211v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.03211v1 },
  abstract={ Wearable sensor systems have demonstrated a great potential for real-time, objective monitoring of physiological health to support behavioral interventions. However, obtaining accurate labels in free-living environments remains difficult due to limited human supervision and the reliance on self-labeling by patients, making data collection and supervised learning particularly challenging. To address this issue, we introduce CUDLE (Cannabis Use Detection with Label Efficiency), a novel framework that leverages self-supervised learning with real-world wearable sensor data to tackle a pressing healthcare challenge: the automatic detection of cannabis consumption in free-living environments. CUDLE identifies cannabis consumption moments using sensor-derived data through a contrastive learning framework. It first learns robust representations via a self-supervised pretext task with data augmentation. These representations are then fine-tuned in a downstream task with a shallow classifier, enabling CUDLE to outperform traditional supervised methods, especially with limited labeled data. To evaluate our approach, we conducted a clinical study with 20 cannabis users, collecting over 500 hours of wearable sensor data alongside user-reported cannabis use moments through EMA (Ecological Momentary Assessment) methods. Our extensive analysis using the collected data shows that CUDLE achieves a higher accuracy of 73.4\%, compared to 71.1\% for the supervised approach, with the performance gap widening as the number of labels decreases. Notably, CUDLE not only surpasses the supervised model while using 75\% less labels, but also reaches peak performance with far fewer subjects. }
}

@article{241002692v1,
  title={ Prediabetes detection in unconstrained conditions using wearable sensors },
  author={ Dimitra Tatli and Vasileios Papapanagiotou and Aris Liakos and Apostolos Tsapas and Anastasios Delopoulos },
  journal={ arXiv preprint arXiv:2410.02692v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.02692v1 },
  abstract={ Prediabetes is a common health condition that often goes undetected until it progresses to type 2 diabetes. Early identification of prediabetes is essential for timely intervention and prevention of complications. This research explores the feasibility of using wearable continuous glucose monitoring along with smartwatches with embedded inertial sensors to collect glucose measurements and acceleration signals respectively, for the early detection of prediabetes. We propose a methodology based on signal processing and machine learning techniques. Two feature sets are extracted from the collected signals, based both on a dynamic modeling of the human glucose-homeostasis system and on the Glucose curve, inspired by three major glucose related blood tests. Features are aggregated per individual using bootstrap. Support Vector Machines are used to classify normoglycemic vs. prediabetic individuals. We collected data from 22 participants for evaluation. The results are highly encouraging, demonstrating high sensitivity and precision. This work is a proof of concept, highlighting the potential of wearable devices in prediabetes assessment. Future directions involve expanding the study to a larger, more diverse population and exploring the integration of CGM and smartwatch functionalities into a unified device. Automated eating detecting algorithms can also be used. }
}

@article{240916339v1,
  title={ Large-scale digital phenotyping: identifying depression and anxiety   indicators in a general UK population with over 10,000 participants },
  author={ Yuezhou Zhang and Callum Stewart and Yatharth Ranjan and Pauline Conde and Heet Sankesara and Zulqarnain Rashid and Shaoxiong Sun and Richard J B Dobson and Amos A Folarin },
  journal={ arXiv preprint arXiv:2409.16339v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2409.16339v1 },
  abstract={ Digital phenotyping offers a novel and cost-efficient approach for managing depression and anxiety. Previous studies, often limited to small-to-medium or specific populations, may lack generalizability. We conducted a cross-sectional analysis of data from 10,129 participants recruited from a UK-based general population between June 2020 and August 2022. Participants shared wearable (Fitbit) data and self-reported questionnaires on depression (PHQ-8), anxiety (GAD-7), and mood via a study app. We first examined the correlations between PHQ-8/GAD-7 scores and wearable-derived features, demographics, health data, and mood assessments. Subsequently, unsupervised clustering was used to identify behavioural patterns associated with depression or anxiety. Finally, we employed separate XGBoost models to predict depression and anxiety and compared the results using different subsets of features. We observed significant associations between the severity of depression and anxiety with several factors, including mood, age, gender, BMI, sleep patterns, physical activity, and heart rate. Clustering analysis revealed that participants simultaneously exhibiting lower physical activity levels and higher heart rates reported more severe symptoms. Prediction models incorporating all types of variables achieved the best performance (\$R\^{}2\$=0.41, MAE=3.42 for depression; \$R\^{}2\$=0.31, MAE=3.50 for anxiety) compared to those using subsets of variables. This study identified potential indicators for depression and anxiety, highlighting the utility of digital phenotyping and machine learning technologies for rapid screening of mental disorders in general populations. These findings provide robust real-world insights for future healthcare applications. }
}

@article{220800739v6,
  title={ Model-Twin Randomization (MoTR) for Estimating One's Own Recurring   Individual Treatment Effect },
  author={ Eric J. Daza and Logan Schneider },
  journal={ arXiv preprint arXiv:2208.00739v6 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2208.00739v6 },
  abstract={ Temporally dense single-person ''small data'' have become widely available thanks to mobile apps and wearable sensors. Many caregivers and self-trackers want to use these data to help a specific person change their behavior to achieve desired health outcomes. Ideally, this involves discerning possible causes from correlations using that person's own observational time series data. In this paper, we estimate within-individual average treatment effects of physical activity on sleep duration, and vice-versa. We introduce the model twin randomization (MoTR; ''motor'') method for analyzing an individual's intensive longitudinal data. Formally, MoTR is an application of the g-formula (i.e., standardization, back-door adjustment) under serial interference. It estimates stable recurring effects, as is done in n-of-1 trials and single case experimental designs. We compare our approach to standard methods (with possible confounding) to show how to use causal inference to make better personalized recommendations for health behavior change, and analyze up to almost eight years of the authors' own Fitbit steps and sleep data. }
}

@article{241003696v1,
  title={ Improving Emotion Recognition Accuracy with Personalized Clustering },
  author={ Laura Gutierrez-Martin and Celia Lopez Ongil and Jose M. Lanza-Gutierrez and Jose A. Miranda Calero },
  journal={ arXiv preprint arXiv:2410.03696v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.03696v1 },
  abstract={ Emotion recognition through artificial intelligence and smart sensing of physical and physiological signals (Affective Computing) is achieving very interesting results in terms of accuracy, inference times, and user-independent models. In this sense, there are applications related to the safety and well-being of people (sexual aggressions, gender-based violence, children and elderly abuse, mental health, etc.) that require even more improvements. Emotion detection should be done with fast, discrete, and non-luxurious systems working in real-time and real life (wearable devices, wireless communications, battery-powered). Furthermore, emotional reactions to violence are not equal in all people. Then, large general models cannot be applied to a multiuser system for people protection, and customized and simple AI models would be welcomed by health and social workers and law enforcement agents. These customized models will be applicable to clusters of subjects sharing similarities in their emotional reactions to external stimuli. This customization requires several steps: creating clusters of subjects with similar behaviors, creating AI models for every cluster, continually updating these models with new data, and enrolling new subjects in clusters when required. A methodology for clustering data compiled (physical and physiological data, together with emotional labels) is presented in this work, as well as the method for including new subjects once the AI model is generated. Experimental results demonstrate an improvement of 4\% in accuracy and 3\% in f1-score w.r.t. the general model, along with a 14\% reduction in variability. }
}

@article{241002790v1,
  title={ Raising the Bar(ometer): Identifying a User's Stair and Lift Usage   Through Wearable Sensor Data Analysis },
  author={ Hrishikesh Balkrishna Karande and Ravikiran Arasur Thippeswamy Shivalingappa and Abdelhafid Nassim Yaici and Iman Haghbin and Niravkumar Bavadiya and Robin Burchard and Kristof Van Laerhoven },
  journal={ arXiv preprint arXiv:2410.02790v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.02790v1 },
  abstract={ Many users are confronted multiple times daily with the choice of whether to take the stairs or the elevator. Whereas taking the stairs could be beneficial for cardiovascular health and wellness, taking the elevator might be more convenient but it also consumes energy. By precisely tracking and boosting users' stairs and elevator usage through their wearable, users might gain health insights and motivation, encouraging a healthy lifestyle and lowering the risk of sedentary-related health problems. This research describes a new exploratory dataset, to examine the patterns and behaviors related to using stairs and lifts. We collected data from 20 participants while climbing and descending stairs and taking a lift in a variety of scenarios. The aim is to provide insights and demonstrate the practicality of using wearable sensor data for such a scenario. Our collected dataset was used to train and test a Random Forest machine learning model, and the results show that our method is highly accurate at classifying stair and lift operations with an accuracy of 87.61\% and a multi-class weighted F1-score of 87.56\% over 8-second time windows. Furthermore, we investigate the effect of various types of sensors and data attributes on the model's performance. Our findings show that combining inertial and pressure sensors yields a viable solution for real-time activity detection. }
}

@article{240918987v1,
  title={ Efficient and Personalized Mobile Health Event Prediction via Small   Language Models },
  author={ Xin Wang and Ting Dang and Vassilis Kostakos and Hong Jia },
  journal={ arXiv preprint arXiv:2409.18987v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2409.18987v1 },
  abstract={ Healthcare monitoring is crucial for early detection, timely intervention, and the ongoing management of health conditions, ultimately improving individuals' quality of life. Recent research shows that Large Language Models (LLMs) have demonstrated impressive performance in supporting healthcare tasks. However, existing LLM-based healthcare solutions typically rely on cloud-based systems, which raise privacy concerns and increase the risk of personal information leakage. As a result, there is growing interest in running these models locally on devices like mobile phones and wearables to protect users' privacy. Small Language Models (SLMs) are potential candidates to solve privacy and computational issues, as they are more efficient and better suited for local deployment. However, the performance of SLMs in healthcare domains has not yet been investigated. This paper examines the capability of SLMs to accurately analyze health data, such as steps, calories, sleep minutes, and other vital statistics, to assess an individual's health status. Our results show that, TinyLlama, which has 1.1 billion parameters, utilizes 4.31 GB memory, and has 0.48s latency, showing the best performance compared other four state-of-the-art (SOTA) SLMs on various healthcare applications. Our results indicate that SLMs could potentially be deployed on wearable or mobile devices for real-time health monitoring, providing a practical solution for efficient and privacy-preserving healthcare. }
}

@article{241000020v1,
  title={ Loneliness Forecasting Using Multi-modal Wearable and Mobile Sensing in   Everyday Settings },
  author={ Zhongqi Yang and Iman Azimi and Salar Jafarlou and Sina Labbaf and Brenda Nguyen and Hana Qureshi and Christopher Marcotullio and Jessica L. Borelli and Nikil Dutt and Amir M. Rahmani },
  journal={ arXiv preprint arXiv:2410.00020v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.00020v1 },
  abstract={ The adverse effects of loneliness on both physical and mental well-being are profound. Although previous research has utilized mobile sensing techniques to detect mental health issues, few studies have utilized state-of-the-art wearable devices to forecast loneliness and estimate the physiological manifestations of loneliness and its predictive nature. The primary objective of this study is to examine the feasibility of forecasting loneliness by employing wearable devices, such as smart rings and watches, to monitor early physiological indicators of loneliness. Furthermore, smartphones are employed to capture initial behavioral signs of loneliness. To accomplish this, we employed personalized machine learning techniques, leveraging a comprehensive dataset comprising physiological and behavioral information obtained during our study involving the monitoring of college students. Through the development of personalized models, we achieved a notable accuracy of 0.82 and an F-1 score of 0.82 in forecasting loneliness levels seven days in advance. Additionally, the application of Shapley values facilitated model explainability. The wealth of data provided by this study, coupled with the forecasting methodology employed, possesses the potential to augment interventions and facilitate the early identification of loneliness within populations at risk. }
}

@article{240909549v1,
  title={ COMFORT: A Continual Fine-Tuning Framework for Foundation Models   Targeted at Consumer Healthcare },
  author={ Chia-Hao Li and Niraj K. Jha },
  journal={ arXiv preprint arXiv:2409.09549v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2409.09549v1 },
  abstract={ Wearable medical sensors (WMSs) are revolutionizing smart healthcare by enabling continuous, real-time monitoring of user physiological signals, especially in the field of consumer healthcare. The integration of WMSs and modern machine learning (ML) enables unprecedented solutions to efficient early-stage disease detection. Despite the success of Transformers in various fields, their application to sensitive domains, such as smart healthcare, remains underexplored due to limited data accessibility and privacy concerns. To bridge the gap between Transformer-based foundation models and WMS-based disease detection, we propose COMFORT, a continual fine-tuning framework for foundation models targeted at consumer healthcare. COMFORT introduces a novel approach for pre-training a Transformer-based foundation model on a large dataset of physiological signals exclusively collected from healthy individuals with commercially available WMSs. We adopt a masked data modeling (MDM) objective to pre-train this health foundation model. We then fine-tune the model using various parameter-efficient fine-tuning (PEFT) methods, such as low-rank adaptation (LoRA) and its variants, to adapt it to various downstream disease detection tasks that rely on WMS data. In addition, COMFORT continually stores the low-rank decomposition matrices obtained from the PEFT algorithms to construct a library for multi-disease detection. The COMFORT library enables scalable and memory-efficient disease detection on edge devices. Our experimental results demonstrate that COMFORT achieves highly competitive performance while reducing memory overhead by up to 52\% relative to conventional methods. Thus, COMFORT paves the way for personalized and proactive solutions to efficient and effective early-stage disease detection for consumer healthcare. }
}

@article{240909135v1,
  title={ Multimodal Fusion with LLMs for Engagement Prediction in Natural   Conversation },
  author={ Cheng Charles Ma and Kevin Hyekang Joo and Alexandria K. Vail and Sunreeta Bhattacharya and Álvaro Fernández García and Kailana Baker-Matsuoka and Sheryl Mathew and Lori L. Holt and Fernando De la Torre },
  journal={ arXiv preprint arXiv:2409.09135v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2409.09135v1 },
  abstract={ Over the past decade, wearable computing devices (``smart glasses'') have undergone remarkable advancements in sensor technology, design, and processing power, ushering in a new era of opportunity for high-density human behavior data. Equipped with wearable cameras, these glasses offer a unique opportunity to analyze non-verbal behavior in natural settings as individuals interact. Our focus lies in predicting engagement in dyadic interactions by scrutinizing verbal and non-verbal cues, aiming to detect signs of disinterest or confusion. Leveraging such analyses may revolutionize our understanding of human communication, foster more effective collaboration in professional environments, provide better mental health support through empathetic virtual interactions, and enhance accessibility for those with communication barriers.   In this work, we collect a dataset featuring 34 participants engaged in casual dyadic conversations, each providing self-reported engagement ratings at the end of each conversation. We introduce a novel fusion strategy using Large Language Models (LLMs) to integrate multiple behavior modalities into a ``multimodal transcript'' that can be processed by an LLM for behavioral reasoning tasks. Remarkably, this method achieves performance comparable to established fusion techniques even in its preliminary implementation, indicating strong potential for further research and optimization. This fusion method is one of the first to approach ``reasoning'' about real-world human behavior through a language model. Smart glasses provide us the ability to unobtrusively gather high-density multimodal data on human behavior, paving the way for new approaches to understanding and improving human communication with the potential for important societal benefits. The features and data collected during the studies will be made publicly available to promote further research. }
}

@article{240505611v2,
  title={ Privacy-Preserving Edge Federated Learning for Intelligent Mobile-Health   Systems },
  author={ Amin Aminifar and Matin Shokri and Amir Aminifar },
  journal={ arXiv preprint arXiv:2405.05611v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2405.05611v2 },
  abstract={ Machine Learning (ML) algorithms are generally designed for scenarios in which all data is stored in one data center, where the training is performed. However, in many applications, e.g., in the healthcare domain, the training data is distributed among several entities, e.g., different hospitals or patients' mobile devices/sensors. At the same time, transferring the data to a central location for learning is certainly not an option, due to privacy concerns and legal issues, and in certain cases, because of the communication and computation overheads. Federated Learning (FL) is the state-of-the-art collaborative ML approach for training an ML model across multiple parties holding local data samples, without sharing them. However, enabling learning from distributed data over such edge Internet of Things (IoT) systems (e.g., mobile-health and wearable technologies, involving sensitive personal/medical data) in a privacy-preserving fashion presents a major challenge mainly due to their stringent resource constraints, i.e., limited computing capacity, communication bandwidth, memory storage, and battery lifetime. In this paper, we propose a privacy-preserving edge FL framework for resource-constrained mobile-health and wearable technologies over the IoT infrastructure. We evaluate our proposed framework extensively and provide the implementation of our technique on Amazon's AWS cloud platform based on the seizure detection application in epilepsy monitoring using wearable technologies. }
}

@article{240509906v3,
  title={ Process-based Inference for Spatial Energetics Using Bayesian Predictive   Stacking },
  author={ Tomoya Wakayama and Sudipto Banerjee },
  journal={ arXiv preprint arXiv:2405.09906v3 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2405.09906v3 },
  abstract={ Rapid developments in streaming data technologies have enabled real-time monitoring of human activity that can deliver high-resolution data on health variables over trajectories or paths carved out by subjects as they conduct their daily physical activities. Wearable devices, such as wrist-worn sensors that monitor gross motor activity, have become prevalent and have kindled the emerging field of ''spatial energetics'' in environmental health sciences. We devise a Bayesian inferential framework for analyzing such data while accounting for information available on specific spatial coordinates comprising a trajectory or path using a Global Positioning System (GPS) device embedded within the wearable device. We offer full probabilistic inference with uncertainty quantification using spatial-temporal process models adapted for data generated from ''actigraph'' units as the subject traverses a path or trajectory in their daily routine. Anticipating the need for fast inference for mobile health data, we pursue exact inference using conjugate Bayesian models and employ predictive stacking to assimilate inference across these individual models. This circumvents issues with iterative estimation algorithms such as Markov chain Monte Carlo. We devise Bayesian predictive stacking in this context for models that treat time as discrete epochs and that treat time as continuous. We illustrate our methods with simulation experiments and analysis of data from the Physical Activity through Sustainable Transport Approaches (PASTA-LA) study conducted by the Fielding School of Public Health at the University of California, Los Angeles. }
}

@article{240905627v1,
  title={ ECG Biometric Authentication Using Self-Supervised Learning for IoT Edge   Sensors },
  author={ Guoxin Wang and Shreejith Shanker and Avishek Nag and Yong Lian and Deepu John },
  journal={ arXiv preprint arXiv:2409.05627v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2409.05627v1 },
  abstract={ Wearable Internet of Things (IoT) devices are gaining ground for continuous physiological data acquisition and health monitoring. These physiological signals can be used for security applications to achieve continuous authentication and user convenience due to passive data acquisition. This paper investigates an electrocardiogram (ECG) based biometric user authentication system using features derived from the Convolutional Neural Network (CNN) and self-supervised contrastive learning. Contrastive learning enables us to use large unlabeled datasets to train the model and establish its generalizability. We propose approaches enabling the CNN encoder to extract appropriate features that distinguish the user from other subjects. When evaluated using the PTB ECG database with 290 subjects, the proposed technique achieved an authentication accuracy of 99.15\%. To test its generalizability, we applied the model to two new datasets, the MIT-BIH Arrhythmia Database and the ECG-ID Database, achieving over 98.5\% accuracy without any modifications. Furthermore, we show that repeating the authentication step three times can increase accuracy to nearly 100\% for both PTBDB and ECGIDDB. This paper also presents model optimizations for embedded device deployment, which makes the system more relevant to real-world scenarios. To deploy our model in IoT edge sensors, we optimized the model complexity by applying quantization and pruning. The optimized model achieves 98.67\% accuracy on PTBDB, with 0.48\% accuracy loss and 62.6\% CPU cycles compared to the unoptimized model. An accuracy-vs-time-complexity tradeoff analysis is performed, and results are presented for different optimization levels. }
}

@article{240904723v1,
  title={ NapTune: Efficient Model Tuning for Mood Classification using Previous   Night's Sleep Measures along with Wearable Time-series },
  author={ Debaditya Shome and Nasim Montazeri Ghahjaverestan and Ali Etemad },
  journal={ arXiv preprint arXiv:2409.04723v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2409.04723v1 },
  abstract={ Sleep is known to be a key factor in emotional regulation and overall mental health. In this study, we explore the integration of sleep measures from the previous night into wearable-based mood recognition. To this end, we propose NapTune, a novel prompt-tuning framework that utilizes sleep-related measures as additional inputs to a frozen pre-trained wearable time-series encoder by adding and training lightweight prompt parameters to each Transformer layer. Through rigorous empirical evaluation, we demonstrate that the inclusion of sleep data using NapTune not only improves mood recognition performance across different wearable time-series namely ECG, PPG, and EDA, but also makes it more sample-efficient. Our method demonstrates significant improvements over the best baselines and unimodal variants. Furthermore, we analyze the impact of adding sleep-related measures on recognizing different moods as well as the influence of individual sleep-related measures. }
}

@article{240903296v1,
  title={ An Efficient Two-Dimensional Functional Mixed-Effect Model Framework for   Repeatedly Measured Functional Data },
  author={ Cheng Cao and Jiguo Cao and Hao Pan and Yunting Zhang and Fan Jiang and Xinyue Li },
  journal={ arXiv preprint arXiv:2409.03296v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2409.03296v1 },
  abstract={ With the rapid development of wearable device technologies, accelerometers can record minute-by-minute physical activity for consecutive days, which provides important insight into a dynamic association between the intensity of physical activity and mental health outcomes for large-scale population studies. Using Shanghai school adolescent cohort we estimate the effect of health assessment results on physical activity profiles recorded by accelerometers throughout a week, which is recognized as repeatedly measured functional data. To achieve this goal, we propose an innovative two-dimensional functional mixed-effect model (2dFMM) for the specialized data, which smoothly varies over longitudinal day observations with covariate-dependent mean and covariance functions. The modeling framework characterizes the longitudinal and functional structures while incorporating two-dimensional fixed effects for covariates of interest. We also develop a fast three-stage estimation procedure to provide accurate fixed-effect inference for model interpretability and improve computational efficiency when encountering large datasets. We find strong evidence of intraday and interday varying significant associations between physical activity and mental health assessments among our cohort population, which shed light on possible intervention strategies targeting daily physical activity patterns to improve school adolescent mental health. Our method is also used in environmental data to illustrate the wide applicability. Supplementary materials for this article are available online. }
}

@article{240900093v1,
  title={ Towards Sustainable Personalized On-Device Human Activity Recognition   with TinyML and Cloud-Enabled Auto Deployment },
  author={ Bidyut Saha and Riya Samanta and Soumya K Ghosh and Ram Babu Roy },
  journal={ arXiv preprint arXiv:2409.00093v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2409.00093v1 },
  abstract={ Human activity recognition (HAR) holds immense potential for transforming health and fitness monitoring, yet challenges persist in achieving personalized outcomes and sustainability for on-device continuous inferences. This work introduces a wrist-worn smart band designed to address these challenges through a novel combination of on-device TinyML-driven computing and cloud-enabled auto-deployment. Leveraging inertial measurement unit (IMU) sensors and a customized 1D Convolutional Neural Network (CNN) for personalized HAR, users can tailor activity classes to their unique movement styles with minimal calibration. By utilising TinyML for local computations, the smart band reduces the necessity for constant data transmission and radio communication, which in turn lowers power consumption and reduces carbon footprint. This method also enhances the privacy and security of user data by limiting its transmission. Through transfer learning and fine-tuning on user-specific data, the system achieves a 37\\textbackslash{}\% increase in accuracy over generalized models in personalized settings. Evaluation using three benchmark datasets, WISDM, PAMAP2, and the BandX demonstrates its effectiveness across various activity domains. Additionally, this work presents a cloud-supported framework for the automatic deployment of TinyML models to remote wearables, enabling seamless customization and on-device inference, even with limited target data. By combining personalized HAR with sustainable strategies for on-device continuous inferences, this system represents a promising step towards fostering healthier and more sustainable societies worldwide. }
}

@article{240814190v1,
  title={ Harnessing the Digital Revolution: A Comprehensive Review of mHealth   Applications for Remote Monitoring in Transforming Healthcare Delivery },
  author={ Avnish Singh Jat and Tor-Morten Grønli },
  journal={ arXiv preprint arXiv:2408.14190v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.14190v1 },
  abstract={ The utilization of mHealth applications for remote monitoring has the potential to revolutionize healthcare delivery by enhancing patient outcomes, increasing access to healthcare services, and reducing healthcare costs. This literature review aims to provide a comprehensive overview of the current state of knowledge on mHealth applications for remote monitoring, including their types, benefits, challenges, and limitations, as well as future directions and research gaps. A systematic search of databases such as PubMed, MEDLINE, EMBASE, CINAHL, and Google Scholar was conducted to identify relevant articles published within the last 5 years. Thematic analysis was used to synthesize the findings. The review highlights various types of mHealth applications used for remote monitoring, such as telemedicine platforms, mobile apps for chronic disease management, and wearable devices. The benefits of these applications include improved patient outcomes, increased access to healthcare, reduced healthcare costs, and addressing healthcare disparities. However, challenges and limitations, such as privacy and security concerns, lack of technical infrastructure, regulatory is-sues, data accuracy, user adherence, and the digital divide, need to be addressed to ensure successful adoption and utilization of mHealth applications. Further research is required in areas such as the long-term effects of mHealth applications on patient outcomes, integration of mHealth data with electronic health records, and the development of artificial intelligence-driven mHealth applica-tions. By harnessing the potential of mHealth applications and addressing the existing challenges, healthcare delivery can be transformed towards a more accessible, cost-effective, and patient-centered model. }
}

@article{231113063v3,
  title={ From Classification to Clinical Insights: Towards Analyzing and   Reasoning About Mobile and Behavioral Health Data With Large Language Models },
  author={ Zachary Englhardt and Chengqian Ma and Margaret E. Morris and Xuhai "Orson" Xu and Chun-Cheng Chang and Lianhui Qin and Daniel McDuff and Xin Liu and Shwetak Patel and Vikram Iyer },
  journal={ arXiv preprint arXiv:2311.13063v3 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2311.13063v3 },
  abstract={ Passively collected behavioral health data from ubiquitous sensors holds significant promise to provide mental health professionals insights from patient's daily lives; however, developing analysis tools to use this data in clinical practice requires addressing challenges of generalization across devices and weak or ambiguous correlations between the measured signals and an individual's mental health. To address these challenges, we take a novel approach that leverages large language models (LLMs) to synthesize clinically useful insights from multi-sensor data. We develop chain of thought prompting methods that use LLMs to generate reasoning about how trends in data such as step count and sleep relate to conditions like depression and anxiety. We first demonstrate binary depression classification with LLMs achieving accuracies of 61.1\% which exceed the state of the art. While it is not robust for clinical use, this leads us to our key finding: even more impactful and valued than classification is a new human-AI collaboration approach in which clinician experts interactively query these tools and combine their domain expertise and context about the patient with AI generated reasoning to support clinical decision-making. We find models like GPT-4 correctly reference numerical data 75\% of the time, and clinician participants express strong interest in using this approach to interpret self-tracking data. }
}

@article{240812682v1,
  title={ MultiMed: Massively Multimodal and Multitask Medical Understanding },
  author={ Shentong Mo and Paul Pu Liang },
  journal={ arXiv preprint arXiv:2408.12682v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.12682v1 },
  abstract={ Biomedical data is inherently multimodal, consisting of electronic health records, medical imaging, digital pathology, genome sequencing, wearable sensors, and more. The application of artificial intelligence tools to these multifaceted sensing technologies has the potential to revolutionize the prognosis, diagnosis, and management of human health and disease. However, current approaches to biomedical AI typically only train and evaluate with one or a small set of medical modalities and tasks. This limitation hampers the development of comprehensive tools that can leverage the rich interconnected information across many heterogeneous biomedical sensors. To address this challenge, we present MultiMed, a benchmark designed to evaluate and enable large-scale learning across a wide spectrum of medical modalities and tasks. MultiMed consists of 2.56 million samples across ten medical modalities such as medical reports, pathology, genomics, and protein data, and is structured into eleven challenging tasks, including disease prognosis, protein structure prediction, and medical question answering. Using MultiMed, we conduct comprehensive experiments benchmarking state-of-the-art unimodal, multimodal, and multitask models. Our analysis highlights the advantages of training large-scale medical models across many related modalities and tasks. Moreover, MultiMed enables studies of generalization across related medical concepts, robustness to real-world noisy data and distribution shifts, and novel modality combinations to improve prediction performance. MultiMed will be publicly available and regularly updated and welcomes inputs from the community. }
}

@article{240320183v3,
  title={ HARMamba: Efficient and Lightweight Wearable Sensor Human Activity   Recognition Based on Bidirectional Mamba },
  author={ Shuangjian Li and Tao Zhu and Furong Duan and Liming Chen and Huansheng Ning and Christopher Nugent and Yaping Wan },
  journal={ arXiv preprint arXiv:2403.20183v3 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2403.20183v3 },
  abstract={ Wearable sensor-based human activity recognition (HAR) is a critical research domain in activity perception. However, achieving high efficiency and long sequence recognition remains a challenge. Despite the extensive investigation of temporal deep learning models, such as CNNs, RNNs, and transformers, their extensive parameters often pose significant computational and memory constraints, rendering them less suitable for resource-constrained mobile health applications. This study introduces HARMamba, an innovative light-weight and versatile HAR architecture that combines selective bidirectional State Spaces Model and hardware-aware design. To optimize real-time resource consumption in practical scenarios, HARMamba employs linear recursive mechanisms and parameter discretization, allowing it to selectively focus on relevant input sequences while efficiently fusing scan and recompute operations. The model employs independent channels to process sensor data streams, dividing each channel into patches and appending classification tokens to the end of the sequence. It utilizes position embedding to represent the sequence order. The patch sequence is subsequently processed by HARMamba Block, and the classification head finally outputs the activity category. The HARMamba Block serves as the fundamental component of the HARMamba architecture, enabling the effective capture of more discriminative activity sequence features. HARMamba outperforms contemporary state-of-the-art frameworks, delivering comparable or better accuracy with significantly reducing computational and memory demands. It's effectiveness has been extensively validated on 4 publically available datasets namely PAMAP2, WISDM, UNIMIB SHAR and UCI. The F1 scores of HARMamba on the four datasets are 99.74\%, 99.20\%, 88.23\% and 97.01\%, respectively. }
}

@article{240513996v2,
  title={ Detecting Gait Abnormalities in Foot-Floor Contacts During Walking   Through Footstep-Induced Structural Vibrations },
  author={ Yiwen Dong and Yuyan Wu and Hae Young Noh },
  journal={ arXiv preprint arXiv:2405.13996v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2405.13996v2 },
  abstract={ Gait abnormality detection is critical for the early discovery and progressive tracking of musculoskeletal and neurological disorders, such as Parkinson's and Cerebral Palsy. Especially, analyzing the foot-floor contacts during walking provides important insights into gait patterns, such as contact area, contact force, and contact time, enabling gait abnormality detection through these measurements. Existing studies use various sensing devices to capture such information, including cameras, wearables, and force plates. However, the former two lack force-related information, making it difficult to identify the causes of gait health issues, while the latter has limited coverage of the walking path. In this study, we leverage footstep-induced structural vibrations to infer foot-floor contact profiles and detect gait abnormalities. The main challenge lies in modeling the complex force transfer mechanism between the foot and the floor surfaces, leading to difficulty in reconstructing the force and contact profile during foot-floor interaction using structural vibrations. To overcome the challenge, we first characterize the floor vibration for each contact type (e.g., heel, midfoot, and toe contact) to understand how contact forces and areas affect the induced floor vibration. Then, we leverage the time-frequency response spectrum resulting from those contacts to develop features that are representative of each contact type. Finally, gait abnormalities are detected by comparing the predicted foot-floor contact force and motion with the healthy gait. To evaluate our approach, we conducted a real-world walking experiment with 8 subjects. Our approach achieves 91.6\% and 96.7\% accuracy in predicting contact type and time, respectively, leading to 91.9\% accuracy in detecting various types of gait abnormalities, including asymmetry, dragging, and midfoot/toe contacts. }
}

@article{240801988v1,
  title={ MetaWearS: A Shortcut in Wearable Systems Lifecycle with Only a Few   Shots },
  author={ Alireza Amirshahi and Maedeh H. Toosi and Siamak Mohammadi and Stefano Albini and Pasquale Davide Schiavone and Giovanni Ansaloni and Amir Aminifar and David Atienza },
  journal={ arXiv preprint arXiv:2408.01988v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.01988v1 },
  abstract={ Wearable systems provide continuous health monitoring and can lead to early detection of potential health issues. However, the lifecycle of wearable systems faces several challenges. First, effective model training for new wearable devices requires substantial labeled data from various subjects collected directly by the wearable. Second, subsequent model updates require further extensive labeled data for retraining. Finally, frequent model updating on the wearable device can decrease the battery life in long-term data monitoring. Addressing these challenges, in this paper, we propose MetaWearS, a meta-learning method to reduce the amount of initial data collection required. Moreover, our approach incorporates a prototypical updating mechanism, simplifying the update process by modifying the class prototype rather than retraining the entire model. We explore the performance of MetaWearS in two case studies, namely, the detection of epileptic seizures and the detection of atrial fibrillation. We show that by fine-tuning with just a few samples, we achieve 70\% and 82\% AUC for the detection of epileptic seizures and the detection of atrial fibrillation, respectively. Compared to a conventional approach, our proposed method performs better with up to 45\% AUC. Furthermore, updating the model with only 16 minutes of additional labeled data increases the AUC by up to 5.3\%. Finally, MetaWearS reduces the energy consumption for model updates by 456x and 418x for epileptic seizure and AF detection, respectively. }
}

@article{240801855v1,
  title={ MoodPupilar: Predicting Mood Through Smartphone Detected Pupillary   Responses in Naturalistic Settings },
  author={ Rahul Islam and Tongze Zhang and Priyanshu Singh Bisen and Sang Won Bae },
  journal={ arXiv preprint arXiv:2408.01855v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.01855v1 },
  abstract={ MoodPupilar introduces a novel method for mood evaluation using pupillary response captured by a smartphone's front-facing camera during daily use. Over a four-week period, data was gathered from 25 participants to develop models capable of predicting daily mood averages. Utilizing the GLOBEM behavior modeling platform, we benchmarked the utility of pupillary response as a predictor for mood. Our proposed model demonstrated a Matthew's Correlation Coefficient (MCC) score of 0.15 for Valence and 0.12 for Arousal, which is on par with or exceeds those achieved by existing behavioral modeling algorithms supported by GLOBEM. This capability to accurately predict mood trends underscores the effectiveness of pupillary response data in providing crucial insights for timely mental health interventions and resource allocation. The outcomes are encouraging, demonstrating the potential of real-time and predictive mood analysis to support mental health interventions. }
}

@article{240707196v2,
  title={ Large Language Models for Wearable Sensor-Based Human Activity   Recognition, Health Monitoring, and Behavioral Modeling: A Survey of Early   Trends, Datasets, and Challenges },
  author={ Emilio Ferrara },
  journal={ arXiv preprint arXiv:2407.07196v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2407.07196v2 },
  abstract={ The proliferation of wearable technology enables the generation of vast amounts of sensor data, offering significant opportunities for advancements in health monitoring, activity recognition, and personalized medicine. However, the complexity and volume of this data present substantial challenges in data modeling and analysis, which have been tamed with approaches spanning time series modeling to deep learning techniques. The latest frontier in this domain is the adoption of Large Language Models (LLMs), such as GPT-4 and Llama, for data analysis, modeling, understanding, and generation of human behavior through the lens of wearable sensor data. This survey explores current trends and challenges in applying LLMs for sensor-based human activity recognition and behavior modeling. We discuss the nature of wearable sensors data, the capabilities and limitations of LLMs to model them and their integration with traditional machine learning techniques. We also identify key challenges, including data quality, computational requirements, interpretability, and privacy concerns. By examining case studies and successful applications, we highlight the potential of LLMs in enhancing the analysis and interpretation of wearable sensors data. Finally, we propose future directions for research, emphasizing the need for improved preprocessing techniques, more efficient and scalable models, and interdisciplinary collaboration. This survey aims to provide a comprehensive overview of the intersection between wearable sensors data and LLMs, offering insights into the current state and future prospects of this emerging field. }
}

@article{240610750v2,
  title={ EchoGuide: Active Acoustic Guidance for LLM-Based Eating Event Analysis   from Egocentric Videos },
  author={ Vineet Parikh and Saif Mahmud and Devansh Agarwal and Ke Li and François Guimbretière and Cheng Zhang },
  journal={ arXiv preprint arXiv:2406.10750v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2406.10750v2 },
  abstract={ Self-recording eating behaviors is a step towards a healthy lifestyle recommended by many health professionals. However, the current practice of manually recording eating activities using paper records or smartphone apps is often unsustainable and inaccurate. Smart glasses have emerged as a promising wearable form factor for tracking eating behaviors, but existing systems primarily identify when eating occurs without capturing details of the eating activities (E.g., what is being eaten). In this paper, we present EchoGuide, an application and system pipeline that leverages low-power active acoustic sensing to guide head-mounted cameras to capture egocentric videos, enabling efficient and detailed analysis of eating activities. By combining active acoustic sensing for eating detection with video captioning models and large-scale language models for retrieval augmentation, EchoGuide intelligently clips and analyzes videos to create concise, relevant activity records on eating. We evaluated EchoGuide with 9 participants in naturalistic settings involving eating activities, demonstrating high-quality summarization and significant reductions in video data needed, paving the way for practical, scalable eating activity tracking. }
}

@article{240721665v1,
  title={ A State-of-the-Art Review of Computational Models for Analyzing   Longitudinal Wearable Sensor Data in Healthcare },
  author={ Paula Lago },
  journal={ arXiv preprint arXiv:2407.21665v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2407.21665v1 },
  abstract={ Wearable devices are increasingly used as tools for biomedical research, as the continuous stream of behavioral and physiological data they collect can provide insights about our health in everyday contexts. Long-term tracking, defined in the timescale of months of year, can provide insights of patterns and changes as indicators of health changes. These insights can make medicine and healthcare more predictive, preventive, personalized, and participative (The 4P's). However, the challenges in modeling, understanding and processing longitudinal data are a significant barrier to their adoption in research studies and clinical settings. In this paper, we review and discuss three models used to make sense of longitudinal data: routines, rhythms and stability metrics. We present the challenges associated with the processing and analysis of longitudinal wearable sensor data, with a special focus on how to handle the different temporal dynamics at various granularities. We then discuss current limitations and identify directions for future work. This review is essential to the advancement of computational modeling and analysis of longitudinal sensor data for pervasive healthcare. }
}

@article{240713515v2,
  title={ CookAR: Affordance Augmentations in Wearable AR to Support Kitchen Tool   Interactions for People with Low Vision },
  author={ Jaewook Lee and Andrew D. Tjahjadi and Jiho Kim and Junpu Yu and Minji Park and Jiawen Zhang and Jon E. Froehlich and Yapeng Tian and Yuhang Zhao },
  journal={ arXiv preprint arXiv:2407.13515v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2407.13515v2 },
  abstract={ Cooking is a central activity of daily living, supporting independence as well as mental and physical health. However, prior work has highlighted key barriers for people with low vision (LV) to cook, particularly around safely interacting with tools, such as sharp knives or hot pans. Drawing on recent advancements in computer vision (CV), we present CookAR, a head-mounted AR system with real-time object affordance augmentations to support safe and efficient interactions with kitchen tools. To design and implement CookAR, we collected and annotated the first egocentric dataset of kitchen tool affordances, fine-tuned an affordance segmentation model, and developed an AR system with a stereo camera to generate visual augmentations. To validate CookAR, we conducted a technical evaluation of our fine-tuned model as well as a qualitative lab study with 10 LV participants for suitable augmentation design. Our technical evaluation demonstrates that our model outperforms the baseline on our tool affordance dataset, while our user study indicates a preference for affordance augmentations over the traditional whole object augmentations. }
}

@article{240717666v1,
  title={ Causal estimands and identification of time-varying effects in   non-stationary time series from N-of-1 mobile device data },
  author={ Xiaoxuan Cai and Li Zeng and Charlotte Fowler and Lisa Dixon and Dost Ongur and Justin T. Baker and Jukka-Pekka Onnela and Linda Valeri },
  journal={ arXiv preprint arXiv:2407.17666v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2407.17666v1 },
  abstract={ Mobile technology (mobile phones and wearable devices) generates continuous data streams encompassing outcomes, exposures and covariates, presented as intensive longitudinal or multivariate time series data. The high frequency of measurements enables granular and dynamic evaluation of treatment effect, revealing their persistence and accumulation over time. Existing methods predominantly focus on the contemporaneous effect, temporal-average, or population-average effects, assuming stationarity or invariance of treatment effects over time, which are inadequate both conceptually and statistically to capture dynamic treatment effects in personalized mobile health data. We here propose new causal estimands for multivariate time series in N-of-1 studies. These estimands summarize how time-varying exposures impact outcomes in both short- and long-term. We propose identifiability assumptions and a g-formula estimator that accounts for exposure-outcome and outcome-covariate feedback. The g-formula employs a state space model framework innovatively to accommodate time-varying behavior of treatment effects in non-stationary time series. We apply the proposed method to a multi-year smartphone observational study of bipolar patients and estimate the dynamic effect of phone-based communication on mood of patients with bipolar disorder in an N-of-1 setting. Our approach reveals substantial heterogeneity in treatment effects over time and across individuals. A simulation-based strategy is also proposed for the development of a short-term, dynamic, and personalized treatment recommendation based on patient's past information, in combination with a novel positivity diagnostics plot, validating proper causal inference in time series data. }
}

@article{240715252v1,
  title={ An Adaptive System for Wearable Devices to Detect Stress Using   Physiological Signals },
  author={ Gelei Xu and Ruiyang Qin and Zhi Zheng and Yiyu Shi },
  journal={ arXiv preprint arXiv:2407.15252v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2407.15252v1 },
  abstract={ Timely stress detection is crucial for protecting vulnerable groups from long-term detrimental effects by enabling early intervention. Wearable devices, by collecting real-time physiological signals, offer a solution for accurate stress detection accommodating individual differences. This position paper introduces an adaptive framework for personalized stress detection using PPG and EDA signals. Unlike traditional methods that rely on a generalized model, which may suffer performance drops when applied to new users due to domain shifts, this framework aims to provide each user with a personalized model for higher stress detection accuracy. The framework involves three stages: developing a generalized model offline with an initial dataset, adapting the model to the user's unlabeled data, and fine-tuning it with a small set of labeled data obtained through user interaction. This approach not only offers a foundation for mobile applications that provide personalized stress detection and intervention but also has the potential to address a wider range of mental health issues beyond stress detection using physiological signals. }
}

@article{240708240v1,
  title={ Leveraging LLMs to Predict Affective States via Smartphone Sensor   Features },
  author={ Tianyi Zhang and Songyan Teng and Hong Jia and Simon D'Alfonso },
  journal={ arXiv preprint arXiv:2407.08240v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2407.08240v1 },
  abstract={ As mental health issues for young adults present a pressing public health concern, daily digital mood monitoring for early detection has become an important prospect. An active research area, digital phenotyping, involves collecting and analysing data from personal digital devices such as smartphones (usage and sensors) and wearables to infer behaviours and mental health. Whilst this data is standardly analysed using statistical and machine learning approaches, the emergence of large language models (LLMs) offers a new approach to make sense of smartphone sensing data. Despite their effectiveness across various domains, LLMs remain relatively unexplored in digital mental health, particularly in integrating mobile sensor data. Our study aims to bridge this gap by employing LLMs to predict affect outcomes based on smartphone sensing data from university students. We demonstrate the efficacy of zero-shot and few-shot embedding LLMs in inferring general wellbeing. Our findings reveal that LLMs can make promising predictions of affect measures using solely smartphone sensing data. This research sheds light on the potential of LLMs for affective state prediction, emphasizing the intricate link between smartphone behavioral patterns and affective states. To our knowledge, this is the first work to leverage LLMs for affective state prediction and digital phenotyping tasks. }
}

@article{240705315v1,
  title={ Topological Persistence Guided Knowledge Distillation for Wearable   Sensor Data },
  author={ Eun Som Jeon and Hongjun Choi and Ankita Shukla and Yuan Wang and Hyunglae Lee and Matthew P. Buman and Pavan Turaga },
  journal={ arXiv preprint arXiv:2407.05315v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2407.05315v1 },
  abstract={ Deep learning methods have achieved a lot of success in various applications involving converting wearable sensor data to actionable health insights. A common application areas is activity recognition, where deep-learning methods still suffer from limitations such as sensitivity to signal quality, sensor characteristic variations, and variability between subjects. To mitigate these issues, robust features obtained by topological data analysis (TDA) have been suggested as a potential solution. However, there are two significant obstacles to using topological features in deep learning: (1) large computational load to extract topological features using TDA, and (2) different signal representations obtained from deep learning and TDA which makes fusion difficult. In this paper, to enable integration of the strengths of topological methods in deep-learning for time-series data, we propose to use two teacher networks, one trained on the raw time-series data, and another trained on persistence images generated by TDA methods. The distilled student model utilizes only the raw time-series data at test-time. This approach addresses both issues. The use of KD with multiple teachers utilizes complementary information, and results in a compact model with strong supervisory features and an integrated richer representation. To assimilate desirable information from different modalities, we design new constraints, including orthogonality imposed on feature correlation maps for improving feature expressiveness and allowing the student to easily learn from the teacher. Also, we apply an annealing strategy in KD for fast saturation and better accommodation from different features, while the knowledge gap between the teachers and student is reduced. Finally, a robust student model is distilled, which uses only the time-series data as an input, while implicitly preserving topological features. }
}

@article{240619716v1,
  title={ Functional Time Transformation Model with Applications to Digital Health },
  author={ Rahul Ghosal and Marcos Matabuena and Sujit K. Ghosh },
  journal={ arXiv preprint arXiv:2406.19716v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2406.19716v1 },
  abstract={ The advent of wearable and sensor technologies now leads to functional predictors which are intrinsically infinite dimensional. While the existing approaches for functional data and survival outcomes lean on the well-established Cox model, the proportional hazard (PH) assumption might not always be suitable in real-world applications. Motivated by physiological signals encountered in digital medicine, we develop a more general and flexible functional time-transformation model for estimating the conditional survival function with both functional and scalar covariates. A partially functional regression model is used to directly model the survival time on the covariates through an unknown monotone transformation and a known error distribution. We use Bernstein polynomials to model the monotone transformation function and the smooth functional coefficients. A sieve method of maximum likelihood is employed for estimation. Numerical simulations illustrate a satisfactory performance of the proposed method in estimation and inference. We demonstrate the application of the proposed model through two case studies involving wearable data i) Understanding the association between diurnal physical activity pattern and all-cause mortality based on accelerometer data from the National Health and Nutrition Examination Survey (NHANES) 2011-2014 and ii) Modelling Time-to-Hypoglycemia events in a cohort of diabetic patients based on distributional representation of continuous glucose monitoring (CGM) data. The results provide important epidemiological insights into the direct association between survival times and the physiological signals and also exhibit superior predictive performance compared to traditional summary based biomarkers in the CGM study. }
}

@article{240619283v1,
  title={ PhysioLLM: Supporting Personalized Health Insights with Wearables and   Large Language Models },
  author={ Cathy Mengying Fang and Valdemar Danry and Nathan Whitmore and Andria Bao and Andrew Hutchison and Cayden Pierce and Pattie Maes },
  journal={ arXiv preprint arXiv:2406.19283v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2406.19283v1 },
  abstract={ We present PhysioLLM, an interactive system that leverages large language models (LLMs) to provide personalized health understanding and exploration by integrating physiological data from wearables with contextual information. Unlike commercial health apps for wearables, our system offers a comprehensive statistical analysis component that discovers correlations and trends in user data, allowing users to ask questions in natural language and receive generated personalized insights, and guides them to develop actionable goals. As a case study, we focus on improving sleep quality, given its measurability through physiological data and its importance to general well-being. Through a user study with 24 Fitbit watch users, we demonstrate that PhysioLLM outperforms both the Fitbit App alone and a generic LLM chatbot in facilitating a deeper, personalized understanding of health data and supporting actionable steps toward personal health goals. }
}

@article{240618848v1,
  title={ Temporally Multi-Scale Sparse Self-Attention for Physical Activity Data   Imputation },
  author={ Hui Wei and Maxwell A. Xu and Colin Samplawski and James M. Rehg and Santosh Kumar and Benjamin M. Marlin },
  journal={ arXiv preprint arXiv:2406.18848v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2406.18848v1 },
  abstract={ Wearable sensors enable health researchers to continuously collect data pertaining to the physiological state of individuals in real-world settings. However, such data can be subject to extensive missingness due to a complex combination of factors. In this work, we study the problem of imputation of missing step count data, one of the most ubiquitous forms of wearable sensor data. We construct a novel and large scale data set consisting of a training set with over 3 million hourly step count observations and a test set with over 2.5 million hourly step count observations. We propose a domain knowledge-informed sparse self-attention model for this task that captures the temporal multi-scale nature of step-count data. We assess the performance of the model relative to baselines and conduct ablation studies to verify our specific model designs. }
}

@article{230616297v2,
  title={ A Meta-Learning Method for Estimation of Causal Excursion Effects to   Assess Time-Varying Moderation },
  author={ Jieru Shi and Walter Dempsey },
  journal={ arXiv preprint arXiv:2306.16297v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2306.16297v2 },
  abstract={ Twin revolutions in wearable technologies and health interventions delivered by smartphones have greatly increased the accessibility of mobile health (mHealth) interventions. Micro-randomized trials (MRTs) are designed to assess the effectiveness of the mHealth intervention and introduce a novel class of causal estimands called ''causal excursion effects.'' These estimands enable the evaluation of how intervention effects change over time and are influenced by individual characteristics or context. However, existing analysis methods for causal excursion effects require prespecified features of the observed high-dimensional history to build a working model for a critical nuisance parameter. Machine learning appears ideal for automatic feature construction, but their naive application can lead to bias under model misspecification. To address this issue, this paper revisits the estimation of causal excursion effects from a meta-learner perspective, where the analyst remains agnostic to the supervised learning algorithms used to estimate nuisance parameters. We present the bidirectional asymptotic properties of the proposed estimators and compare them both theoretically and through extensive simulations. The results show relative efficiency gains and support the suggestion of a doubly robust alternative to existing methods. Finally, the proposed methods' practical utilities are demonstrated by analyzing data from a multi-institution cohort of first-year medical residents in the United States (NeCamp et al., 2020). }
}

@article{240616252v2,
  title={ Graph-Augmented LLMs for Personalized Health Insights: A Case Study in   Sleep Analysis },
  author={ Ajan Subramanian and Zhongqi Yang and Iman Azimi and Amir M. Rahmani },
  journal={ arXiv preprint arXiv:2406.16252v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2406.16252v2 },
  abstract={ Health monitoring systems have revolutionized modern healthcare by enabling the continuous capture of physiological and behavioral data, essential for preventive measures and early health intervention. While integrating this data with Large Language Models (LLMs) has shown promise in delivering interactive health advice, traditional methods like Retrieval-Augmented Generation (RAG) and fine-tuning often fail to fully utilize the complex, multi-dimensional, and temporally relevant data from wearable devices. These conventional approaches typically provide limited actionable and personalized health insights due to their inadequate capacity to dynamically integrate and interpret diverse health data streams. In response, this paper introduces a graph-augmented LLM framework designed to significantly enhance the personalization and clarity of health insights. Utilizing a hierarchical graph structure, the framework captures inter and intra-patient relationships, enriching LLM prompts with dynamic feature importance scores derived from a Random Forest Model. The effectiveness of this approach is demonstrated through a sleep analysis case study involving 20 college students during the COVID-19 lockdown, highlighting the potential of our model to generate actionable and personalized health insights efficiently. We leverage another LLM to evaluate the insights for relevance, comprehensiveness, actionability, and personalization, addressing the critical need for models that process and interpret complex health data effectively. Our findings show that augmenting prompts with our framework yields significant improvements in all 4 criteria. Through our framework, we can elicit well-crafted, more thoughtful responses tailored to a specific patient. }
}

@article{240510979v2,
  title={ Private Data Leakage in Federated Human Activity Recognition for   Wearable Healthcare Devices },
  author={ Kongyang Chen and Dongping Zhang and Sijia Guan and Bing Mi and Jiaxing Shen and Guoqing Wang },
  journal={ arXiv preprint arXiv:2405.10979v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2405.10979v2 },
  abstract={ Wearable data serves various health monitoring purposes, such as determining activity states based on user behavior and providing tailored exercise recommendations. However, the individual data perception and computational capabilities of wearable devices are limited, often necessitating the joint training of models across multiple devices. Federated Human Activity Recognition (HAR) presents a viable research avenue, allowing for global model training without the need to upload users' local activity data. Nonetheless, recent studies have revealed significant privacy concerns persisting within federated learning frameworks. To address this gap, we focus on investigating privacy leakage issues within federated user behavior recognition modeling across multiple wearable devices. Our proposed system entails a federated learning architecture comprising \$N\$ wearable device users and a parameter server, which may exhibit curiosity in extracting sensitive user information from model parameters. Consequently, we consider a membership inference attack based on a malicious server, leveraging differences in model generalization across client data. Experimentation conducted on five publicly available HAR datasets demonstrates an accuracy rate of 92\\textbackslash{}\% for malicious server-based membership inference. Our study provides preliminary evidence of substantial privacy risks associated with federated training across multiple wearable devices, offering a novel research perspective within this domain. }
}

@article{230915375v4,
  title={ PPG-to-ECG Signal Translation for Continuous Atrial Fibrillation   Detection via Attention-based Deep State-Space Modeling },
  author={ Khuong Vo and Mostafa El-Khamy and Yoojin Choi },
  journal={ arXiv preprint arXiv:2309.15375v4 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2309.15375v4 },
  abstract={ Photoplethysmography (PPG) is a cost-effective and non-invasive technique that utilizes optical methods to measure cardiac physiology. PPG has become increasingly popular in health monitoring and is used in various commercial and clinical wearable devices. Compared to electrocardiography (ECG), PPG does not provide substantial clinical diagnostic value, despite the strong correlation between the two. Here, we propose a subject-independent attention-based deep state-space model (ADSSM) to translate PPG signals to corresponding ECG waveforms. The model is not only robust to noise but also data-efficient by incorporating probabilistic prior knowledge. To evaluate our approach, 55 subjects' data from the MIMIC-III database were used in their original form, and then modified with noise, mimicking real-world scenarios. Our approach was proven effective as evidenced by the PR-AUC of 0.986 achieved when inputting the translated ECG signals into an existing atrial fibrillation (AFib) detector. ADSSM enables the integration of ECG's extensive knowledge base and PPG's continuous measurement for early diagnosis of cardiovascular disease. }
}

@article{240508969v2,
  title={ Wearable Sensor-Based Few-Shot Continual Learning on Hand Gestures for   Motor-Impaired Individuals via Latent Embedding Exploitation },
  author={ Riyad Bin Rafiq and Weishi Shi and Mark V. Albert },
  journal={ arXiv preprint arXiv:2405.08969v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2405.08969v2 },
  abstract={ Hand gestures can provide a natural means of human-computer interaction and enable people who cannot speak to communicate efficiently. Existing hand gesture recognition methods heavily depend on pre-defined gestures, however, motor-impaired individuals require new gestures tailored to each individual's gesture motion and style. Gesture samples collected from different persons have distribution shifts due to their health conditions, the severity of the disability, motion patterns of the arms, etc. In this paper, we introduce the Latent Embedding Exploitation (LEE) mechanism in our replay-based Few-Shot Continual Learning (FSCL) framework that significantly improves the performance of fine-tuning a model for out-of-distribution data. Our method produces a diversified latent feature space by leveraging a preserved latent embedding known as gesture prior knowledge, along with intra-gesture divergence derived from two additional embeddings. Thus, the model can capture latent statistical structure in highly variable gestures with limited samples. We conduct an experimental evaluation using the SmartWatch Gesture and the Motion Gesture datasets. The proposed method results in an average test accuracy of 57.0\%, 64.6\%, and 69.3\% by using one, three, and five samples for six different gestures. Our method helps motor-impaired persons leverage wearable devices, and their unique styles of movement can be learned and applied in human-computer interaction and social communication. Code is available at: https://github.com/riyadRafiq/wearable-latent-embedding-exploitation }
}

@article{240606464v2,
  title={ Transforming Wearable Data into Health Insights using Large Language   Model Agents },
  author={ Mike A. Merrill and Akshay Paruchuri and Naghmeh Rezaei and Geza Kovacs and Javier Perez and Yun Liu and Erik Schenck and Nova Hammerquist and Jake Sunshine and Shyam Tailor and Kumar Ayush and Hao-Wei Su and Qian He and Cory Y. McLean and Mark Malhotra and Shwetak Patel and Jiening Zhan and Tim Althoff and Daniel McDuff and Xin Liu },
  journal={ arXiv preprint arXiv:2406.06464v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2406.06464v2 },
  abstract={ Despite the proliferation of wearable health trackers and the importance of sleep and exercise to health, deriving actionable personalized insights from wearable data remains a challenge because doing so requires non-trivial open-ended analysis of these data. The recent rise of large language model (LLM) agents, which can use tools to reason about and interact with the world, presents a promising opportunity to enable such personalized analysis at scale. Yet, the application of LLM agents in analyzing personal health is still largely untapped. In this paper, we introduce the Personal Health Insights Agent (PHIA), an agent system that leverages state-of-the-art code generation and information retrieval tools to analyze and interpret behavioral health data from wearables. We curate two benchmark question-answering datasets of over 4000 health insights questions. Based on 650 hours of human and expert evaluation we find that PHIA can accurately address over 84\% of factual numerical questions and more than 83\% of crowd-sourced open-ended questions. This work has implications for advancing behavioral health across the population, potentially enabling individuals to interpret their own wearable data, and paving the way for a new era of accessible, personalized wellness regimens that are informed by data-driven insights. }
}

@article{240606474v1,
  title={ Towards a Personal Health Large Language Model },
  author={ Justin Cosentino and Anastasiya Belyaeva and Xin Liu and Nicholas A. Furlotte and Zhun Yang and Chace Lee and Erik Schenck and Yojan Patel and Jian Cui and Logan Douglas Schneider and Robby Bryant and Ryan G. Gomes and Allen Jiang and Roy Lee and Yun Liu and Javier Perez and Jameson K. Rogers and Cathy Speed and Shyam Tailor and Megan Walker and Jeffrey Yu and Tim Althoff and Conor Heneghan and John Hernandez and Mark Malhotra and Leor Stern and Yossi Matias and Greg S. Corrado and Shwetak Patel and Shravya Shetty and Jiening Zhan and Shruthi Prabhakara and Daniel McDuff and Cory Y. McLean },
  journal={ arXiv preprint arXiv:2406.06474v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2406.06474v1 },
  abstract={ In health, most large language model (LLM) research has focused on clinical tasks. However, mobile and wearable devices, which are rarely integrated into such tasks, provide rich, longitudinal data for personal health monitoring. Here we present Personal Health Large Language Model (PH-LLM), fine-tuned from Gemini for understanding and reasoning over numerical time-series personal health data. We created and curated three datasets that test 1) production of personalized insights and recommendations from sleep patterns, physical activity, and physiological responses, 2) expert domain knowledge, and 3) prediction of self-reported sleep outcomes. For the first task we designed 857 case studies in collaboration with domain experts to assess real-world scenarios in sleep and fitness. Through comprehensive evaluation of domain-specific rubrics, we observed that Gemini Ultra 1.0 and PH-LLM are not statistically different from expert performance in fitness and, while experts remain superior for sleep, fine-tuning PH-LLM provided significant improvements in using relevant domain knowledge and personalizing information for sleep insights. We evaluated PH-LLM domain knowledge using multiple choice sleep medicine and fitness examinations. PH-LLM achieved 79\% on sleep and 88\% on fitness, exceeding average scores from a sample of human experts. Finally, we trained PH-LLM to predict self-reported sleep quality outcomes from textual and multimodal encoding representations of wearable data, and demonstrate that multimodal encoding is required to match performance of specialized discriminative models. Although further development and evaluation are necessary in the safety-critical personal health domain, these results demonstrate both the broad knowledge and capabilities of Gemini models and the benefit of contextualizing physiological data for personal health applications as done with PH-LLM. }
}

@article{240503393v2,
  title={ On-site scale factor linearity calibration of MEMS triaxial gyroscopes },
  author={ Yaqi Li and Li Wang and Zhitao Wang and Xiangqing Li and Jiaojiao Li and Steven Weidong Su },
  journal={ arXiv preprint arXiv:2405.03393v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2405.03393v2 },
  abstract={ The calibration of MEMS triaxial gyroscopes is crucial for achieving precise attitude estimation for various wearable health monitoring applications. However, gyroscope calibration poses greater challenges compared to accelerometers and magnetometers. This paper introduces an efficient method for calibrating MEMS triaxial gyroscopes via only a servo motor, making it well-suited for field environments. The core strategy of the method involves utilizing the fact that the dot product of the measured gravity and the rotational speed in a fixed frame remains constant. To eliminate the influence of rotating centrifugal force on the accelerometer, the accelerometer data is measured while stationary. The proposed calibration experiment scheme, which allows gyroscopic measurements when operating each axis at a specific rotation speed, making it easier to evaluate the linearity across a related speed range constituted by a series of rotation speeds. Moreover, solely the classical least squares algorithm proves adequate for estimating the scale factor, notably streamlining the analysis of the calibration process. Extensive numerical simulations were conducted to analyze the proposed method's performance in calibrating a triaxial gyroscope model. Experimental validation was also carried out using a commercially available MEMS inertial measurement unit (LSM9DS1 from Arduino nano 33 BLE SENSE) and a servo motor capable of controlling precise speed. The experimental results effectively demonstrate the efficacy of the proposed calibration approach. }
}

@article{240600848v1,
  title={ Eating Smart: Advancing Health Informatics with the Grounding DINO based   Dietary Assistant App },
  author={ Abdelilah Nossair and Hamza El Housni },
  journal={ arXiv preprint arXiv:2406.00848v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2406.00848v1 },
  abstract={ The Smart Dietary Assistant utilizes Machine Learning to provide personalized dietary advice, focusing on users with conditions like diabetes. This app leverages the Grounding DINO model, which combines a text encoder and image backbone to enhance food item detection without requiring a labeled dataset. With an AP score of 52.5 on the COCO dataset, the model demonstrates high accuracy in real-world scenarios, utilizing attention mechanisms to precisely recognize objects based on user-provided labels and images. Developed using React Native and TypeScript, the app operates seamlessly across multiple platforms and integrates a self-hosted PostgreSQL database, ensuring data integrity and enhancing user privacy. Key functionalities include personalized nutrition profiles, real-time food scanning, and health insights, facilitating informed dietary choices for health management and lifestyle optimization. Future developments aim to integrate wearable technologies for more tailored health recommendations. Keywords: Food Image Recognition, Machine Learning in Nutrition, Zero-Shot Object Detection }
}

@article{231112392v4,
  title={ Individualized Dynamic Latent Factor Model for Multi-resolutional Data   with Application to Mobile Health },
  author={ Jiuchen Zhang and Fei Xue and Qi Xu and Jung-Ah Lee and Annie Qu },
  journal={ arXiv preprint arXiv:2311.12392v4 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2311.12392v4 },
  abstract={ Mobile health has emerged as a major success for tracking individual health status, due to the popularity and power of smartphones and wearable devices. This has also brought great challenges in handling heterogeneous, multi-resolution data which arise ubiquitously in mobile health due to irregular multivariate measurements collected from individuals. In this paper, we propose an individualized dynamic latent factor model for irregular multi-resolution time series data to interpolate unsampled measurements of time series with low resolution. One major advantage of the proposed method is the capability to integrate multiple irregular time series and multiple subjects by mapping the multi-resolution data to the latent space. In addition, the proposed individualized dynamic latent factor model is applicable to capturing heterogeneous longitudinal information through individualized dynamic latent factors. Our theory provides a bound on the integrated interpolation error and the convergence rate for B-spline approximation methods. Both the simulation studies and the application to smartwatch data demonstrate the superior performance of the proposed method compared to existing methods. }
}

@article{240516395v1,
  title={ Daily Physical Activity Monitoring -- Adaptive Learning from   Multi-source Motion Sensor Data },
  author={ Haoting Zhang and Donglin Zhan and Yunduan Lin and Jinghai He and Qing Zhu and Zuo-Jun Max Shen and Zeyu Zheng },
  journal={ arXiv preprint arXiv:2405.16395v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2405.16395v1 },
  abstract={ In healthcare applications, there is a growing need to develop machine learning models that use data from a single source, such as that from a wrist wearable device, to monitor physical activities, assess health risks, and provide immediate health recommendations or interventions. However, the limitation of using single-source data often compromises the model's accuracy, as it fails to capture the full scope of human activities. While a more comprehensive dataset can be gathered in a lab setting using multiple sensors attached to various body parts, this approach is not practical for everyday use due to the impracticality of wearing multiple sensors. To address this challenge, we introduce a transfer learning framework that optimizes machine learning models for everyday applications by leveraging multi-source data collected in a laboratory setting. We introduce a novel metric to leverage the inherent relationship between these multiple data sources, as they are all paired to capture aspects of the same physical activity. Through numerical experiments, our framework outperforms existing methods in classification accuracy and robustness to noise, offering a promising avenue for the enhancement of daily activity monitoring. }
}

@article{240515085v1,
  title={ Acoustical Features as Knee Health Biomarkers: A Critical Analysis },
  author={ Christodoulos Kechris and Jerome Thevenot and Tomas Teijeiro and Vincent A. Stadelmann and Nicola A. Maffiuletti and David Atienza },
  journal={ arXiv preprint arXiv:2405.15085v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2405.15085v1 },
  abstract={ Acoustical knee health assessment has long promised an alternative to clinically available medical imaging tools, but this modality has yet to be adopted in medical practice. The field is currently led by machine learning models processing acoustical features, which have presented promising diagnostic performances. However, these methods overlook the intricate multi-source nature of audio signals and the underlying mechanisms at play. By addressing this critical gap, the present paper introduces a novel causal framework for validating knee acoustical features. We argue that current machine learning methodologies for acoustical knee diagnosis lack the required assurances and thus cannot be used to classify acoustic features as biomarkers. Our framework establishes a set of essential theoretical guarantees necessary to validate this claim. We apply our methodology to three real-world experiments investigating the effect of researchers' expectations, the experimental protocol and the wearable employed sensor. This investigation reveals latent issues such as underlying shortcut learning and performance inflation. This study is the first independent result reproduction study in the field of acoustical knee health evaluation. We conclude with actionable insights from our findings, offering valuable guidance to navigate these crucial limitations in future research. }
}

@article{230304719v2,
  title={ Soft insoles for estimating 3D ground reaction forces using 3D printed   foam-like sensors },
  author={ Nick Willemstein and Saivimal Sridar and Herman van der Kooij and Ali Sadeghi },
  journal={ arXiv preprint arXiv:2303.04719v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2303.04719v2 },
  abstract={ Sensorized insoles provide a tool for gait studies and health monitoring during daily life. For users to accept such insoles they need to be comfortable and lightweight. Previous work has already demonstrated that estimation of ground reaction forces (GRFs) is possible with insoles. However, these are often assemblies of commercial components restricting design freedom and customization. Within this work, we investigate using four 3D-printed soft foam-like sensors to sensorize an insole. These sensors were combined with system identification of Hammerstein-Wiener models to estimate the 3D GRFs, which were compared to values from an instrumented treadmill as the golden standard. It was observed that the four sensors behaved in line with the expected change in pressure distribution during the gait cycle. In addition, the identified (personalized) Hammerstein-Wiener models showed the best estimation performance (on average RMS error 9.3\%, R\^{}2=0.85 and mean absolute error (MAE) 7\%) of the vertical, mediolateral, and anteroposterior GRFs. Thereby showing that these sensors can estimate the resulting 3D force reasonably well. These results for nine participants were comparable to or outperformed other works that used commercial FSRs with machine learning. The identified models did decrease in estimation performance over time but stayed on average 11.35\% RMS and 8.6\% MAE after a week with the Hammerstein-Wiener model seeming consistent between days two and seven. These results show promise for using 3D-printed soft piezoresistive foam-like sensors with system identification to be a viable approach for applications that require softness, lightweight, and customization such as wearable (force) sensors. }
}

@article{230605285v2,
  title={ Unsupervised Statistical Feature-Guided Diffusion Model for Sensor-based   Human Activity Recognition },
  author={ Si Zuo and Vitor Fortes Rey and Sungho Suh and Stephan Sigg and Paul Lukowicz },
  journal={ arXiv preprint arXiv:2306.05285v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2306.05285v2 },
  abstract={ Human activity recognition (HAR) from on-body sensors is a core functionality in many AI applications: from personal health, through sports and wellness to Industry 4.0. A key problem holding up progress in wearable sensor-based HAR, compared to other ML areas, such as computer vision, is the unavailability of diverse and labeled training data. Particularly, while there are innumerable annotated images available in online repositories, freely available sensor data is sparse and mostly unlabeled. We propose an unsupervised statistical feature-guided diffusion model specifically optimized for wearable sensor-based human activity recognition with devices such as inertial measurement unit (IMU) sensors. The method generates synthetic labeled time-series sensor data without relying on annotated training data. Thereby, it addresses the scarcity and annotation difficulties associated with real-world sensor data. By conditioning the diffusion model on statistical information such as mean, standard deviation, Z-score, and skewness, we generate diverse and representative synthetic sensor data. We conducted experiments on public human activity recognition datasets and compared the method to conventional oversampling and state-of-the-art generative adversarial network methods. Experimental results demonstrate that this can improve the performance of human activity recognition and outperform existing techniques. }
}

@article{240113327v2,
  title={ Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable   Stress Detection },
  author={ Lucas Lange and Nils Wenzlitschke and Erhard Rahm },
  journal={ arXiv preprint arXiv:2401.13327v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2401.13327v2 },
  abstract={ Smartwatch health sensor data are increasingly utilized in smart health applications and patient monitoring, including stress detection. However, such medical data often comprise sensitive personal information and are resource-intensive to acquire for research purposes. In response to this challenge, we introduce the privacy-aware synthetization of multi-sensor smartwatch health readings related to moments of stress, employing Generative Adversarial Networks (GANs) and Differential Privacy (DP) safeguards. Our method not only protects patient information but also enhances data availability for research. To ensure its usefulness, we test synthetic data from multiple GANs and employ different data enhancement strategies on an actual stress detection task. Our GAN-based augmentation methods demonstrate significant improvements in model performance, with private DP training scenarios observing an 11.90-15.48\% increase in F1-score, while non-private training scenarios still see a 0.45\% boost. These results underline the potential of differentially private synthetic data in optimizing utility-privacy trade-offs, especially with the limited availability of real training samples. Through rigorous quality assessments, we confirm the integrity and plausibility of our synthetic data, which, however, are significantly impacted when increasing privacy requirements. }
}

@article{240106076v4,
  title={ Wireless Ear EEG to Monitor Drowsiness },
  author={ Ryan Kaveh and Carolyn Schwendeman and Leslie Pu and Ana C. Arias and Rikky Muller },
  journal={ arXiv preprint arXiv:2401.06076v4 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2401.06076v4 },
  abstract={ Neural wearables can enable life-saving drowsiness and health monitoring for pilots and drivers. While existing in-cabin sensors may provide alerts, wearables can enable monitoring across more environments. Current neural wearables are promising but most require wet-electrodes and bulky electronics. This work showcases in-ear, dry-electrode earpieces used to monitor drowsiness with compact hardware. The employed system integrates additive-manufacturing for dry, user-generic earpieces, existing wireless electronics, and offline classification algorithms. Thirty-five hours of electrophysiological data were recorded across nine subjects performing drowsiness-inducing tasks. Three classifier models were trained with user-specific, leave-one-trial-out, and leave-one-user-out splits. The support-vector-machine classifier achieved an accuracy of 93.2\% while evaluating users it has seen before and 93.3\% when evaluating a never-before-seen user. These results demonstrate wireless, dry, user-generic earpieces used to classify drowsiness with comparable accuracies to existing state-of-the-art, wet electrode in-ear and scalp systems. Further, this work illustrates the feasibility of population-trained classification in future electrophysiological applications. }
}

@article{240418976v1,
  title={ Foundations of Multisensory Artificial Intelligence },
  author={ Paul Pu Liang },
  journal={ arXiv preprint arXiv:2404.18976v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2404.18976v1 },
  abstract={ Building multisensory AI systems that learn from multiple sensory inputs such as text, speech, video, real-world sensors, wearable devices, and medical data holds great promise for impact in many scientific areas with practical benefits, such as in supporting human health and well-being, enabling multimedia content processing, and enhancing real-world autonomous agents. By synthesizing a range of theoretical frameworks and application domains, this thesis aims to advance the machine learning foundations of multisensory AI. In the first part, we present a theoretical framework formalizing how modalities interact with each other to give rise to new information for a task. These interactions are the basic building blocks in all multimodal problems, and their quantification enables users to understand their multimodal datasets, design principled approaches to learn these interactions, and analyze whether their model has succeeded in learning. In the second part, we study the design of practical multimodal foundation models that generalize over many modalities and tasks, which presents a step toward grounding large language models to real-world sensory modalities. We introduce MultiBench, a unified large-scale benchmark across a wide range of modalities, tasks, and research areas, followed by the cross-modal attention and multimodal transformer architectures that now underpin many of today's multimodal foundation models. Scaling these architectures on MultiBench enables the creation of general-purpose multisensory AI systems, and we discuss our collaborative efforts in applying these models for real-world impact in affective computing, mental health, cancer prognosis, and robotics. Finally, we conclude this thesis by discussing how future work can leverage these ideas toward more general, interactive, and safe multisensory AI. }
}

@article{240106866v2,
  title={ Health-LLM: Large Language Models for Health Prediction via Wearable   Sensor Data },
  author={ Yubin Kim and Xuhai Xu and Daniel McDuff and Cynthia Breazeal and Hae Won Park },
  journal={ arXiv preprint arXiv:2401.06866v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2401.06866v2 },
  abstract={ Large language models (LLMs) are capable of many natural language tasks, yet they are far from perfect. In health applications, grounding and interpreting domain-specific and non-linguistic data is crucial. This paper investigates the capacity of LLMs to make inferences about health based on contextual information (e.g. user demographics, health knowledge) and physiological data (e.g. resting heart rate, sleep minutes). We present a comprehensive evaluation of 12 state-of-the-art LLMs with prompting and fine-tuning techniques on four public health datasets (PMData, LifeSnaps, GLOBEM and AW\_FB). Our experiments cover 10 consumer health prediction tasks in mental health, activity, metabolic, and sleep assessment. Our fine-tuned model, HealthAlpaca exhibits comparable performance to much larger models (GPT-3.5, GPT-4 and Gemini-Pro), achieving the best performance in 8 out of 10 tasks. Ablation studies highlight the effectiveness of context enhancement strategies. Notably, we observe that our context enhancement can yield up to 23.8\% improvement in performance. While constructing contextually rich prompts (combining user context, health knowledge and temporal information) exhibits synergistic improvement, the inclusion of health knowledge context in prompts significantly enhances overall performance. }
}

@article{240417391v1,
  title={ M3BAT: Unsupervised Domain Adaptation for Multimodal Mobile Sensing with   Multi-Branch Adversarial Training },
  author={ Lakmal Meegahapola and Hamza Hassoune and Daniel Gatica-Perez },
  journal={ arXiv preprint arXiv:2404.17391v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2404.17391v1 },
  abstract={ Over the years, multimodal mobile sensing has been used extensively for inferences regarding health and well being, behavior, and context. However, a significant challenge hindering the widespread deployment of such models in real world scenarios is the issue of distribution shift. This is the phenomenon where the distribution of data in the training set differs from the distribution of data in the real world, the deployment environment. While extensively explored in computer vision and natural language processing, and while prior research in mobile sensing briefly addresses this concern, current work primarily focuses on models dealing with a single modality of data, such as audio or accelerometer readings, and consequently, there is little research on unsupervised domain adaptation when dealing with multimodal sensor data. To address this gap, we did extensive experiments with domain adversarial neural networks (DANN) showing that they can effectively handle distribution shifts in multimodal sensor data. Moreover, we proposed a novel improvement over DANN, called M3BAT, unsupervised domain adaptation for multimodal mobile sensing with multi-branch adversarial training, to account for the multimodality of sensor data during domain adaptation with multiple branches. Through extensive experiments conducted on two multimodal mobile sensing datasets, three inference tasks, and 14 source-target domain pairs, including both regression and classification, we demonstrate that our approach performs effectively on unseen domains. Compared to directly deploying a model trained in the source domain to the target domain, the model shows performance increases up to 12\% AUC (area under the receiver operating characteristics curves) on classification tasks, and up to 0.13 MAE (mean absolute error) on regression tasks. }
}

@article{230512624v2,
  title={ Scalable regression calibration approaches to correcting measurement   error in multi-level generalized functional linear regression models with   heteroscedastic measurement errors },
  author={ Yuanyuan Luan and Roger S. Zoh and Erjia Cui and Xue Lan and Sneha Jadhav and Carmen D. Tekwe },
  journal={ arXiv preprint arXiv:2305.12624v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2305.12624v2 },
  abstract={ Wearable devices permit the continuous monitoring of biological processes, such as blood glucose metabolism, and behavior, such as sleep quality and physical activity. The continuous monitoring often occurs in epochs of 60 seconds over multiple days, resulting in high dimensional longitudinal curves that are best described and analyzed as functional data. From this perspective, the functional data are smooth, latent functions obtained at discrete time intervals and prone to homoscedastic white noise. However, the assumption of homoscedastic errors might not be appropriate in this setting because the devices collect the data serially. While researchers have previously addressed measurement error in scalar covariates prone to errors, less work has been done on correcting measurement error in high dimensional longitudinal curves prone to heteroscedastic errors. We present two new methods for correcting measurement error in longitudinal functional curves prone to complex measurement error structures in multi-level generalized functional linear regression models. These methods are based on two-stage scalable regression calibration. We assume that the distribution of the scalar responses and the surrogate measures prone to heteroscedastic errors both belong in the exponential family and that the measurement errors follow Gaussian processes. In simulations and sensitivity analyses, we established some finite sample properties of these methods. In our simulations, both regression calibration methods for correcting measurement error performed better than estimators based on averaging the longitudinal functional data and using observations from a single day. We also applied the methods to assess the relationship between physical activity and type 2 diabetes in community dwelling adults in the United States who participated in the National Health and Nutrition Examination Survey. }
}

@article{240410092v1,
  title={ Integration of Federated Learning and Blockchain in Healthcare: A   Tutorial },
  author={ Yahya Shahsavari and Oussama A. Dambri and Yaser Baseri and Abdelhakim Senhaji Hafid and Dimitrios Makrakis },
  journal={ arXiv preprint arXiv:2404.10092v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2404.10092v1 },
  abstract={ Wearable devices and medical sensors revolutionize health monitoring, raising concerns about data privacy in ML for healthcare. This tutorial explores FL and BC integration, offering a secure and privacy-preserving approach to healthcare analytics. FL enables decentralized model training on local devices at healthcare institutions, keeping patient data localized. This facilitates collaborative model development without compromising privacy. However, FL introduces vulnerabilities. BC, with its tamper-proof ledger and smart contracts, provides a robust framework for secure collaborative learning in FL. After presenting a taxonomy for the various types of data used in ML in medical applications, and a concise review of ML techniques for healthcare use cases, this tutorial explores three integration architectures for balancing decentralization, scalability, and reliability in healthcare data. Furthermore, it investigates how BCFL enhances data security and collaboration in disease prediction, medical image analysis, patient monitoring, and drug discovery. By providing a tutorial on FL, blockchain, and their integration, along with a review of BCFL applications, this paper serves as a valuable resource for researchers and practitioners seeking to leverage these technologies for secure and privacy-preserving healthcare ML. It aims to accelerate advancements in secure and collaborative healthcare analytics, ultimately improving patient outcomes. }
}

@article{240415353v1,
  title={ SQUWA: Signal Quality Aware DNN Architecture for Enhanced Accuracy in   Atrial Fibrillation Detection from Noisy PPG Signals },
  author={ Runze Yan and Cheng Ding and Ran Xiao and Aleksandr Fedorov and Randall J Lee and Fadi Nahab and Xiao Hu },
  journal={ arXiv preprint arXiv:2404.15353v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2404.15353v1 },
  abstract={ Atrial fibrillation (AF), a common cardiac arrhythmia, significantly increases the risk of stroke, heart disease, and mortality. Photoplethysmography (PPG) offers a promising solution for continuous AF monitoring, due to its cost efficiency and integration into wearable devices. Nonetheless, PPG signals are susceptible to corruption from motion artifacts and other factors often encountered in ambulatory settings. Conventional approaches typically discard corrupted segments or attempt to reconstruct original signals, allowing for the use of standard machine learning techniques. However, this reduces dataset size and introduces biases, compromising prediction accuracy and the effectiveness of continuous monitoring. We propose a novel deep learning model, Signal Quality Weighted Fusion of Attentional Convolution and Recurrent Neural Network (SQUWA), designed to learn how to retain accurate predictions from partially corrupted PPG. Specifically, SQUWA innovatively integrates an attention mechanism that directly considers signal quality during the learning process, dynamically adjusting the weights of time series segments based on their quality. This approach enhances the influence of higher-quality segments while reducing that of lower-quality ones, effectively utilizing partially corrupted segments. This approach represents a departure from the conventional methods that exclude such segments, enabling the utilization of a broader range of data, which has great implications for less disruption when monitoring of AF risks and more accurate estimation of AF burdens. Our extensive experiments show that SQUWA outperform existing PPG-based models, achieving the highest AUCPR of 0.89 with label noise mitigation. This also exceeds the 0.86 AUCPR of models trained with using both electrocardiogram (ECG) and PPG data. }
}

@article{230613674v3,
  title={ MeciFace: Mechanomyography and Inertial Fusion-based Glasses for Edge   Real-Time Recognition of Facial and Eating Activities },
  author={ Hymalai Bello and Sungho Suh and Bo Zhou and Paul Lukowicz },
  journal={ arXiv preprint arXiv:2306.13674v3 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2306.13674v3 },
  abstract={ The increasing prevalence of stress-related eating behaviors and their impact on overall health highlights the importance of effective and ubiquitous monitoring systems. In this paper, we present MeciFace, an innovative wearable technology designed to monitor facial expressions and eating activities in real-time on-the-edge (RTE). MeciFace aims to provide a low-power, privacy-conscious, and highly accurate tool for promoting healthy eating behaviors and stress management. We employ lightweight convolutional neural networks as backbone models for facial expression and eating monitoring scenarios. The MeciFace system ensures efficient data processing with a tiny memory footprint, ranging from 11KB to 19 KB. During RTE evaluation, the system achieves an F1-score of < 86\% for facial expression recognition and 94\% for eating/drinking monitoring, for the RTE of unseen users (user-independent case). }
}

@article{240317219v2,
  title={ SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental   Health Sensing Studies },
  author={ Akshat Choube and Vedant Das Swain and Varun Mishra },
  journal={ arXiv preprint arXiv:2403.17219v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2403.17219v2 },
  abstract={ Advances in mobile and wearable technologies have enabled the potential to passively monitor a person's mental, behavioral, and affective health. These approaches typically rely on longitudinal collection of self-reported outcomes, e.g., depression, stress, and anxiety, to train machine learning (ML) models. However, the need to continuously self-report adds a significant burden on the participants, often resulting in attrition, missing labels, or insincere responses. In this work, we introduce the Scale Scores Simulation using Mental Models (SeSaMe) framework to alleviate participants' burden in digital mental health studies. By leveraging pre-trained large language models (LLMs), SeSaMe enables the simulation of participants' responses on psychological scales. In SeSaMe, researchers can prompt LLMs with information on participants' internal behavioral dispositions, enabling LLMs to construct mental models of participants to simulate their responses on psychological scales. We demonstrate an application of SeSaMe, where we use GPT-4 to simulate responses on one scale using responses from another as behavioral information. We also evaluate the alignment between human and SeSaMe-simulated responses to psychological scales. Then, we present experiments to inspect the utility of SeSaMe-simulated responses as ground truth in training ML models by replicating established depression and anxiety screening tasks from a previous study. Our results indicate SeSaMe to be a promising approach, but its alignment may vary across scales and specific prediction objectives. We also observed that model performance with simulated data was on par with using the real data for training in most evaluation scenarios. We conclude by discussing the potential implications of SeSaMe in addressing some challenges researchers face with ground-truth collection in passive sensing studies. }
}

@article{240415294v1,
  title={ Multimodal Physical Fitness Monitoring (PFM) Framework Based on   TimeMAE-PFM in Wearable Scenarios },
  author={ Junjie Zhang and Zheming Zhang and Huachen Xiang and Yangquan Tan and Linnan Huo and Fengyi Wang },
  journal={ arXiv preprint arXiv:2404.15294v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2404.15294v1 },
  abstract={ Physical function monitoring (PFM) plays a crucial role in healthcare especially for the elderly. Traditional assessment methods such as the Short Physical Performance Battery (SPPB) have failed to capture the full dynamic characteristics of physical function. Wearable sensors such as smart wristbands offer a promising solution to this issue. However, challenges exist, such as the computational complexity of machine learning methods and inadequate information capture. This paper proposes a multi-modal PFM framework based on an improved TimeMAE, which compresses time-series data into a low-dimensional latent space and integrates a self-enhanced attention module. This framework achieves effective monitoring of physical health, providing a solution for real-time and personalized assessment. The method is validated using the NHATS dataset, and the results demonstrate an accuracy of 70.6\% and an AUC of 82.20\%, surpassing other state-of-the-art time-series classification models. }
}

@article{230907133v2,
  title={ Assessing cognitive function among older adults using machine learning   and wearable device data: a feasibility study },
  author={ Collin Sakal and Tingyou Li and Juan Li and Xinyue Li },
  journal={ arXiv preprint arXiv:2309.07133v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2309.07133v2 },
  abstract={ Timely implementation of interventions to slow cognitive decline among older adults requires accurate monitoring to detect changes in cognitive function. Data gathered using wearable devices that can continuously monitor factors known to be associated with cognition could be used to train machine learning models and develop wearable-based cognitive monitoring systems. Using data from over 2,400 older adults in the National Health and Nutrition Examination Survey (NHANES) we developed prediction models to differentiate older adults with normal cognition from those with poor cognition based on outcomes from three cognitive tests measuring different domains of cognitive function. During repeated cross-validation, CatBoost, XGBoost, and Random Forest models performed best when predicting cognition based on processing speed, working memory, and attention (median AUCs >0.82) compared to immediate and delayed recall (median AUCs >0.72) and categorical verbal fluency (median AUC >0.68). Activity and sleep parameters were also more strongly associated with processing speed, working memory, and attention compared to other cognitive subdomains. Our work provides proof of concept that wearable-based cognitive monitoring systems may be a viable alternative to traditional methods for monitoring processing speeds, working memory, and attention. We further identified novel metrics that could be targets in future causal studies seeking to better understand how sleep and activity parameters influence cognitive function among older adults. }
}

@article{240313841v2,
  title={ Integrating Wearable Sensor Data and Self-reported Diaries for   Personalized Affect Forecasting },
  author={ Zhongqi Yang and Yuning Wang and Ken S. Yamashita and Maryam Sabah and Elahe Khatibi and Iman Azimi and Nikil Dutt and Jessica L. Borelli and Amir M. Rahmani },
  journal={ arXiv preprint arXiv:2403.13841v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2403.13841v2 },
  abstract={ Emotional states, as indicators of affect, are pivotal to overall health, making their accurate prediction before onset crucial. Current studies are primarily centered on immediate short-term affect detection using data from wearable and mobile devices. These studies typically focus on objective sensory measures, often neglecting other forms of self-reported information like diaries and notes. In this paper, we propose a multimodal deep learning model for affect status forecasting. This model combines a transformer encoder with a pre-trained language model, facilitating the integrated analysis of objective metrics and self-reported diaries. To validate our model, we conduct a longitudinal study, enrolling college students and monitoring them over a year, to collect an extensive dataset including physiological, environmental, sleep, metabolic, and physical activity parameters, alongside open-ended textual diaries provided by the participants. Our results demonstrate that the proposed model achieves predictive accuracy of 82.50\% for positive affect and 82.76\% for negative affect, a full week in advance. The effectiveness of our model is further elevated by its explainability. }
}

@article{240312323v1,
  title={ Enhanced Detection of Transdermal Alcohol Levels Using Hyperdimensional   Computing on Embedded Devices },
  author={ Manuel E. Segura and Pere Verges and Justin Tian Jin Chen and Ramesh Arangott and Angela Kristine Garcia and Laura Garcia Reynoso and Alexandru Nicolau and Tony Givargis and Sergio Gago-Masague },
  journal={ arXiv preprint arXiv:2403.12323v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2403.12323v1 },
  abstract={ Alcohol consumption has a significant impact on individuals' health, with even more pronounced consequences when consumption becomes excessive. One approach to promoting healthier drinking habits is implementing just-in-time interventions, where timely notifications indicating intoxication are sent during heavy drinking episodes. However, the complexity or invasiveness of an intervention mechanism may deter an individual from using them in practice. Previous research tackled this challenge using collected motion data and conventional Machine Learning (ML) algorithms to classify heavy drinking episodes, but with impractical accuracy and computational efficiency for mobile devices. Consequently, we have elected to use Hyperdimensional Computing (HDC) to design a just-in-time intervention approach that is practical for smartphones, smart wearables, and IoT deployment. HDC is a framework that has proven results in processing real-time sensor data efficiently. This approach offers several advantages, including low latency, minimal power consumption, and high parallelism. We explore various HDC encoding designs and combine them with various HDC learning models to create an optimal and feasible approach for mobile devices. Our findings indicate an accuracy rate of 89\\textbackslash{}\%, which represents a substantial 12\\textbackslash{}\% improvement over the current state-of-the-art. }
}

@article{231205409v2,
  title={ Large-scale Training of Foundation Models for Wearable Biosignals },
  author={ Salar Abbaspourazad and Oussama Elachqar and Andrew C. Miller and Saba Emrani and Udhyakumar Nallasamy and Ian Shapiro },
  journal={ arXiv preprint arXiv:2312.05409v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2312.05409v2 },
  abstract={ Tracking biosignals is crucial for monitoring wellness and preempting the development of severe medical conditions. Today, wearable devices can conveniently record various biosignals, creating the opportunity to monitor health status without disruption to one's daily routine. Despite widespread use of wearable devices and existing digital biomarkers, the absence of curated data with annotated medical labels hinders the development of new biomarkers to measure common health conditions. In fact, medical datasets are usually small in comparison to other domains, which is an obstacle for developing neural network models for biosignals. To address this challenge, we have employed self-supervised learning using the unlabeled sensor data collected under informed consent from the large longitudinal Apple Heart and Movement Study (AHMS) to train foundation models for two common biosignals: photoplethysmography (PPG) and electrocardiogram (ECG) recorded on Apple Watch. We curated PPG and ECG datasets from AHMS that include data from \~{}141K participants spanning \~{}3 years. Our self-supervised learning framework includes participant level positive pair selection, stochastic augmentation module and a regularized contrastive loss optimized with momentum training, and generalizes well to both PPG and ECG modalities. We show that the pre-trained foundation models readily encode information regarding participants' demographics and health conditions. To the best of our knowledge, this is the first study that builds foundation models using large-scale PPG and ECG data collected via wearable consumer devices \$\\textbackslash{}unicode\{x2013\}\$ prior works have commonly used smaller-size datasets collected in clinical and experimental settings. We believe PPG and ECG foundation models can enhance future wearable devices by reducing the reliance on labeled data and hold the potential to help the users improve their health. }
}

@article{240303274v1,
  title={ From Noise to Signal: Unveiling Treatment Effects from Digital Health   Data through Pharmacology-Informed Neural-SDE },
  author={ Samira Pakravan and Nikolaos Evangelou and Maxime Usdin and Logan Brooks and James Lu },
  journal={ arXiv preprint arXiv:2403.03274v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2403.03274v1 },
  abstract={ Digital health technologies (DHT), such as wearable devices, provide personalized, continuous, and real-time monitoring of patient. These technologies are contributing to the development of novel therapies and personalized medicine. Gaining insight from these technologies requires appropriate modeling techniques to capture clinically-relevant changes in disease state. The data generated from these devices is characterized by being stochastic in nature, may have missing elements, and exhibits considerable inter-individual variability - thereby making it difficult to analyze using traditional longitudinal modeling techniques. We present a novel pharmacology-informed neural stochastic differential equation (SDE) model capable of addressing these challenges. Using synthetic data, we demonstrate that our approach is effective in identifying treatment effects and learning causal relationships from stochastic data, thereby enabling counterfactual simulation. }
}

@article{240301000v1,
  title={ A linear mixed model approach for measurement error adjustment:   applications to sedentary behavior assessment from wearable devices },
  author={ Ruohui Chen and Dori Rosenberg and Chongzhi Di and Rong Zablocki and Sheri J Hartman and Andrea Lacroix and Xin Tu and Loki Natarajan and Lin Liu },
  journal={ arXiv preprint arXiv:2403.01000v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2403.01000v1 },
  abstract={ In recent years, wearable devices have become more common to capture a wide range of health behaviors, especially for physical activity and sedentary behavior. These sensor-based measures are deemed to be objective and thus less prone to self-reported biases, inherent in questionnaire assessments. While this is undoubtedly a major advantage, there can still be measurement errors from the device recordings, which pose serious challenges for conducting statistical analysis and obtaining unbiased risk estimates. There is a vast literature proposing statistical methods for adjusting for measurement errors in self-reported behaviors, such as in dietary intake. However, there is much less research on error correction for sensor-based device measures, especially sedentary behavior. In this paper, we address this gap. Exploiting the excessive multiple-day assessments typically collected when sensor devices are deployed, we propose a two-stage linear mixed effect model (LME) based approach to correct bias caused by measurement errors. We provide theoretical proof of the debiasing process using the Best Linear Unbiased Predictors (BLUP), and use both simulation and real data from a cohort study to demonstrate the performance of the proposed approach while comparing to the na\\textbackslash{}''ive plug-in approach that directly uses device measures without appropriately adjusting measurement errors. Our results indicate that employing our easy-to-implement BLUP correction method can greatly reduce biases in disease risk estimates and thus enhance the validity of study findings. }
}

@article{240205698v1,
  title={ Evolving AI for Wellness: Dynamic and Personalized Real-time Loneliness   Detection Using Passive Sensing },
  author={ Malik Muhammad Qirtas and Evi Zafeiridi and Eleanor Bantry White and Dirk Pesch },
  journal={ arXiv preprint arXiv:2402.05698v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2402.05698v1 },
  abstract={ Loneliness is a growing health concern as it can lead to depression and other associated mental health problems for people who experience feelings of loneliness over prolonged periods of time. Utilizing passive sensing methods that use smartphone and wearable sensor data to capture daily behavioural patterns offers a promising approach for the early detection of loneliness. Given the subjective nature of loneliness and people's varying daily routines, past detection approaches using machine learning models often face challenges with effectively detecting loneliness. This paper proposes a methodologically novel approach, particularly developing a loneliness detection system that evolves over time, adapts to new data, and provides real-time detection. Our study utilized the Globem dataset, a comprehensive collection of passive sensing data acquired over 10 weeks from university students. The base of our approach is the continuous identification and refinement of similar behavioural groups among students using an incremental clustering method. As we add new data, the model improves based on changing behavioural patterns. Parallel to this, we create and update classification models to detect loneliness among the evolving behavioural groups of students. When unique behavioural patterns are observed among student data, specialized classification models have been created. For predictions of loneliness, a collaborative effort between the generalized and specialized models is employed, treating each prediction as a vote. This study's findings reveal that group-based loneliness detection models exhibit superior performance compared to generic models, underscoring the necessity for more personalized approaches tailored to specific behavioural patterns. These results pave the way for future research, emphasizing the development of finely-tuned, individualized mental health interventions. }
}

@article{230714385v4,
  title={ Mental-LLM: Leveraging Large Language Models for Mental Health   Prediction via Online Text Data },
  author={ Xuhai Xu and Bingsheng Yao and Yuanzhe Dong and Saadia Gabriel and Hong Yu and James Hendler and Marzyeh Ghassemi and Anind K. Dey and Dakuo Wang },
  journal={ arXiv preprint arXiv:2307.14385v4 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2307.14385v4 },
  abstract={ Advances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work, we present a comprehensive evaluation of multiple LLMs on various mental health prediction tasks via online text data, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9\% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8\%. They further perform on par with the state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs' capability on mental health reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs' capability for mental health tasks. Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research. }
}

@article{240111113v2,
  title={ SleepNet: Attention-Enhanced Robust Sleep Prediction using Dynamic   Social Networks },
  author={ Maryam Khalid and Elizabeth B. Klerman and Andrew W. Mchill and Andrew J. K. Phillips and Akane Sano },
  journal={ arXiv preprint arXiv:2401.11113v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2401.11113v2 },
  abstract={ Sleep behavior significantly impacts health and acts as an indicator of physical and mental well-being. Monitoring and predicting sleep behavior with ubiquitous sensors may therefore assist in both sleep management and tracking of related health conditions. While sleep behavior depends on, and is reflected in the physiology of a person, it is also impacted by external factors such as digital media usage, social network contagion, and the surrounding weather. In this work, we propose SleepNet, a system that exploits social contagion in sleep behavior through graph networks and integrates it with physiological and phone data extracted from ubiquitous mobile and wearable devices for predicting next-day sleep labels about sleep duration. Our architecture overcomes the limitations of large-scale graphs containing connections irrelevant to sleep behavior by devising an attention mechanism. The extensive experimental evaluation highlights the improvement provided by incorporating social networks in the model. Additionally, we conduct robustness analysis to demonstrate the system's performance in real-life conditions. The outcomes affirm the stability of SleepNet against perturbations in input data. Further analyses emphasize the significance of network topology in prediction performance revealing that users with higher eigenvalue centrality are more vulnerable to data perturbations. }
}

@article{240114107v1,
  title={ Learning under Label Noise through Few-Shot Human-in-the-Loop Refinement },
  author={ Aaqib Saeed and Dimitris Spathis and Jungwoo Oh and Edward Choi and Ali Etemad },
  journal={ arXiv preprint arXiv:2401.14107v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2401.14107v1 },
  abstract={ Wearable technologies enable continuous monitoring of various health metrics, such as physical activity, heart rate, sleep, and stress levels. A key challenge with wearable data is obtaining quality labels. Unlike modalities like video where the videos themselves can be effectively used to label objects or events, wearable data do not contain obvious cues about the physical manifestation of the users and usually require rich metadata. As a result, label noise can become an increasingly thorny issue when labeling such data. In this paper, we propose a novel solution to address noisy label learning, entitled Few-Shot Human-in-the-Loop Refinement (FHLR). Our method initially learns a seed model using weak labels. Next, it fine-tunes the seed model using a handful of expert corrections. Finally, it achieves better generalizability and robustness by merging the seed and fine-tuned models via weighted parameter averaging. We evaluate our approach on four challenging tasks and datasets, and compare it against eight competitive baselines designed to deal with noisy labels. We show that FHLR achieves significantly better performance when learning from noisy labels and achieves state-of-the-art by a large margin, with up to 19\% accuracy improvement under symmetric and asymmetric noise. Notably, we find that FHLR is particularly robust to increased label noise, unlike prior works that suffer from severe performance degradation. Our work not only achieves better generalization in high-stakes health sensing benchmarks but also sheds light on how noise affects commonly-used models. }
}

@article{240113722v1,
  title={ Proactive Emotion Tracker: AI-Driven Continuous Mood and Emotion   Monitoring },
  author={ Mohammad Asif and Sudhakar Mishra and Ankush Sonker and Sanidhya Gupta and Somesh Kumar Maurya and Uma Shanker Tiwary },
  journal={ arXiv preprint arXiv:2401.13722v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2401.13722v1 },
  abstract={ This research project aims to tackle the growing mental health challenges in today's digital age. It employs a modified pre-trained BERT model to detect depressive text within social media and users' web browsing data, achieving an impressive 93\% test accuracy. Simultaneously, the project aims to incorporate physiological signals from wearable devices, such as smartwatches and EEG sensors, to provide long-term tracking and prognosis of mood disorders and emotional states. This comprehensive approach holds promise for enhancing early detection of depression and advancing overall mental health outcomes. }
}

@article{240107421v1,
  title={ A Bayesian Approach to Modeling Variance of Intensive Longitudinal   Biomarker Data as a Predictor of Health Outcomes },
  author={ Mingyan Yu and Zhenke Wu and Margaret Hicken and Michael R. Elliott },
  journal={ arXiv preprint arXiv:2401.07421v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2401.07421v1 },
  abstract={ Intensive longitudinal biomarker data are increasingly common in scientific studies that seek temporally granular understanding of the role of behavioral and physiological factors in relation to outcomes of interest. Intensive longitudinal biomarker data, such as those obtained from wearable devices, are often obtained at a high frequency typically resulting in several hundred to thousand observations per individual measured over minutes, hours, or days. Often in longitudinal studies, the primary focus is on relating the means of biomarker trajectories to an outcome, and the variances are treated as nuisance parameters, although they may also be informative for the outcomes. In this paper, we propose a Bayesian hierarchical model to jointly model a cross-sectional outcome and the intensive longitudinal biomarkers. To model the variability of biomarkers and deal with the high intensity of data, we develop subject-level cubic B-splines and allow the sharing of information across individuals for both the residual variability and the random effects variability. Then different levels of variability are extracted and incorporated into an outcome submodel for inferential and predictive purposes. We demonstrate the utility of the proposed model via an application involving bio-monitoring of hertz-level heart rate information from a study on social stress. }
}

@article{231020331v2,
  title={ Energy-Aware Adaptive Sampling for Self-Sustainability in   Resource-Constrained IoT Devices },
  author={ Marco Giordano and Silvano Cortesi and Prodromos-Vasileios Mekikis and Michele Crabolu and Giovanni Bellusci and Michele Magno },
  journal={ arXiv preprint arXiv:2310.20331v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2310.20331v2 },
  abstract={ In the ever-growing Internet of Things (IoT) landscape, smart power management algorithms combined with energy harvesting solutions are crucial to obtain self-sustainability. This paper presents an energy-aware adaptive sampling rate algorithm designed for embedded deployment in resource-constrained, battery-powered IoT devices. The algorithm, based on a finite state machine (FSM) and inspired by Transmission Control Protocol (TCP) Reno's additive increase and multiplicative decrease, maximizes sensor sampling rates, ensuring power self-sustainability without risking battery depletion. Moreover, we characterized our solar cell with data acquired over 48 days and used the model created to obtain energy data from an open-source world-wide dataset. To validate our approach, we introduce the EcoTrack device, a versatile device with global navigation satellite system (GNSS) capabilities and Long-Term Evolution Machine Type Communication (LTE-M) connectivity, supporting MQTT protocol for cloud data relay. This multi-purpose device can be used, for instance, as a health and safety wearable, remote hazard monitoring system, or as a global asset tracker. The results, validated on data from three different European cities, show that the proposed algorithm enables self-sustainability while maximizing sampled locations per day. In experiments conducted with a 3000 mAh battery capacity, the algorithm consistently maintained a minimum of 24 localizations per day and achieved peaks of up to 3000. }
}

@article{231211226v1,
  title={ CDRH Seeks Public Comment: Digital Health Technologies for Detecting   Prediabetes and Undiagnosed Type 2 Diabetes },
  author={ Manuel Cossio },
  journal={ arXiv preprint arXiv:2312.11226v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2312.11226v1 },
  abstract={ This document provides responses to the FDA's request for public comments (Docket No FDA 2023 N 4853) on the role of digital health technologies (DHTs) in detecting prediabetes and undiagnosed type 2 diabetes. It explores current DHT applications in prevention, detection, treatment and reversal of prediabetes, highlighting AI chatbots, online forums, wearables and mobile apps. The methods employed by DHTs to capture health signals like glucose, diet, symptoms and community insights are outlined. Key subpopulations that could benefit most from remote screening tools include rural residents, minority groups, high-risk individuals and those with limited healthcare access. Capturable high-impact risk factors encompass glycemic variability, cardiovascular parameters, respiratory health, blood biomarkers and patient reported symptoms. An array of non-invasive monitoring tools are discussed, although further research into their accuracy for diverse groups is warranted. Extensive health datasets providing immense opportunities for AI and ML based risk modeling are presented. Promising techniques leveraging EHRs, imaging, wearables and surveys to enhance screening through AI and ML algorithms are showcased. Analysis of social media and streaming data further allows disease prediction across populations. Ongoing innovation focused on inclusivity and accessibility is highlighted as pivotal in unlocking DHTs potential for transforming prediabetes and diabetes prevention and care. }
}

@article{221207514v2,
  title={ PulseImpute: A Novel Benchmark Task for Pulsative Physiological Signal   Imputation },
  author={ Maxwell A. Xu and Alexander Moreno and Supriya Nagesh and V. Burak Aydemir and David W. Wetter and Santosh Kumar and James M. Rehg },
  journal={ arXiv preprint arXiv:2212.07514v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2212.07514v2 },
  abstract={ The promise of Mobile Health (mHealth) is the ability to use wearable sensors to monitor participant physiology at high frequencies during daily life to enable temporally-precise health interventions. However, a major challenge is frequent missing data. Despite a rich imputation literature, existing techniques are ineffective for the pulsative signals which comprise many mHealth applications, and a lack of available datasets has stymied progress. We address this gap with PulseImpute, the first large-scale pulsative signal imputation challenge which includes realistic mHealth missingness models, an extensive set of baselines, and clinically-relevant downstream tasks. Our baseline models include a novel transformer-based architecture designed to exploit the structure of pulsative signals. We hope that PulseImpute will enable the ML community to tackle this significant and challenging task. }
}

@article{240105367v1,
  title={ Context-Aware Stress Monitoring using Wearable and Mobile Technologies   in Everyday Settings },
  author={ Seyed Amir Hossein Aqajari and Sina Labbaf and Phuc Hoang Tran and Brenda Nguyen and Milad Asgari Mehrabadi and Marco Levorato and Nikil Dutt and Amir M. Rahmani },
  journal={ arXiv preprint arXiv:2401.05367v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2401.05367v1 },
  abstract={ Daily monitoring of stress is a critical component of maintaining optimal physical and mental health. Physiological signals and contextual information have recently emerged as promising indicators for detecting instances of heightened stress. Nonetheless, developing a real-time monitoring system that utilizes both physiological and contextual data to anticipate stress levels in everyday settings while also gathering stress labels from participants represents a significant challenge. We present a monitoring system that objectively tracks daily stress levels by utilizing both physiological and contextual data in a daily-life environment. Additionally, we have integrated a smart labeling approach to optimize the ecological momentary assessment (EMA) collection, which is required for building machine learning models for stress detection. We propose a three-tier Internet-of-Things-based system architecture to address the challenges. We utilized a cross-validation technique to accurately estimate the performance of our stress models. We achieved the F1-score of 70\\textbackslash{}\% with a Random Forest classifier using both PPG and contextual data, which is considered an acceptable score in models built for everyday settings. Whereas using PPG data alone, the highest F1-score achieved is approximately 56\\textbackslash{}\%, emphasizing the significance of incorporating both PPG and contextual data in stress detection tasks. }
}

@article{231202953v1,
  title={ Longitudinal Assessment of Seasonal Impacts and Depression Associations   on Circadian Rhythm Using Multimodal Wearable Sensing },
  author={ Yuezhou Zhang and Amos A Folarin and Shaoxiong Sun and Nicholas Cummins and Yatharth Ranjan and Zulqarnain Rashid and Callum Stewart and Pauline Conde and Heet Sankesara and Petroula Laiou and Faith Matcham and Katie M White and Carolin Oetzmann and Femke Lamers and Sara Siddi and Sara Simblett and Srinivasan Vairavan and Inez Myin-Germeys and David C. Mohr and Til Wykes and Josep Maria Haro and Peter Annas and Brenda WJH Penninx and Vaibhav A Narayan and Matthew Hotopf and Richard JB Dobson and RADAR-CNS consortium },
  journal={ arXiv preprint arXiv:2312.02953v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2312.02953v1 },
  abstract={ Objective: This study aimed to explore the associations between depression severity and wearable-measured circadian rhythms, accounting for seasonal impacts and quantifying seasonal changes in circadian rhythms.Materials and Methods: Data used in this study came from a large longitudinal mobile health study. Depression severity (measured biweekly using the 8-item Patient Health Questionnaire [PHQ-8]) and behaviors (monitored by Fitbit) were tracked for up to two years. Twelve features were extracted from Fitbit recordings to approximate circadian rhythms. Three nested linear mixed-effects models were employed for each feature: (1) incorporating the PHQ-8 score as an independent variable; (2) adding the season variable; and (3) adding an interaction term between season and the PHQ-8 score. Results: This study analyzed 10,018 PHQ-8 records with Fitbit data from 543 participants. Upon adjusting for seasonal effects, higher PHQ-8 scores were associated with reduced activity, irregular behaviors, and delayed rhythms. Notably, the negative association with daily step counts was stronger in summer and spring than in winter, and the positive association with the onset of the most active continuous 10-hour period was significant only during summer. Furthermore, participants had shorter and later sleep, more activity, and delayed circadian rhythms in summer compared to winter. Discussion and Conclusions: Our findings underscore the significant seasonal impacts on human circadian rhythms and their associations with depression and indicate that wearable-measured circadian rhythms have the potential to be the digital biomarkers of depression. }
}

@article{231202300v1,
  title={ Reconsideration on evaluation of machine learning models in continuous   monitoring using wearables },
  author={ Cheng Ding and Zhicheng Guo and Cynthia Rudin and Ran Xiao and Fadi B Nahab and Xiao Hu },
  journal={ arXiv preprint arXiv:2312.02300v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2312.02300v1 },
  abstract={ This paper explores the challenges in evaluating machine learning (ML) models for continuous health monitoring using wearable devices beyond conventional metrics. We state the complexities posed by real-world variability, disease dynamics, user-specific characteristics, and the prevalence of false notifications, necessitating novel evaluation strategies. Drawing insights from large-scale heart studies, the paper offers a comprehensive guideline for robust ML model evaluation on continuous health monitoring. }
}

@article{220705784v4,
  title={ Distilled Non-Semantic Speech Embeddings with Binary Neural Networks for   Low-Resource Devices },
  author={ Harlin Lee and Aaqib Saeed },
  journal={ arXiv preprint arXiv:2207.05784v4 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2207.05784v4 },
  abstract={ This work introduces BRILLsson, a novel binary neural network-based representation learning model for a broad range of non-semantic speech tasks. We train the model with knowledge distillation from a large and real-valued TRILLsson model with only a fraction of the dataset used to train TRILLsson. The resulting BRILLsson models are only 2MB in size with a latency less than 8ms, making them suitable for deployment in low-resource devices such as wearables. We evaluate BRILLsson on eight benchmark tasks (including but not limited to spoken language identification, emotion recognition, health condition diagnosis, and keyword spotting), and demonstrate that our proposed ultra-light and low-latency models perform as well as large-scale models. }
}

@article{231009932v2,
  title={ ''Reading Between the Heat'': Co-Teaching Body Thermal Signatures for   Non-intrusive Stress Detection },
  author={ Yi Xiao and Harshit Sharma and Zhongyang Zhang and Dessa Bergen-Cico and Tauhidur Rahman and Asif Salekin },
  journal={ arXiv preprint arXiv:2310.09932v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2310.09932v2 },
  abstract={ Stress impacts our physical and mental health as well as our social life. A passive and contactless indoor stress monitoring system can unlock numerous important applications such as workplace productivity assessment, smart homes, and personalized mental health monitoring. While the thermal signatures from a user's body captured by a thermal camera can provide important information about the ''fight-flight'' response of the sympathetic and parasympathetic nervous system, relying solely on thermal imaging for training a stress prediction model often lead to overfitting and consequently a suboptimal performance. This paper addresses this challenge by introducing ThermaStrain, a novel co-teaching framework that achieves high-stress prediction performance by transferring knowledge from the wearable modality to the contactless thermal modality. During training, ThermaStrain incorporates a wearable electrodermal activity (EDA) sensor to generate stress-indicative representations from thermal videos, emulating stress-indicative representations from a wearable EDA sensor. During testing, only thermal sensing is used, and stress-indicative patterns from thermal data and emulated EDA representations are extracted to improve stress assessment. The study collected a comprehensive dataset with thermal video and EDA data under various stress conditions and distances. ThermaStrain achieves an F1 score of 0.8293 in binary stress classification, outperforming the thermal-only baseline approach by over 9\%. Extensive evaluations highlight ThermaStrain's effectiveness in recognizing stress-indicative attributes, its adaptability across distances and stress scenarios, real-time executability on edge platforms, its applicability to multi-individual sensing, ability to function on limited visibility and unfamiliar conditions, and the advantages of its co-teaching approach. }
}

@article{230309077v2,
  title={ Towards the Understanding of Receptivity and Affect in EMAs using   Physiological based Machine Learning Method: Analysis of Receptivity and   Affect },
  author={ Zachary D King and Han Yu and Thomas Vaessen and Iniz Myin-Germeys and Akane Sano },
  journal={ arXiv preprint arXiv:2303.09077v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2303.09077v2 },
  abstract={ As mobile health (mHealth) studies become increasingly productive due to the advancements in wearable and mobile sensor technology, our ability to monitor and model human behavior will be constrained by participant receptivity. The reliance on subjective responses for health constructs poses challenges, especially in populations with lower receptivity rates. Researchers have proposed machine-learning approaches to optimize survey timing and delivery to address this. However, there are concerns regarding potential biases or unintended influences on participant responses. Our study delves into factors impacting receptivity to ecological momentary assessments (EMA) in a 10-day mHealth study, exploring physiological relationships indicative of receptivity and affect. Utilizing data from 45 participants with wearable devices measuring various biometrics, we employ unsupervised (k-means clustering) and supervised (Random Forest and Neural Networks) machine learning methods to infer affect during non-responses. Findings reveal that triggering EMAs based on a receptivity model reduces reported negative affect by over 3 points (0.29 standard deviations). The predicted affect during non-responses exhibits a bimodal distribution, suggesting more frequent initiation during states of higher positive emotions. The study underscores a clear relationship between affect and receptivity, impacting mHealth study efficacy, especially those using machine learning for EMA triggering. Therefore, we propose a smart trigger that promotes EMA receptivity without influencing affect during sampled time points as future work. }
}

@article{231209422v1,
  title={ Joint Alignment of Multivariate Quasi-Periodic Functional Data Using   Deep Learning },
  author={ Vi Thanh Pham and Jonas Bille Nielsen and Klaus Fuglsang Kofoed and Jørgen Tobias Kühl and Andreas Kryger Jensen },
  journal={ arXiv preprint arXiv:2312.09422v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2312.09422v1 },
  abstract={ The joint alignment of multivariate functional data plays an important role in various fields such as signal processing, neuroscience and medicine, including the statistical analysis of data from wearable devices. Traditional methods often ignore the phase variability and instead focus on the variability in the observed amplitude. We present a novel method for joint alignment of multivariate quasi-periodic functions using deep neural networks, decomposing, but retaining all the information in the data by preserving both phase and amplitude variability. Our proposed neural network uses a special activation of the output that builds on the unit simplex transformation, and we utilize a loss function based on the Fisher-Rao metric to train our model. Furthermore, our method is unsupervised and can provide an optimal common template function as well as subject-specific templates. We demonstrate our method on two simulated datasets and one real example, comprising data from 12-lead 10s electrocardiogram recordings. }
}

@article{231107765v1,
  title={ FedOpenHAR: Federated Multi-Task Transfer Learning for Sensor-Based   Human Activity Recognition },
  author={ Egemen İşgüder and Özlem Durmaz İncel },
  journal={ arXiv preprint arXiv:2311.07765v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2311.07765v1 },
  abstract={ Motion sensors integrated into wearable and mobile devices provide valuable information about the device users. Machine learning and, recently, deep learning techniques have been used to characterize sensor data. Mostly, a single task, such as recognition of activities, is targeted, and the data is processed centrally at a server or in a cloud environment. However, the same sensor data can be utilized for multiple tasks and distributed machine-learning techniques can be used without the requirement of the transmission of data to a centre. This paper explores Federated Transfer Learning in a Multi-Task manner for both sensor-based human activity recognition and device position identification tasks. The OpenHAR framework is used to train the models, which contains ten smaller datasets. The aim is to obtain model(s) applicable for both tasks in different datasets, which may include only some label types. Multiple experiments are carried in the Flower federated learning environment using the DeepConvLSTM architecture. Results are presented for federated and centralized versions under different parameters and restrictions. By utilizing transfer learning and training a task-specific and personalized federated model, we obtained a similar accuracy with training each client individually and higher accuracy than a fully centralized approach. }
}

@article{231104705v1,
  title={ Negotiation Strategies in Ubiquitous Human-Computer Interaction: A Novel   Storyboards Scale \& Field Study },
  author={ Sofia Yfantidou and Georgia Yfantidou and Panagiota Balaska and Athena Vakali },
  journal={ arXiv preprint arXiv:2311.04705v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2311.04705v1 },
  abstract={ In today's connected society, self-tracking technologies (STTs), such as wearables and mobile fitness apps, empower humans to improve their health and well-being through ubiquitous physical activity monitoring, with several personal and societal benefits. Despite the advances in such technologies' hardware, low user engagement and decreased effectiveness limitations demand more informed and theoretically-founded Human-Computer Interaction designs. To address these challenges, we build upon the previously unexplored Leisure Constraints Negotiation Model and the Transtheoretical Model to systematically define and assess the effectiveness of STTs' features that acknowledge users' contextual constraints and establish human-negotiated STTs narratives. Specifically, we introduce and validate a human-centric scale, StoryWear, which exploits and explores eleven dimensions of negotiation strategies that humans utilize to overcome constraints regarding exercise participation, captured through an inclusive storyboards format. Based on our preliminary studies, StoryWear shows high reliability, rendering it suitable for future work in ubiquitous computing. Our results indicate that negotiation strategies vary in perceived effectiveness and have higher appeal for existing STTs' users, with self-motivation, commitment, and understanding of the negative impact of non-exercise placed at the top. Finally, we give actionable guidelines for real-world implementation and a commentary on the future of personalized training. }
}

@article{231102251v1,
  title={ The Potential of Wearable Sensors for Assessing Patient Acuity in   Intensive Care Unit (ICU) },
  author={ Jessica Sena and Mohammad Tahsin Mostafiz and Jiaqing Zhang and Andrea Davidson and Sabyasachi Bandyopadhyay and Ren Yuanfang and Tezcan Ozrazgat-Baslanti and Benjamin Shickel and Tyler Loftus and William Robson Schwartz and Azra Bihorac and Parisa Rashidi },
  journal={ arXiv preprint arXiv:2311.02251v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2311.02251v1 },
  abstract={ Acuity assessments are vital in critical care settings to provide timely interventions and fair resource allocation. Traditional acuity scores rely on manual assessments and documentation of physiological states, which can be time-consuming, intermittent, and difficult to use for healthcare providers. Furthermore, such scores do not incorporate granular information such as patients' mobility level, which can indicate recovery or deterioration in the ICU. We hypothesized that existing acuity scores could be potentially improved by employing Artificial Intelligence (AI) techniques in conjunction with Electronic Health Records (EHR) and wearable sensor data. In this study, we evaluated the impact of integrating mobility data collected from wrist-worn accelerometers with clinical data obtained from EHR for developing an AI-driven acuity assessment score. Accelerometry data were collected from 86 patients wearing accelerometers on their wrists in an academic hospital setting. The data was analyzed using five deep neural network models: VGG, ResNet, MobileNet, SqueezeNet, and a custom Transformer network. These models outperformed a rule-based clinical score (SOFA= Sequential Organ Failure Assessment) used as a baseline, particularly regarding the precision, sensitivity, and F1 score. The results showed that while a model relying solely on accelerometer data achieved limited performance (AUC 0.50, Precision 0.61, and F1-score 0.68), including demographic information with the accelerometer data led to a notable enhancement in performance (AUC 0.69, Precision 0.75, and F1-score 0.67). This work shows that the combination of mobility and patient information can successfully differentiate between stable and unstable states in critically ill patients. }
}

@article{231014784v2,
  title={ An Efficient Imbalance-Aware Federated Learning Approach for Wearable   Healthcare with Autoregressive Ratio Observation },
  author={ Wenhao Yan and He Li and Kaoru Ota and Mianxiong Dong },
  journal={ arXiv preprint arXiv:2310.14784v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2310.14784v2 },
  abstract={ Widely available healthcare services are now getting popular because of advancements in wearable sensing techniques and mobile edge computing. People's health information is collected by edge devices such as smartphones and wearable bands for further analysis on servers, then send back suggestions and alerts for abnormal conditions. The recent emergence of federated learning allows users to train private data on local devices while updating models collaboratively. However, the heterogeneous distribution of the health condition data may lead to significant risks to model performance due to class imbalance. Meanwhile, as FL training is powered by sharing gradients only with the server, training data is almost inaccessible. The conventional solutions to class imbalance do not work for federated learning. In this work, we propose a new federated learning framework FedImT, dedicated to addressing the challenges of class imbalance in federated learning scenarios. FedImT contains an online scheme that can estimate the data composition during each round of aggregation, then introduces a self-attenuating iterative equivalent to track variations of multiple estimations and promptly tweak the balance of the loss computing for minority classes. Experiments demonstrate the effectiveness of FedImT in solving the imbalance problem without extra energy consumption and avoiding privacy risks. }
}

@article{231013965v1,
  title={ Cloud-Connected Wireless Holter Monitor Machine with Neural Networks   Based ECG Analysis for Remote Health Monitoring },
  author={ Azlaan Ranjha and Laiba Jabbar and Osaid Ahmed },
  journal={ arXiv preprint arXiv:2310.13965v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2310.13965v1 },
  abstract={ This study describes the creation of a wireless, transportable Holter monitor to improve the accuracy of cardiac disease diagnosis. The main goal of this study is to develop a low-cost cardiac screening system suited explicitly for underprivileged areas, addressing the rising rates of cardiovascular death. The suggested system includes a wireless Electrocardiogram (ECG) module for real-time cardiac signal gathering using attached electrodes, with data transfer made possible by WiFi to a cloud server for archival and analysis. The system uses a neural network model for automated ECG classification, concentrating on the identification of cardiac anomalies. The diagnostic performance of cardiologist-level ECG analysis is surpassed by our upgraded deep neural network architecture, which underwent thorough evaluation and showed a stunning accuracy rate of more than 88\\textbackslash{}\%. A quick, accurate, and reasonably priced option for cardiac screening is provided by this ground-breaking technology, which smoothly merges wireless data transfer with AI-assisted diagnostics. In addition to providing a thorough overview of the development process, this paper also highlights methods used to improve model accuracy, such as data preparation, class imbalance correction using oversampling, and model fine-tuning. The work shows the viability of a comprehensive remote cardiac screening system powered by AI and maximising the use of wearable and cloud computing resources. Such cutting-edge remote health monitoring technologies have great promise for improved health outcomes and early identification, especially in resource-constrained countries. }
}

@article{231007000v1,
  title={ CarDS-Plus ECG Platform: Development and Feasibility Evaluation of a   Multiplatform Artificial Intelligence Toolkit for Portable and Wearable   Device Electrocardiograms },
  author={ Sumukh Vasisht Shankar and Evangelos K Oikonomou and Rohan Khera },
  journal={ arXiv preprint arXiv:2310.07000v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2310.07000v1 },
  abstract={ In the rapidly evolving landscape of modern healthcare, the integration of wearable \& portable technology provides a unique opportunity for personalized health monitoring in the community. Devices like the Apple Watch, FitBit, and AliveCor KardiaMobile have revolutionized the acquisition and processing of intricate health data streams. Amidst the variety of data collected by these gadgets, single-lead electrocardiogram (ECG) recordings have emerged as a crucial source of information for monitoring cardiovascular health. There has been significant advances in artificial intelligence capable of interpreting these 1-lead ECGs, facilitating clinical diagnosis as well as the detection of rare cardiac disorders. This design study describes the development of an innovative multiplatform system aimed at the rapid deployment of AI-based ECG solutions for clinical investigation \& care delivery. The study examines design considerations, aligning them with specific applications, develops data flows to maximize efficiency for research \& clinical use. This process encompasses the reception of single-lead ECGs from diverse wearable devices, channeling this data into a centralized data lake \& facilitating real-time inference through AI models for ECG interpretation. An evaluation of the platform demonstrates a mean duration from acquisition to reporting of results of 33.0 to 35.7 seconds, after a standard 30 second acquisition. There were no substantial differences in acquisition to reporting across two commercially available devices (Apple Watch and KardiaMobile). These results demonstrate the succcessful translation of design principles into a fully integrated \& efficient strategy for leveraging 1-lead ECGs across platforms \& interpretation by AI-ECG algorithms. Such a platform is critical to translating AI discoveries for wearable and portable ECG devices to clinical impact through rapid deployment. }
}

@article{230503308v3,
  title={ Tiny-PPG: A Lightweight Deep Neural Network for Real-Time Detection of   Motion Artifacts in Photoplethysmogram Signals on Edge Devices },
  author={ Yali Zheng and Chen Wu and Peizheng Cai and Zhiqiang Zhong and Hongda Huang and Yuqi Jiang },
  journal={ arXiv preprint arXiv:2305.03308v3 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2305.03308v3 },
  abstract={ Photoplethysmogram (PPG) signals are easily contaminated by motion artifacts in real-world settings, despite their widespread use in Internet-of-Things (IoT) based wearable and smart health devices for cardiovascular health monitoring. This study proposed a lightweight deep neural network, called Tiny-PPG, for accurate and real-time PPG artifact segmentation on IoT edge devices. The model was trained and tested on a public dataset, PPG DaLiA, which featured complex artifacts with diverse lengths and morphologies during various daily activities of 15 subjects using a watch-type device (Empatica E4). The model structure, training method and loss function were specifically designed to balance detection accuracy and speed for real-time PPG artifact detection in resource-constrained embedded devices. To optimize the model size and capability in multi-scale feature representation, the model employed depth-wise separable convolution and atrous spatial pyramid pooling modules, respectively. Additionally, the contrastive loss was also utilized to further optimize the feature embeddings. With additional model pruning, Tiny-PPG achieved state-of-the-art detection accuracy of 87.4\% while only having 19,726 model parameters (0.15 megabytes), and was successfully deployed on an STM32 embedded system for real-time PPG artifact detection. Therefore, this study provides an effective solution for resource-constraint IoT smart health devices in PPG artifact detection. }
}

@article{231005643v1,
  title={ CLAID: Closing the Loop on AI \& Data Collection -- A Cross-Platform   Transparent Computing Middleware Framework for Smart Edge-Cloud and Digital   Biomarker Applications },
  author={ Patrick Langer and Elgar Fleisch and Filipe Barata },
  journal={ arXiv preprint arXiv:2310.05643v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2310.05643v1 },
  abstract={ The increasing number of edge devices with enhanced sensing capabilities, such as smartphones, wearables, and IoT devices equipped with sensors, holds the potential for innovative smart-edge applications in healthcare. These devices generate vast amounts of multimodal data, enabling the implementation of digital biomarkers which can be leveraged by machine learning solutions to derive insights, predict health risks, and allow personalized interventions. Training these models requires collecting data from edge devices and aggregating it in the cloud. To validate and verify those models, it is essential to utilize them in real-world scenarios and subject them to testing using data from diverse cohorts. Since some models are too computationally expensive to be run on edge devices directly, a collaborative framework between the edge and cloud becomes necessary. In this paper, we present CLAID, an open-source cross-platform middleware framework based on transparent computing compatible with Android, iOS, WearOS, Linux, macOS, and Windows. CLAID enables logical integration of devices running different operating systems into an edge-cloud system, facilitating communication and offloading between them, with bindings available in different programming languages. We provide Modules for data collection from various sensors as well as for the deployment of machine-learning models. Furthermore, we propose a novel methodology, ''ML-Model in the Loop'' for verifying deployed machine learning models, which helps to analyze problems that may occur during the migration of models from cloud to edge devices. We verify our framework in three different experiments and achieve 100\% sampling coverage for data collection across different sensors as well as an equal performance of a cough detection model deployed on both Android and iOS devices. We evaluate the memory and battery consumption of our framework. }
}

@article{231001733v1,
  title={ Health Guardian: Using Multi-modal Data to Understand Individual Health },
  author={ Vince S. Siu and Kuan Yu Hsieh and Italo Buleje and Takashi Itoh and Tian Hao and Ben Civjan and Nigel Hinds and Bing Dang and Jeffrey L. Rogers and Bo Wen },
  journal={ arXiv preprint arXiv:2310.01733v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2310.01733v1 },
  abstract={ Artificial intelligence (AI) has shown great promise in revolutionizing the field of digital health by improving disease diagnosis, treatment, and prevention. This paper describes the Health Guardian platform, a non-commercial, scientific research-based platform developed by the IBM Digital Health team to rapidly translate AI research into cloud-based microservices. The platform can collect health-related data from various digital devices, including wearables and mobile applications. Its flexible architecture supports microservices that accept diverse data types such as text, audio, and video, expanding the range of digital health assessments and enabling holistic health evaluations by capturing voice, facial, and motion bio-signals. These microservices can be deployed to a clinical cohort specified through the Clinical Task Manager (CTM). The CTM then collects multi-modal, clinical data that can iteratively improve the accuracy of AI predictive models, discover new disease mechanisms, or identify novel biomarkers. This paper highlights three microservices with different input data types, including a text-based microservice for depression assessment, a video-based microservice for sit-to-stand mobility assessment, and a wearable-based microservice for functional mobility assessment. The CTM is also discussed as a tool to help design and set up clinical studies to unlock the full potential of the platform. Today, the Health Guardian platform is being leveraged in collaboration with research partners to optimize the development of AI models by utilizing a multitude of input sources. This approach streamlines research efforts, enhances efficiency, and facilitates the development and validation of digital health applications. }
}

@article{230915292v1,
  title={ Scaling Representation Learning from Ubiquitous ECG with State-Space   Models },
  author={ Kleanthis Avramidis and Dominika Kunc and Bartosz Perz and Kranti Adsul and Tiantian Feng and Przemysław Kazienko and Stanisław Saganowski and Shrikanth Narayanan },
  journal={ arXiv preprint arXiv:2309.15292v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2309.15292v1 },
  abstract={ Ubiquitous sensing from wearable devices in the wild holds promise for enhancing human well-being, from diagnosing clinical conditions and measuring stress to building adaptive health promoting scaffolds. But the large volumes of data therein across heterogeneous contexts pose challenges for conventional supervised learning approaches. Representation Learning from biological signals is an emerging realm catalyzed by the recent advances in computational modeling and the abundance of publicly shared databases. The electrocardiogram (ECG) is the primary researched modality in this context, with applications in health monitoring, stress and affect estimation. Yet, most studies are limited by small-scale controlled data collection and over-parameterized architecture choices. We introduce \\textbackslash{}textbf\{WildECG\}, a pre-trained state-space model for representation learning from ECG signals. We train this model in a self-supervised manner with 275,000 10s ECG recordings collected in the wild and evaluate it on a range of downstream tasks. The proposed model is a robust backbone for ECG analysis, providing competitive performance on most of the tasks considered, while demonstrating efficacy in low-resource regimes. The code and pre-trained weights are shared publicly at https://github.com/klean2050/tiles\_ecg\_model. }
}

@article{230911097v1,
  title={ Evaluating Mental Stress Among College Students Using Heart Rate and   Hand Acceleration Data Collected from Wearable Sensors },
  author={ Moein Razavi and Anthony McDonald and Ranjana Mehta and Farzan Sasangohar },
  journal={ arXiv preprint arXiv:2309.11097v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2309.11097v1 },
  abstract={ Stress is various mental health disorders including depression and anxiety among college students. Early stress diagnosis and intervention may lower the risk of developing mental illnesses. We examined a machine learning-based method for identification of stress using data collected in a naturalistic study utilizing self-reported stress as ground truth as well as physiological data such as heart rate and hand acceleration. The study involved 54 college students from a large campus who used wearable wrist-worn sensors and a mobile health (mHealth) application continuously for 40 days. The app gathered physiological data including heart rate and hand acceleration at one hertz frequency. The application also enabled users to self-report stress by tapping on the watch face, resulting in a time-stamped record of the self-reported stress. We created, evaluated, and analyzed machine learning algorithms for identifying stress episodes among college students using heart rate and accelerometer data. The XGBoost method was the most reliable model with an AUC of 0.64 and an accuracy of 84.5\%. The standard deviation of hand acceleration, standard deviation of heart rate, and the minimum heart rate were the most important features for stress detection. This evidence may support the efficacy of identifying patterns in physiological reaction to stress using smartwatch sensors and may inform the design of future tools for real-time detection of stress. }
}

@article{230906236v1,
  title={ The first step is the hardest: Pitfalls of Representing and Tokenizing   Temporal Data for Large Language Models },
  author={ Dimitris Spathis and Fahim Kawsar },
  journal={ arXiv preprint arXiv:2309.06236v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2309.06236v1 },
  abstract={ Large Language Models (LLMs) have demonstrated remarkable generalization across diverse tasks, leading individuals to increasingly use them as personal assistants and universal computing engines. Nevertheless, a notable obstacle emerges when feeding numerical/temporal data into these models, such as data sourced from wearables or electronic health records. LLMs employ tokenizers in their input that break down text into smaller units. However, tokenizers are not designed to represent numerical values and might struggle to understand repetitive patterns and context, treating consecutive values as separate tokens and disregarding their temporal relationships. Here, we discuss recent works that employ LLMs for human-centric tasks such as in mobile health sensing and present a case study showing that popular LLMs tokenize temporal data incorrectly. To address that, we highlight potential solutions such as prompt tuning with lightweight embedding layers as well as multimodal adapters, that can help bridge this ''modality gap''. While the capability of language models to generalize to other modalities with minimal or no finetuning is exciting, this paper underscores the fact that their outputs cannot be meaningful if they stumble over input nuances. }
}

@article{230811773v2,
  title={ Identifying depression-related topics in smartphone-collected   free-response speech recordings using an automatic speech recognition system   and a deep learning topic model },
  author={ Yuezhou Zhang and Amos A Folarin and Judith Dineley and Pauline Conde and Valeria de Angel and Shaoxiong Sun and Yatharth Ranjan and Zulqarnain Rashid and Callum Stewart and Petroula Laiou and Heet Sankesara and Linglong Qian and Faith Matcham and Katie M White and Carolin Oetzmann and Femke Lamers and Sara Siddi and Sara Simblett and Björn W. Schuller and Srinivasan Vairavan and Til Wykes and Josep Maria Haro and Brenda WJH Penninx and Vaibhav A Narayan and Matthew Hotopf and Richard JB Dobson and Nicholas Cummins and RADAR-CNS consortium },
  journal={ arXiv preprint arXiv:2308.11773v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2308.11773v2 },
  abstract={ Language use has been shown to correlate with depression, but large-scale validation is needed. Traditional methods like clinic studies are expensive. So, natural language processing has been employed on social media to predict depression, but limitations remain-lack of validated labels, biased user samples, and no context. Our study identified 29 topics in 3919 smartphone-collected speech recordings from 265 participants using the Whisper tool and BERTopic model. Six topics with a median PHQ-8 greater than or equal to 10 were regarded as risk topics for depression: No Expectations, Sleep, Mental Therapy, Haircut, Studying, and Coursework. To elucidate the topic emergence and associations with depression, we compared behavioral (from wearables) and linguistic characteristics across identified topics. The correlation between topic shifts and changes in depression severity over time was also investigated, indicating the importance of longitudinally monitoring language use. We also tested the BERTopic model on a similar smaller dataset (356 speech recordings from 57 participants), obtaining some consistent results. In summary, our findings demonstrate specific speech topics may indicate depression severity. The presented data-driven workflow provides a practical approach to collecting and analyzing large-scale speech data from real-world settings for digital health research. }
}

@article{230902022v1,
  title={ Dynamic Early Exiting Predictive Coding Neural Networks },
  author={ Alaa Zniber and Ouassim Karrakchou and Mounir Ghogho },
  journal={ arXiv preprint arXiv:2309.02022v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2309.02022v1 },
  abstract={ Internet of Things (IoT) sensors are nowadays heavily utilized in various real-world applications ranging from wearables to smart buildings passing by agrotechnology and health monitoring. With the huge amounts of data generated by these tiny devices, Deep Learning (DL) models have been extensively used to enhance them with intelligent processing. However, with the urge for smaller and more accurate devices, DL models became too heavy to deploy. It is thus necessary to incorporate the hardware's limited resources in the design process. Therefore, inspired by the human brain known for its efficiency and low power consumption, we propose a shallow bidirectional network based on predictive coding theory and dynamic early exiting for halting further computations when a performance threshold is surpassed. We achieve comparable accuracy to VGG-16 in image classification on CIFAR-10 with fewer parameters and less computational complexity. }
}

@article{230814245v1,
  title={ A Comparison of Personalized and Generalized Approaches to Emotion   Recognition Using Consumer Wearable Devices: Machine Learning Study },
  author={ Joe Li and Peter Washington },
  journal={ arXiv preprint arXiv:2308.14245v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2308.14245v1 },
  abstract={ Background: Studies have shown the potential adverse health effects, ranging from headaches to cardiovascular disease, associated with long-term negative emotions and chronic stress. Since many indicators of stress are imperceptible to observers, the early detection and intervention of stress remains a pressing medical need. Physiological signals offer a non-invasive method of monitoring emotions and are easily collected by smartwatches. Existing research primarily focuses on developing generalized machine learning-based models for emotion classification. Objective: We aim to study the differences between personalized and generalized machine learning models for three-class emotion classification (neutral, stress, and amusement) using wearable biosignal data. Methods: We developed a convolutional encoder for the three-class emotion classification problem using data from WESAD, a multimodal dataset with physiological signals for 15 subjects. We compared the results between a subject-exclusive generalized, subject-inclusive generalized, and personalized model. Results: For the three-class classification problem, our personalized model achieved an average accuracy of 95.06\% and F1-score of 91.71, our subject-inclusive generalized model achieved an average accuracy of 66.95\% and F1-score of 42.50, and our subject-exclusive generalized model achieved an average accuracy of 67.65\% and F1-score of 43.05. Conclusions: Our results emphasize the need for increased research in personalized emotion recognition models given that they outperform generalized models in certain contexts. We also demonstrate that personalized machine learning models for emotion classification are viable and can achieve high performance. }
}

@article{230401568v2,
  title={ Arrhythmia Classifier Based on Ultra-Lightweight Binary Neural Network },
  author={ Ninghao Pu and Zhongxing Wu and Ao Wang and Hanshi Sun and Zijin Liu and Hao Liu },
  journal={ arXiv preprint arXiv:2304.01568v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2304.01568v2 },
  abstract={ Reasonably and effectively monitoring arrhythmias through ECG signals has significant implications for human health. With the development of deep learning, numerous ECG classification algorithms based on deep learning have emerged. However, most existing algorithms trade off high accuracy for complex models, resulting in high storage usage and power consumption. This also inevitably increases the difficulty of implementation on wearable Artificial Intelligence-of-Things (AIoT) devices with limited resources. In this study, we proposed a universally applicable ultra-lightweight binary neural network(BNN) that is capable of 5-class and 17-class arrhythmia classification based on ECG signals. Our BNN achieves 96.90\% (full precision 97.09\%) and 97.50\% (full precision 98.00\%) accuracy for 5-class and 17-class classification, respectively, with state-of-the-art storage usage (3.76 KB and 4.45 KB). Compared to other binarization works, our approach excels in supporting two multi-classification modes while achieving the smallest known storage space. Moreover, our model achieves optimal accuracy in 17-class classification and boasts an elegantly simple network architecture. The algorithm we use is optimized specifically for hardware implementation. Our research showcases the potential of lightweight deep learning models in the healthcare industry, specifically in wearable medical devices, which hold great promise for improving patient outcomes and quality of life. Code is available on: https://github.com/xpww/ECG\_BNN\_Net }
}

@article{230203224v3,
  title={ Undersampling and Cumulative Class Re-decision Methods to Improve   Detection of Agitation in People with Dementia },
  author={ Zhidong Meng and Andrea Iaboni and Bing Ye and Kristine Newman and Alex Mihailidis and Zhihong Deng and Shehroz S. Khan },
  journal={ arXiv preprint arXiv:2302.03224v3 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2302.03224v3 },
  abstract={ Agitation is one of the most prevalent symptoms in people with dementia (PwD) that can place themselves and the caregiver's safety at risk. Developing objective agitation detection approaches is important to support health and safety of PwD living in a residential setting. In a previous study, we collected multimodal wearable sensor data from 17 participants for 600 days and developed machine learning models for detecting agitation in one-minute windows. However, there are significant limitations in the dataset, such as imbalance problem and potential imprecise labelsas the occurrence of agitation is much rarer in comparison to the normal behaviours. In this paper, we first implemented different undersampling methods to eliminate the imbalance problem, and came to the conclusion that only 20\% of normal behaviour data were adequate to train a competitive agitation detection model. Then, we designed a weighted undersampling method to evaluate the manual labeling mechanism given the ambiguous time interval assumption. After that, the postprocessing method of cumulative class re-decision (CCR) was proposed based on the historical sequential information and continuity characteristic of agitation, improving the decision-making performance for the potential application of agitation detection system. The results showed that a combination of undersampling and CCR improved F1-score and other metrics to varying degrees with less training time and data. }
}

@article{221210540v2,
  title={ Challenges in Using mHealth Data From Smartphones and Wearable Devices   to Predict Depression Symptom Severity: Retrospective Analysis },
  author={ Shaoxiong Sun and Amos A. Folarin and Yuezhou Zhang and Nicholas Cummins and Rafael Garcia-Dias and Callum Stewart and Yatharth Ranjan and Zulqarnain Rashid and Pauline Conde and Petroula Laiou and Heet Sankesara and Faith Matcham and Daniel Leightley and Katie M. White and Carolin Oetzmann and Alina Ivan and Femke Lamers and Sara Siddi and Sara Simblett and Raluca Nica and Aki Rintala and David C. Mohr and Inez Myin-Germeys and Til Wykes and Josep Maria Haro and Brenda W. J. H. Penninx and Srinivasan Vairavan and Vaibhav A. Narayan and Peter Annas and Matthew Hotopf and Richard J. B. Dobson },
  journal={ arXiv preprint arXiv:2212.10540v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2212.10540v2 },
  abstract={ A number of challenges exist for the analysis of mHealth data: maintaining participant engagement over extended time periods and therefore understanding what constitutes an acceptable threshold of missing data; distinguishing between the cross-sectional and longitudinal relationships for different features to determine their utility in tracking within-individual longitudinal variation or screening individuals at high risk; and understanding the heterogeneity with which depression manifests itself in behavioral patterns quantified by the passive features. From 479 participants with MDD, we extracted 21 features capturing mobility, sleep, and smartphone use. We investigated the impact of the number of days of available data on feature quality using the intraclass correlation coefficient and Bland-Altman analysis. We then examined the nature of the correlation between the 8-item Patient Health Questionnaire (PHQ-8) depression scale (measured every 14 days) and the features using the individual-mean correlation, repeated measures correlation, and linear mixed effects model. Furthermore, we stratified the participants based on their behavioral difference, quantified by the features, between periods of high (depression) and low (no depression) PHQ-8 scores using the Gaussian mixture model. We demonstrated that at least 8 (range 2-12) days were needed for reliable calculation of most of the features in the 14-day time window. We observed that features such as sleep onset time correlated better with PHQ-8 scores cross-sectionally than longitudinally, whereas features such as wakefulness after sleep onset correlated well with PHQ-8 longitudinally but worse cross-sectionally. Finally, we found that participants could be separated into 3 distinct clusters according to their behavioral difference between periods of depression and periods of no depression. }
}

@article{230805759v1,
  title={ A machine-learning sleep-wake classification model using a reduced   number of features derived from photoplethysmography and activity signals },
  author={ Douglas A. Almeida and Felipe M. Dias and Marcelo A. F. Toledo and Diego A. C. Cardenas and Filipe A. C. Oliveira and Estela Ribeiro and Jose E. Krieger and Marco A. Gutierrez },
  journal={ arXiv preprint arXiv:2308.05759v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2308.05759v1 },
  abstract={ Sleep is a crucial aspect of our overall health and well-being. It plays a vital role in regulating our mental and physical health, impacting our mood, memory, and cognitive function to our physical resilience and immune system. The classification of sleep stages is a mandatory step to assess sleep quality, providing the metrics to estimate the quality of sleep and how well our body is functioning during this essential period of rest. Photoplethysmography (PPG) has been demonstrated to be an effective signal for sleep stage inference, meaning it can be used on its own or in a combination with others signals to determine sleep stage. This information is valuable in identifying potential sleep issues and developing strategies to improve sleep quality and overall health. In this work, we present a machine learning sleep-wake classification model based on the eXtreme Gradient Boosting (XGBoost) algorithm and features extracted from PPG signal and activity counts. The performance of our method was comparable to current state-of-the-art methods with a Sensitivity of 91.15 \$\\textbackslash{}pm\$ 1.16\%, Specificity of 53.66 \$\\textbackslash{}pm\$ 1.12\%, F1-score of 83.88 \$\\textbackslash{}pm\$ 0.56\%, and Kappa of 48.0 \$\\textbackslash{}pm\$ 0.86\%. Our method offers a significant improvement over other approaches as it uses a reduced number of features, making it suitable for implementation in wearable devices that have limited computational power. }
}

@article{230803805v1,
  title={ Weakly Supervised Multi-Task Representation Learning for Human Activity   Analysis Using Wearables },
  author={ Taoran Sheng and Manfred Huber },
  journal={ arXiv preprint arXiv:2308.03805v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2308.03805v1 },
  abstract={ Sensor data streams from wearable devices and smart environments are widely studied in areas like human activity recognition (HAR), person identification, or health monitoring. However, most of the previous works in activity and sensor stream analysis have been focusing on one aspect of the data, e.g. only recognizing the type of the activity or only identifying the person who performed the activity. We instead propose an approach that uses a weakly supervised multi-output siamese network that learns to map the data into multiple representation spaces, where each representation space focuses on one aspect of the data. The representation vectors of the data samples are positioned in the space such that the data with the same semantic meaning in that aspect are closely located to each other. Therefore, as demonstrated with a set of experiments, the trained model can provide metrics for clustering data based on multiple aspects, allowing it to address multiple tasks simultaneously and even to outperform single task supervised methods in many situations. In addition, further experiments are presented that in more detail analyze the effect of the architecture and of using multiple tasks within this framework, that investigate the scalability of the model to include additional tasks, and that demonstrate the ability of the framework to combine data for which only partial relationship information with respect to the target tasks is available. }
}

@article{230802731v1,
  title={ Personalization of Stress Mobile Sensing using Self-Supervised Learning },
  author={ Tanvir Islam and Peter Washington },
  journal={ arXiv preprint arXiv:2308.02731v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2308.02731v1 },
  abstract={ Stress is widely recognized as a major contributor to a variety of health issues. Stress prediction using biosignal data recorded by wearables is a key area of study in mobile sensing research because real-time stress prediction can enable digital interventions to immediately react at the onset of stress, helping to avoid many psychological and physiological symptoms such as heart rhythm irregularities. Electrodermal activity (EDA) is often used to measure stress. However, major challenges with the prediction of stress using machine learning include the subjectivity and sparseness of the labels, a large feature space, relatively few labels, and a complex nonlinear and subjective relationship between the features and outcomes. To tackle these issues, we examine the use of model personalization: training a separate stress prediction model for each user. To allow the neural network to learn the temporal dynamics of each individual's baseline biosignal patterns, thus enabling personalization with very few labels, we pre-train a 1-dimensional convolutional neural network (CNN) using self-supervised learning (SSL). We evaluate our method using the Wearable Stress and Affect prediction (WESAD) dataset. We fine-tune the pre-trained networks to the stress prediction task and compare against equivalent models without any self-supervised pre-training. We discover that embeddings learned using our pre-training method outperform supervised baselines with significantly fewer labeled data points: the models trained with SSL require less than 30\% of the labels to reach equivalent performance without personalized SSL. This personalized learning method can enable precision health systems which are tailored to each subject and require few annotations by the end user, thus allowing for the mobile sensing of increasingly complex, heterogeneous, and subjective outcomes such as stress. }
}

@article{230716664v1,
  title={ Generative models for wearables data },
  author={ Arinbjörn Kolbeinsson and Luca Foschini },
  journal={ arXiv preprint arXiv:2307.16664v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2307.16664v1 },
  abstract={ Data scarcity is a common obstacle in medical research due to the high costs associated with data collection and the complexity of gaining access to and utilizing data. Synthesizing health data may provide an efficient and cost-effective solution to this shortage, enabling researchers to explore distributions and populations that are not represented in existing observations or difficult to access due to privacy considerations. To that end, we have developed a multi-task self-attention model that produces realistic wearable activity data. We examine the characteristics of the generated data and quantify its similarity to genuine samples with both quantitative and qualitative approaches. }
}

@article{230315592v2,
  title={ Uncovering Bias in Personal Informatics },
  author={ Sofia Yfantidou and Pavlos Sermpezis and Athena Vakali and Ricardo Baeza-Yates },
  journal={ arXiv preprint arXiv:2303.15592v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2303.15592v2 },
  abstract={ Personal informatics (PI) systems, powered by smartphones and wearables, enable people to lead healthier lifestyles by providing meaningful and actionable insights that break down barriers between users and their health information. Today, such systems are used by billions of users for monitoring not only physical activity and sleep but also vital signs and women's and heart health, among others. Despite their widespread usage, the processing of sensitive PI data may suffer from biases, which may entail practical and ethical implications. In this work, we present the first comprehensive empirical and analytical study of bias in PI systems, including biases in raw data and in the entire machine learning life cycle. We use the most detailed framework to date for exploring the different sources of bias and find that biases exist both in the data generation and the model learning and implementation streams. According to our results, the most affected minority groups are users with health issues, such as diabetes, joint issues, and hypertension, and female users, whose data biases are propagated or even amplified by learning models, while intersectional biases can also be observed. }
}

@article{230708766v1,
  title={ Quality Assessment of Photoplethysmography Signals For Cardiovascular   Biomarkers Monitoring Using Wearable Devices },
  author={ Felipe M. Dias and Marcelo A. F. Toledo and Diego A. C. Cardenas and Douglas A. Almeida and Filipe A. C. Oliveira and Estela Ribeiro and Jose E. Krieger and Marco A. Gutierrez },
  journal={ arXiv preprint arXiv:2307.08766v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2307.08766v1 },
  abstract={ Photoplethysmography (PPG) is a non-invasive technology that measures changes in blood volume in the microvascular bed of tissue. It is commonly used in medical devices such as pulse oximeters and wrist worn heart rate monitors to monitor cardiovascular hemodynamics. PPG allows for the assessment of parameters (e.g., heart rate, pulse waveform, and peripheral perfusion) that can indicate conditions such as vasoconstriction or vasodilation, and provides information about microvascular blood flow, making it a valuable tool for monitoring cardiovascular health. However, PPG is subject to a number of sources of variations that can impact its accuracy and reliability, especially when using a wearable device for continuous monitoring, such as motion artifacts, skin pigmentation, and vasomotion. In this study, we extracted 27 statistical features from the PPG signal for training machine-learning models based on gradient boosting (XGBoost and CatBoost) and Random Forest (RF) algorithms to assess quality of PPG signals that were labeled as good or poor quality. We used the PPG time series from a publicly available dataset and evaluated the algorithm s performance using Sensitivity (Se), Positive Predicted Value (PPV), and F1-score (F1) metrics. Our model achieved Se, PPV, and F1-score of 94.4, 95.6, and 95.0 for XGBoost, 94.7, 95.9, and 95.3 for CatBoost, and 93.7, 91.3 and 92.5 for RF, respectively. Our findings are comparable to state-of-the-art reported in the literature but using a much simpler model, indicating that ML models are promising for developing remote, non-invasive, and continuous measurement devices. }
}

@article{230706380v1,
  title={ Personalized Anomaly Detection in PPG Data using Representation Learning   and Biometric Identification },
  author={ Ramin Ghorbani and Marcel J. T. Reinders and David M. J. Tax },
  journal={ arXiv preprint arXiv:2307.06380v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2307.06380v1 },
  abstract={ Photoplethysmography (PPG) signals, typically acquired from wearable devices, hold significant potential for continuous fitness-health monitoring. In particular, heart conditions that manifest in rare and subtle deviating heart patterns may be interesting. However, robust and reliable anomaly detection within these data remains a challenge due to the scarcity of labeled data and high inter-subject variability. This paper introduces a two-stage framework leveraging representation learning and personalization to improve anomaly detection performance in PPG data. The proposed framework first employs representation learning to transform the original PPG signals into a more discriminative and compact representation. We then apply three different unsupervised anomaly detection methods for movement detection and biometric identification. We validate our approach using two different datasets in both generalized and personalized scenarios. The results show that representation learning significantly improves anomaly detection performance while reducing the high inter-subject variability. Personalized models further enhance anomaly detection performance, underscoring the role of personalization in PPG-based fitness-health monitoring systems. The results from biometric identification show that it's easier to distinguish a new user from one intended authorized user than from a group of users. Overall, this study provides evidence of the effectiveness of representation learning and personalization for anomaly detection in PPG data. }
}

@article{230703832v1,
  title={ A Bayesian Circadian Hidden Markov Model to Infer Rest-Activity Rhythms   Using 24-hour Actigraphy Data },
  author={ Jiachen Lu and Qian Xiao and Cici Bauer },
  journal={ arXiv preprint arXiv:2307.03832v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2307.03832v1 },
  abstract={ 24-hour actigraphy data collected by wearable devices offer valuable insights into physical activity types, intensity levels, and rest-activity rhythms (RAR). RARs, or patterns of rest and activity exhibited over a 24-hour period, are regulated by the body's circadian system, synchronizing physiological processes with external cues like the light-dark cycle. Disruptions to these rhythms, such as irregular sleep patterns, daytime drowsiness or shift work, have been linked to adverse health outcomes including metabolic disorders, cardiovascular disease, depression, and even cancer, making RARs a critical area of health research.   In this study, we propose a Bayesian Circadian Hidden Markov Model (BCHMM) that explicitly incorporates 24-hour circadian oscillators mirroring human biological rhythms. The model assumes that observed activity counts are conditional on hidden activity states through Gaussian emission densities, with transition probabilities modeled by state-specific sinusoidal functions. Our comprehensive simulation study reveals that BCHMM outperforms frequentist approaches in identifying the underlying hidden states, particularly when the activity states are difficult to separate. BCHMM also excels with smaller Kullback-Leibler divergence on estimated densities. With the Bayesian framework, we address the label-switching problem inherent to hidden Markov models via a positive constraint on mean parameters. From the proposed BCHMM, we can infer the 24-hour rest-activity profile via time-varying state probabilities, to characterize the person-level RAR. We demonstrate the utility of the proposed BCHMM using 2011-2014 National Health and Nutrition Examination Survey (NHANES) data, where worsened RAR, indicated by lower probabilities in low-activity state during the day and higher probabilities in high-activity state at night, is associated with an increased risk of diabetes. }
}

@article{230703337v1,
  title={ Personalized Prediction of Recurrent Stress Events Using Self-Supervised   Learning on Multimodal Time-Series Data },
  author={ Tanvir Islam and Peter Washington },
  journal={ arXiv preprint arXiv:2307.03337v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2307.03337v1 },
  abstract={ Chronic stress can significantly affect physical and mental health. The advent of wearable technology allows for the tracking of physiological signals, potentially leading to innovative stress prediction and intervention methods. However, challenges such as label scarcity and data heterogeneity render stress prediction difficult in practice. To counter these issues, we have developed a multimodal personalized stress prediction system using wearable biosignal data. We employ self-supervised learning (SSL) to pre-train the models on each subject's data, allowing the models to learn the baseline dynamics of the participant's biosignals prior to fine-tuning the stress prediction task. We test our model on the Wearable Stress and Affect Detection (WESAD) dataset, demonstrating that our SSL models outperform non-SSL models while utilizing less than 5\% of the annotations. These results suggest that our approach can personalize stress prediction to each user with minimal annotations. This paradigm has the potential to enable personalized prediction of a variety of recurring health events using complex multimodal data streams. }
}

@article{230700883v1,
  title={ Augmenting Deep Learning Adaptation for Wearable Sensor Data through   Combined Temporal-Frequency Image Encoding },
  author={ Yidong Zhu and Md Mahmudur Rahman and Mohammad Arif Ul Alam },
  journal={ arXiv preprint arXiv:2307.00883v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2307.00883v1 },
  abstract={ Deep learning advancements have revolutionized scalable classification in many domains including computer vision. However, when it comes to wearable-based classification and domain adaptation, existing computer vision-based deep learning architectures and pretrained models trained on thousands of labeled images for months fall short. This is primarily because wearable sensor data necessitates sensor-specific preprocessing, architectural modification, and extensive data collection. To overcome these challenges, researchers have proposed encoding of wearable temporal sensor data in images using recurrent plots. In this paper, we present a novel modified-recurrent plot-based image representation that seamlessly integrates both temporal and frequency domain information. Our approach incorporates an efficient Fourier transform-based frequency domain angular difference estimation scheme in conjunction with the existing temporal recurrent plot image. Furthermore, we employ mixup image augmentation to enhance the representation. We evaluate the proposed method using accelerometer-based activity recognition data and a pretrained ResNet model, and demonstrate its superior performance compared to existing approaches. }
}

@article{230601110v2,
  title={ Comparative Study on the Effects of Noise in ML-Based Anxiety Detection },
  author={ Samuel Schapiro and Abdul Alkurdi and Elizabeth Hsiao-Wecksler },
  journal={ arXiv preprint arXiv:2306.01110v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2306.01110v2 },
  abstract={ Wearable health devices are ushering in a new age of continuous and noninvasive remote monitoring. One application of this technology is in anxiety detection. Many advancements in anxiety detection have happened in controlled lab settings, but noise prevents these advancements from generalizing to real-world conditions. We seek to progress the field by studying how noise impacts model performance and developing models that are robust to noisy, real-world conditions and, hence, attuned to the commotion of everyday life. In this study we look to investigate why and how previous methods have failed. Using the wearable stress and affect detection (WESAD) dataset, we compare the effect of various intensities of noise on machine learning models classifying levels of physiological arousal in the three-class classification problem: baseline vs. stress vs. amusement. Before introducing noise, our baseline model performance reaches 98.7\%, compared to Schmidt 2018's 80.3\%. We discuss potential sources of this discrepancy in results through a careful evaluation of feature extraction and model architecture choices. Finally, after the introduction of noise, we provide a thorough analysis of the effect of noise on each model architecture. }
}

@article{230610164v1,
  title={ MultiWave: Multiresolution Deep Architectures through Wavelet   Decomposition for Multivariate Time Series Prediction },
  author={ Iman Deznabi and Madalina Fiterau },
  journal={ arXiv preprint arXiv:2306.10164v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2306.10164v1 },
  abstract={ The analysis of multivariate time series data is challenging due to the various frequencies of signal changes that can occur over both short and long terms. Furthermore, standard deep learning models are often unsuitable for such datasets, as signals are typically sampled at different rates. To address these issues, we introduce MultiWave, a novel framework that enhances deep learning time series models by incorporating components that operate at the intrinsic frequencies of signals. MultiWave uses wavelets to decompose each signal into subsignals of varying frequencies and groups them into frequency bands. Each frequency band is handled by a different component of our model. A gating mechanism combines the output of the components to produce sparse models that use only specific signals at specific frequencies. Our experiments demonstrate that MultiWave accurately identifies informative frequency bands and improves the performance of various deep learning models, including LSTM, Transformer, and CNN-based models, for a wide range of applications. It attains top performance in stress and affect detection from wearables. It also increases the AUC of the best-performing model by 5\% for in-hospital COVID-19 mortality prediction from patient blood samples and for human activity recognition from accelerometer and gyroscope data. We show that MultiWave consistently identifies critical features and their frequency components, thus providing valuable insights into the applications studied. }
}

@article{220804705v2,
  title={ Classification of Stress via Ambulatory ECG and GSR Data },
  author={ Zachary Dair and Muhammad Muneeb Saad and Urja Pawar and Samantha Dockray and Ruairi O'Reilly },
  journal={ arXiv preprint arXiv:2208.04705v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2208.04705v2 },
  abstract={ In healthcare, detecting stress and enabling individuals to monitor their mental health and wellbeing is challenging. Advancements in wearable technology now enable continuous physiological data collection. This data can provide insights into mental health and behavioural states through psychophysiological analysis. However, automated analysis is required to provide timely results due to the quantity of data collected. Machine learning has shown efficacy in providing an automated classification of physiological data for health applications in controlled laboratory environments. Ambulatory uncontrolled environments, however, provide additional challenges requiring further modelling to overcome. This work empirically assesses several approaches utilising machine learning classifiers to detect stress using physiological data recorded in an ambulatory setting with self-reported stress annotations. A subset of the training portion SMILE dataset enables the evaluation of approaches before submission. The optimal stress detection approach achieves 90.77\% classification accuracy, 91.24 F1-Score, 90.42 Sensitivity and 91.08 Specificity, utilising an ExtraTrees classifier and feature imputation methods. Meanwhile, accuracy on the challenge data is much lower at 59.23\% (submission \#54 from BEaTS-MTU, username ZacDair). The cause of the performance disparity is explored in this work. }
}

@article{220713700v2,
  title={ Remote Medication Status Prediction for Individuals with Parkinson's   Disease using Time-series Data from Smartphones },
  author={ Weijian Li and Wei Zhu and E. Ray Dorsey and Jiebo Luo },
  journal={ arXiv preprint arXiv:2207.13700v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2207.13700v2 },
  abstract={ Medication for neurological diseases such as the Parkinson's disease usually happens remotely away from hospitals. Such out-of-lab environments pose challenges in collecting timely and accurate health status data. Individual differences in behavioral signals collected from wearable sensors also lead to difficulties in adopting current general machine learning analysis pipelines. To address these challenges, we present a method for predicting the medication status of Parkinson's disease patients using the public mPower dataset, which contains 62,182 remote multi-modal test records collected on smartphones from 487 patients. The proposed method shows promising results in predicting three medication statuses objectively: Before Medication (AUC=0.95), After Medication (AUC=0.958), and Another Time (AUC=0.976) by examining patient-wise historical records with the attention weights learned through a Transformer model. Our method provides an innovative way for personalized remote health sensing in a timely and objective fashion which could benefit a broad range of similar applications. }
}

@article{230515525v1,
  title={ Large Language Models are Few-Shot Health Learners },
  author={ Xin Liu and Daniel McDuff and Geza Kovacs and Isaac Galatzer-Levy and Jacob Sunshine and Jiening Zhan and Ming-Zher Poh and Shun Liao and Paolo Di Achille and Shwetak Patel },
  journal={ arXiv preprint arXiv:2305.15525v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2305.15525v1 },
  abstract={ Large language models (LLMs) can capture rich representations of concepts that are useful for real-world tasks. However, language alone is limited. While existing LLMs excel at text-based inferences, health applications require that models be grounded in numerical data (e.g., vital signs, laboratory values in clinical domains; steps, movement in the wellness domain) that is not easily or readily expressed as text in existing training corpus. We demonstrate that with only few-shot tuning, a large language model is capable of grounding various physiological and behavioral time-series data and making meaningful inferences on numerous health tasks for both clinical and wellness contexts. Using data from wearable and medical sensor recordings, we evaluate these capabilities on the tasks of cardiac signal analysis, physical activity recognition, metabolic calculation (e.g., calories burned), and estimation of stress reports and mental health screeners. }
}

@article{230507744v1,
  title={ Research Focused Software Development Kits and Wearable Devices in   Physical Activity Research },
  author={ Jason Tsang and Harry Prapavessis },
  journal={ arXiv preprint arXiv:2305.07744v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2305.07744v1 },
  abstract={ Introduction: The Canadian Guidelines recommend physical activity for overall health benefits, including cognitive, emotional, functional, and physical health. However, traditional research methods are inefficient and outdated. This paper aims to guide researchers in enhancing their research methods using software development kits and wearable smart devices. Methods: A generic model application was transformed into a research-based mobile application based on the UCLA researchers who collaborated with Apple. First, the research question and goals were identified. Then, three open-source software development kits (SDKs) were used to modify the generic model into the desired application. ResearchKit was used for informed consent, surveys, and active tasks. CareKit was the protocol manager to create participant protocols and track progress. Finally, HealthKit was used to access and share health-related data. The content expert evaluated the application, and the participant experience was optimized for easy use. The collected health-related data were analyzed to identify any significant findings. Results: Wearable health devices offer a convenient and non-invasive way to monitor and track health-related information. Conclusion: Leveraging the data provided by wearable devices, researchers can gain insights into the effectiveness of interventions and inform the development of evidence-based physical activity guidelines. The use of software development kits and wearable devices can enhance research methods and provide valuable insights into overall health benefits. }
}

@article{230414916v1,
  title={ ''Can't Take the Pressure?'': Examining the Challenges of Blood Pressure   Estimation via Pulse Wave Analysis },
  author={ Suril Mehta and Nipun Kwatra and Mohit Jain and Daniel McDuff },
  journal={ arXiv preprint arXiv:2304.14916v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2304.14916v1 },
  abstract={ The use of observed wearable sensor data (e.g., photoplethysmograms [PPG]) to infer health measures (e.g., glucose level or blood pressure) is a very active area of research. Such technology can have a significant impact on health screening, chronic disease management and remote monitoring. A common approach is to collect sensor data and corresponding labels from a clinical grade device (e.g., blood pressure cuff), and train deep learning models to map one to the other. Although well intentioned, this approach often ignores a principled analysis of whether the input sensor data has enough information to predict the desired metric. We analyze the task of predicting blood pressure from PPG pulse wave analysis. Our review of the prior work reveals that many papers fall prey data leakage, and unrealistic constraints on the task and the preprocessing steps. We propose a set of tools to help determine if the input signal in question (e.g., PPG) is indeed a good predictor of the desired label (e.g., blood pressure). Using our proposed tools, we have found that blood pressure prediction using PPG has a high multi-valued mapping factor of 33.2\% and low mutual information of 9.8\%. In comparison, heart rate prediction using PPG, a well-established task, has a very low multi-valued mapping factor of 0.75\% and high mutual information of 87.7\%. We argue that these results provide a more realistic representation of the current progress towards to goal of wearable blood pressure measurement via PPG pulse wave analysis. }
}

@article{230410643v1,
  title={ Activity Classification Using Unsupervised Domain Transfer from Body   Worn Sensors },
  author={ Chaitra Hedge and Gezheng Wen and Layne C. Price },
  journal={ arXiv preprint arXiv:2304.10643v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2304.10643v1 },
  abstract={ Activity classification has become a vital feature of wearable health tracking devices. As innovation in this field grows, wearable devices worn on different parts of the body are emerging. To perform activity classification on a new body location, labeled data corresponding to the new locations are generally required, but this is expensive to acquire. In this work, we present an innovative method to leverage an existing activity classifier, trained on Inertial Measurement Unit (IMU) data from a reference body location (the source domain), in order to perform activity classification on a new body location (the target domain) in an unsupervised way, i.e. without the need for classification labels at the new location. Specifically, given an IMU embedding model trained to perform activity classification at the source domain, we train an embedding model to perform activity classification at the target domain by replicating the embeddings at the source domain. This is achieved using simultaneous IMU measurements at the source and target domains. The replicated embeddings at the target domain are used by a classification model that has previously been trained on the source domain to perform activity classification at the target domain. We have evaluated the proposed methods on three activity classification datasets PAMAP2, MHealth, and Opportunity, yielding high F1 scores of 67.19\%, 70.40\% and 68.34\%, respectively when the source domain is the wrist and the target domain is the torso. }
}

@article{230406335v1,
  title={ Deep Learning-based Fall Detection Algorithm Using Ensemble Model of   Coarse-fine CNN and GRU Networks },
  author={ Chien-Pin Liu and Ju-Hsuan Li and En-Ping Chu and Chia-Yeh Hsieh and Kai-Chun Liu and Chia-Tai Chan and Yu Tsao },
  journal={ arXiv preprint arXiv:2304.06335v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2304.06335v1 },
  abstract={ Falls are the public health issue for the elderly all over the world since the fall-induced injuries are associated with a large amount of healthcare cost. Falls can cause serious injuries, even leading to death if the elderly suffers a ''long-lie''. Hence, a reliable fall detection (FD) system is required to provide an emergency alarm for first aid. Due to the advances in wearable device technology and artificial intelligence, some fall detection systems have been developed using machine learning and deep learning methods to analyze the signal collected from accelerometer and gyroscopes. In order to achieve better fall detection performance, an ensemble model that combines a coarse-fine convolutional neural network and gated recurrent unit is proposed in this study. The parallel structure design used in this model restores the different grains of spatial characteristics and capture temporal dependencies for feature representation. This study applies the FallAllD public dataset to validate the reliability of the proposed model, which achieves a recall, precision, and F-score of 92.54\%, 96.13\%, and 94.26\%, respectively. The results demonstrate the reliability of the proposed ensemble model in discriminating falls from daily living activities and its superior performance compared to the state-of-the-art convolutional neural network long short-term memory (CNN-LSTM) for FD. }
}

@article{230406489v1,
  title={ Domain Adaptation for Inertial Measurement Unit-based Human Activity   Recognition: A Survey },
  author={ Avijoy Chakma and Abu Zaher Md Faridee and Indrajeet Ghosh and Nirmalya Roy },
  journal={ arXiv preprint arXiv:2304.06489v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2304.06489v1 },
  abstract={ Machine learning-based wearable human activity recognition (WHAR) models enable the development of various smart and connected community applications such as sleep pattern monitoring, medication reminders, cognitive health assessment, sports analytics, etc. However, the widespread adoption of these WHAR models is impeded by their degraded performance in the presence of data distribution heterogeneities caused by the sensor placement at different body positions, inherent biases and heterogeneities across devices, and personal and environmental diversities. Various traditional machine learning algorithms and transfer learning techniques have been proposed in the literature to address the underpinning challenges of handling such data heterogeneities. Domain adaptation is one such transfer learning techniques that has gained significant popularity in recent literature. In this paper, we survey the recent progress of domain adaptation techniques in the Inertial Measurement Unit (IMU)-based human activity recognition area, discuss potential future directions. }
}

@article{230314028v1,
  title={ Non-invasive urinary bladder volume estimation with artefact-suppressed   bio-impedance measurements },
  author={ Kanika Dheman and Stefan Walser and Philipp Mayer and Manuel Eggimann and Marko Kozomara and Denise Franke and Thomas Hermanns and Hugo Sax and Simone Schürle and Michele Magno },
  journal={ arXiv preprint arXiv:2303.14028v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2303.14028v1 },
  abstract={ Urine output is a vital parameter to gauge kidney health. Current monitoring methods include manually written records, invasive urinary catheterization or ultrasound measurements performed by highly skilled personnel. Catheterization bears high risks of infection while intermittent ultrasound measures and manual recording are time consuming and might miss early signs of kidney malfunction. Bioimpedance (BI) measurements may serve as a non-invasive alternative for measuring urine volume in vivo. However, limited robustness have prevented its clinical translation. Here, a deep learning-based algorithm is presented that processes the local BI of the lower abdomen and suppresses artefacts to measure the bladder volume quantitatively, non-invasively and without the continuous need for additional personnel. A tetrapolar BI wearable system called ANUVIS was used to collect continuous bladder volume data from three healthy subjects to demonstrate feasibility of operation, while clinical gold standards of urodynamic (n=6) and uroflowmetry tests (n=8) provided the ground truth. Optimized location for electrode placement and a model for the change in BI with changing bladder volume is deduced. The average error for full bladder volume estimation and for residual volume estimation was -29 +/-87.6 ml, thus, comparable to commercial portable ultrasound devices (Bland Altman analysis showed a bias of -5.2 ml with LoA between 119.7 ml to -130.1 ml), while providing the additional benefit of hands-free, non-invasive, and continuous bladder volume estimation. The combination of the wearable BI sensor node and the presented algorithm provides an attractive alternative to current standard of care with potential benefits in providing insights into kidney function. }
}

@article{230308538v1,
  title={ Health Monitoring of Movement Disorder Subject based on Diamond Stacked   Sparse Autoencoder Ensemble Model },
  author={ Likun Tang and Jie Ma and Yongming Li },
  journal={ arXiv preprint arXiv:2303.08538v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2303.08538v1 },
  abstract={ The health monitoring of chronic diseases is very important for people with movement disorders because of their limited mobility and long duration of chronic diseases. Machine learning-based processing of data collected from the human with movement disorders using wearable sensors is an effective method currently available for health monitoring. However, wearable sensor systems are difficult to obtain high-quality and large amounts of data, which cannot meet the requirement for diagnostic accuracy. Moreover, existing machine learning methods do not handle this problem well. Feature learning is key to machine learning. To solve this problem, a health monitoring of movement disorder subject based on diamond stacked sparse autoencoder ensemble model (DsaeEM) is proposed in this paper. This algorithm has two major components. First, feature expansion is designed using feature-embedded stacked sparse autoencoder (FSSAE). Second, a feature reduction mechanism is designed to remove the redundancy among the expanded features. This mechanism includes L1 regularized feature-reduction algorithm and the improved manifold dimensionality reduction algorithm. This paper refers to the combined feature expansion and feature reduction mechanism as the diamond-like feature learning mechanism. The method is experimentally verified with several state of art algorithms and on two datasets. The results show that the proposed algorithm has higher accuracy apparently. In conclusion, this study developed an effective and feasible feature-learning algorithm for the recognition of chronic diseases. }
}

@article{230308215v1,
  title={ Stress Detection using Context-Aware Sensor Fusion from Wearable Devices },
  author={ Nafiul Rashid and Trier Mortlock and Mohammad Abdullah Al Faruque },
  journal={ arXiv preprint arXiv:2303.08215v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2303.08215v1 },
  abstract={ Wearable medical technology has become increasingly popular in recent years. One function of wearable health devices is stress detection, which relies on sensor inputs to determine the mental state of patients. This continuous, real-time monitoring can provide healthcare professionals with vital physiological data and enhance the quality of patient care. Current methods of stress detection lack: (i) robustness -- wearable health sensors contain high levels of measurement noise that degrades performance, and (ii) adaptation -- static architectures fail to adapt to changing contexts in sensing conditions. We propose to address these deficiencies with SELF-CARE, a generalized selective sensor fusion method of stress detection that employs novel techniques of context identification and ensemble machine learning. SELF-CARE uses a learning-based classifier to process sensor features and model the environmental variations in sensing conditions known as the noise context. SELF-CARE uses noise context to selectively fuse different sensor combinations across an ensemble of models to perform robust stress classification. Our findings suggest that for wrist-worn devices, sensors that measure motion are most suitable to understand noise context, while for chest-worn devices, the most suitable sensors are those that detect muscle contraction. SELF-CARE demonstrates state-of-the-art performance on the WESAD dataset. Using wrist-based sensors, SELF-CARE achieves 86.34\% and 94.12\% accuracy for the 3-class and 2-class stress classification problems, respectively. For chest-based wearable sensors, SELF-CARE achieves 86.19\% (3-class) and 93.68\% (2-class) classification accuracy. This work demonstrates the benefits of utilizing selective, context-aware sensor fusion in mobile health sensing that can be applied broadly to Internet of Things applications. }
}

@article{210101624v4,
  title={ Bayesian Hierarchical Modeling and Analysis for Actigraph Data from   Wearable Devices },
  author={ Pierfrancesco Alaimo Di Loro and Marco Mingione and Jonah Lipsitt and Christina M. Batteate and Michael Jerrett and Sudipto Banerjee },
  journal={ arXiv preprint arXiv:2101.01624v4 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2101.01624v4 },
  abstract={ The majority of Americans fail to achieve recommended levels of physical activity, which leads to numerous preventable health problems such as diabetes, hypertension, and heart diseases. This has generated substantial interest in monitoring human activity to gear interventions toward environmental features that may relate to higher physical activity. Wearable devices, such as wrist-worn sensors that monitor gross motor activity (actigraph units) continuously record the activity levels of a subject, producing massive amounts of high-resolution measurements. Analyzing actigraph data needs to account for spatial and temporal information on trajectories or paths traversed by subjects wearing such devices. Inferential objectives include estimating a subject's physical activity levels along a given trajectory; identifying trajectories that are more likely to produce higher levels of physical activity for a given subject; and predicting expected levels of physical activity in any proposed new trajectory for a given set of health attributes. Here, we devise a Bayesian hierarchical modeling framework for spatial-temporal actigraphy data to deliver fully model-based inference on trajectories while accounting for subject-level health attributes and spatial-temporal dependencies. We undertake a comprehensive analysis of an original dataset from the Physical Activity through Sustainable Transport Approaches in Los Angeles (PASTA-LA) study to ascertain spatial zones and trajectories exhibiting significantly higher levels of physical activity while accounting for various sources of heterogeneity. }
}

@article{230110156v1,
  title={ Sleep Activity Recognition and Characterization from Multi-Source   Passively Sensed Data },
  author={ María Martínez-García and Fernando Moreno-Pino and Pablo M. Olmos and Antonio Artés-Rodríguez },
  journal={ arXiv preprint arXiv:2301.10156v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2301.10156v1 },
  abstract={ Sleep constitutes a key indicator of human health, performance, and quality of life. Sleep deprivation has long been related to the onset, development, and worsening of several mental and metabolic disorders, constituting an essential marker for preventing, evaluating, and treating different health conditions. Sleep Activity Recognition methods can provide indicators to assess, monitor, and characterize subjects' sleep-wake cycles and detect behavioral changes. In this work, we propose a general method that continuously operates on passively sensed data from smartphones to characterize sleep and identify significant sleep episodes. Thanks to their ubiquity, these devices constitute an excellent alternative data source to profile subjects' biorhythms in a continuous, objective, and non-invasive manner, in contrast to traditional sleep assessment methods that usually rely on intrusive and subjective procedures. A Heterogeneous Hidden Markov Model is used to model a discrete latent variable process associated with the Sleep Activity Recognition task in a self-supervised way. We validate our results against sleep metrics reported by tested wearables, proving the effectiveness of the proposed approach and advocating its use to assess sleep without more reliable sources. }
}

@article{221000888v2,
  title={ Smart-Badge: A wearable badge with multi-modal sensors for kitchen   activity recognition },
  author={ Mengxi Liu and Sungho Suh and Bo Zhou and Agnes Gruenerbl and Paul Lukowicz },
  journal={ arXiv preprint arXiv:2210.00888v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2210.00888v2 },
  abstract={ Human health is closely associated with their daily behavior and environment. However, keeping a healthy lifestyle is still challenging for most people as it is difficult to recognize their living behaviors and identify their surrounding situations to take appropriate action. Human activity recognition is a promising approach to building a behavior model of users, by which users can get feedback about their habits and be encouraged to develop a healthier lifestyle. In this paper, we present a smart light wearable badge with six kinds of sensors, including an infrared array sensor MLX90640 offering privacy-preserving, low-cost, and non-invasive features, to recognize daily activities in a realistic unmodified kitchen environment. A multi-channel convolutional neural network (MC-CNN) based on data and feature fusion methods is applied to classify 14 human activities associated with potentially unhealthy habits. Meanwhile, we evaluate the impact of the infrared array sensor on the recognition accuracy of these activities. We demonstrate the performance of the proposed work to detect the 14 activities performed by ten volunteers with an average accuracy of 92.44 \% and an F1 score of 88.27 \%. }
}

@article{230102475v1,
  title={ Hearables: Feasibility of Recording Cardiac Rhythms from Single Ear   Locations },
  author={ Metin Yarici and Wilhelm Von Rosenberg and Ghena Hammour and Harry Davies and Pierluigi Amadori and Nico Lingg and Yiannis Demiris and Danilo P. Mandic },
  journal={ arXiv preprint arXiv:2301.02475v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2301.02475v1 },
  abstract={ Wearable technologies are envisaged to provide critical support to future healthcare systems. Hearables - devices worn in the ear - are of particular interest due to their ability to provide health monitoring in an efficient, reliable and unobtrusive way. Despite the considerable potential of these devices, the ECG signal that can be acquired through a hearable device worn on a single ear is still relatively unexplored. Biophysics modelling of ECG volume conduction was used to establish principles behind the single ear ECG signal, and measurements of cardiac rhythms from 10 subjects were found to be in good correspondence with simulated equivalents. Additionally, the viability of the single ear ECG in real-world environments was determined through one hour duration measurements during a simulated driving task on 5 subjects. Results demonstrated that the single ear ECG resembles the Lead I signal, the most widely used ECG signal in the identification of heart conditions such as myocardial infarction and atrial fibrillation, and was robust against real-world measurement noise, even after prolonged measurements. This study conclusively demonstrates that hearables can enable continuous monitoring of vital signs in an unobtrusive and seamless way, with the potential for reliable identification and management of heart conditions such as myocardial infarction and atrial fibrillation. }
}

@article{230103469v1,
  title={ KIDS: kinematics-based (in)activity detection and segmentation in a   sleep case study },
  author={ Omar Elnaggar and Roselina Arelhi and Frans Coenen and Andrew Hopkinson and Lyndon Mason and Paolo Paoletti },
  journal={ arXiv preprint arXiv:2301.03469v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2301.03469v1 },
  abstract={ Sleep behaviour and in-bed movements contain rich information on the neurophysiological health of people, and have a direct link to the general well-being and quality of life. Standard clinical practices rely on polysomnography for sleep assessment; however, it is intrusive, performed in unfamiliar environments and requires trained personnel. Progress has been made on less invasive sensor technologies, such as actigraphy, but clinical validation raises concerns over their reliability and precision. Additionally, the field lacks a widely acceptable algorithm, with proposed approaches ranging from raw signal or feature thresholding to data-hungry classification models, many of which are unfamiliar to medical staff. This paper proposes an online Bayesian probabilistic framework for objective (in)activity detection and segmentation based on clinically meaningful joint kinematics, measured by a custom-made wearable sensor. Intuitive three-dimensional visualisations of kinematic timeseries were accomplished through dimension reduction based preprocessing, offering out-of-the-box framework explainability potentially useful for clinical monitoring and diagnosis. The proposed framework attained up to 99.2\\textbackslash{}\% \$F\_1\$-score and 0.96 Pearson's correlation coefficient in, respectively, the posture change detection and inactivity segmentation tasks. The work paves the way for a reliable home-based analysis of movements during sleep which would serve patient-centred longitudinal care plans. }
}

@article{200908798v3,
  title={ Designing Compact Features for Remote Stroke Rehabilitation Monitoring   using Wearable Accelerometers },
  author={ Xi Chen and Yu Guan and Jian Qing Shi and Xiu-Li Du and Janet Eyre },
  journal={ arXiv preprint arXiv:2009.08798v3 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2009.08798v3 },
  abstract={ Stroke is known as a major global health problem, and for stroke survivors it is key to monitor the recovery levels. However, traditional stroke rehabilitation assessment methods (such as the popular clinical assessment) can be subjective and expensive, and it is also less convenient for patients to visit clinics in a high frequency. To address this issue, in this work based on wearable sensing and machine learning techniques, we develop an automated system that can predict the assessment score in an objective manner. With wrist-worn sensors, accelerometer data is collected from 59 stroke survivors in free-living environments for a duration of 8 weeks, and we map the week-wise accelerometer data(3 days per week) to the assessment score by developing signal processing and predictive model pipeline. To achieve this, we propose two types of new features, which can encode the rehabilitation information from both paralysed and non-paralysed sides while suppressing the high level noises such as irrelevant daily activities. Based on the proposed features, we further develop the longitudinal mixed-effects model with Gaussian process prior (LMGP), which can model the random effects caused by different subjects and time slots (during the 8 weeks). Comprehensive experiments are conducted to evaluate our system on both acute and chronic patients, and the promising results suggest its effectiveness. }
}

@article{221204902v2,
  title={ Self-Supervised PPG Representation Learning Shows High Inter-Subject   Variability },
  author={ Ramin Ghorbani and Marcel J. T. Reinders and David M. J. Tax },
  journal={ arXiv preprint arXiv:2212.04902v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2212.04902v2 },
  abstract={ With the progress of sensor technology in wearables, the collection and analysis of PPG signals are gaining more interest. Using Machine Learning, the cardiac rhythm corresponding to PPG signals can be used to predict different tasks such as activity recognition, sleep stage detection, or more general health status. However, supervised learning is often limited by the amount of available labeled data, which is typically expensive to obtain. To address this problem, we propose a Self-Supervised Learning (SSL) method with a pretext task of signal reconstruction to learn an informative generalized PPG representation. The performance of the proposed SSL framework is compared with two fully supervised baselines. The results show that in a very limited label data setting (10 samples per class or less), using SSL is beneficial, and a simple classifier trained on SSL-learned representations outperforms fully supervised deep neural networks. However, the results reveal that the SSL-learned representations are too focused on encoding the subjects. Unfortunately, there is high inter-subject variability in the SSL-learned representations, which makes working with this data more challenging when labeled data is scarce. The high inter-subject variability suggests that there is still room for improvements in learning representations. In general, the results suggest that SSL may pave the way for the broader use of machine learning models on PPG data in label-scarce regimes. }
}

@article{221203357v1,
  title={ Contactless Oxygen Monitoring with Gated Transformer },
  author={ Hao He and Yuan Yuan and Ying-Cong Chen and Peng Cao and Dina Katabi },
  journal={ arXiv preprint arXiv:2212.03357v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2212.03357v1 },
  abstract={ With the increasing popularity of telehealth, it becomes critical to ensure that basic physiological signals can be monitored accurately at home, with minimal patient overhead. In this paper, we propose a contactless approach for monitoring patients' blood oxygen at home, simply by analyzing the radio signals in the room, without any wearable devices. We extract the patients' respiration from the radio signals that bounce off their bodies and devise a novel neural network that infers a patient's oxygen estimates from their breathing signal. Our model, called \\textbackslash{}emph\{Gated BERT-UNet\}, is designed to adapt to the patient's medical indices (e.g., gender, sleep stages). It has multiple predictive heads and selects the most suitable head via a gate controlled by the person's physiological indices. Extensive empirical results show that our model achieves high accuracy on both medical and radio datasets. }
}

@article{220203019v3,
  title={ A Riemann Manifold Model Framework for Longitudinal Changes in Physical   Activity Patterns },
  author={ Jingjing Zou and Tuo Lin and Chongzhi Di and John Bellettiere and Marta M. Jankowska and Sheri J. Hartman and Dorothy D. Sears and Andrea Z. LaCroix and Cheryl L. Rock and Loki Natarajan },
  journal={ arXiv preprint arXiv:2202.03019v3 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2202.03019v3 },
  abstract={ Physical activity (PA) is significantly associated with many health outcomes. The wide usage of wearable accelerometer-based activity trackers in recent years has provided a unique opportunity for in-depth research on PA and its relations with health outcomes and interventions. Past analysis of activity tracker data relies heavily on aggregating minute-level PA records into day-level summary statistics, in which important information of PA temporal/diurnal patterns is lost. In this paper we propose a novel functional data analysis approach based on Riemann manifolds for modeling PA and its longitudinal changes. We model smoothed minute-level PA of a day as one-dimensional Riemann manifolds and longitudinal changes in PA in different visits as deformations between manifolds. The variability in changes of PA among a cohort of subjects is characterized via variability in the deformation. Functional principal component analysis is further adopted to model the deformations and PC scores are used as a proxy in modeling the relation between changes in PA and health outcomes and/or interventions. We conduct comprehensive analyses on data from two clinical trials: Reach for Health (RfH) and Metabolism, Exercise and Nutrition at UCSD (MENU), focusing on the effect of interventions on longitudinal changes in PA patterns and how different modes of changes in PA influence weight loss, respectively. The proposed approach reveals unique modes of changes including overall enhanced PA, boosted morning PA, and shifts of active hours specific to each study cohort. The results bring new insights into the study of longitudinal changes in PA and health and have the potential to facilitate designing of effective health interventions and guidelines. }
}

@article{221110475v1,
  title={ Turning Silver into Gold: Domain Adaptation with Noisy Labels for   Wearable Cardio-Respiratory Fitness Prediction },
  author={ Yu Wu and Dimitris Spathis and Hong Jia and Ignacio Perez-Pozuelo and Tomas I. Gonzales and Soren Brage and Nicholas Wareham and Cecilia Mascolo },
  journal={ arXiv preprint arXiv:2211.10475v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2211.10475v1 },
  abstract={ Deep learning models have shown great promise in various healthcare applications. However, most models are developed and validated on small-scale datasets, as collecting high-quality (gold-standard) labels for health applications is often costly and time-consuming. As a result, these models may suffer from overfitting and not generalize well to unseen data. At the same time, an extensive amount of data with imprecise labels (silver-standard) is starting to be generally available, as collected from inexpensive wearables like accelerometers and electrocardiography sensors. These currently underutilized datasets and labels can be leveraged to produce more accurate clinical models. In this work, we propose UDAMA, a novel model with two key components: Unsupervised Domain Adaptation and Multi-discriminator Adversarial training, which leverage noisy data from source domain (the silver-standard dataset) to improve gold-standard modeling. We validate our framework on the challenging task of predicting lab-measured maximal oxygen consumption (VO\$\_\{2\}\$max), the benchmark metric of cardio-respiratory fitness, using free-living wearable sensor data from two cohort studies as inputs. Our experiments show that the proposed framework achieves the best performance of corr = 0.665 \$\\textbackslash{}pm\$ 0.04, paving the way for accurate fitness estimation at scale. }
}

@article{220308176v2,
  title={ SemiPFL: Personalized Semi-Supervised Federated Learning Framework for   Edge Intelligence },
  author={ Arvin Tashakori and Wenwen Zhang and Z. Jane Wang and Peyman Servati },
  journal={ arXiv preprint arXiv:2203.08176v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2203.08176v2 },
  abstract={ Recent advances in wearable devices and Internet-of-Things (IoT) have led to massive growth in sensor data generated in edge devices. Labeling such massive data for classification tasks has proven to be challenging. In addition, data generated by different users bear various personal attributes and edge heterogeneity, rendering it impractical to develop a global model that adapts well to all users. Concerns over data privacy and communication costs also prohibit centralized data accumulation and training. We propose SemiPFL that supports edge users having no label or limited labeled datasets and a sizable amount of unlabeled data that is insufficient to train a well-performing model. In this work, edge users collaborate to train a Hyper-network in the server, generating personalized autoencoders for each user. After receiving updates from edge users, the server produces a set of base models for each user, which the users locally aggregate them using their own labeled dataset. We comprehensively evaluate our proposed framework on various public datasets from a wide range of application scenarios, from wearable health to IoT, and demonstrate that SemiPFL outperforms state-of-art federated learning frameworks under the same assumptions regarding user performance, network footprint, and computational consumption. We also show that the solution performs well for users without label or having limited labeled datasets and increasing performance for increased labeled data and number of users, signifying the effectiveness of SemiPFL for handling data heterogeneity and limited annotation. We also demonstrate the stability of SemiPFL for handling user hardware resource heterogeneity in three real-time scenarios. }
}

@article{221014152v3,
  title={ SleepMore: Inferring Sleep Duration at Scale via Multi-Device WiFi   Sensing },
  author={ Camellia Zakaria and Gizem Yilmaz and Priyanka Mammen and Michael Chee and Prashant Shenoy and Rajesh Balan },
  journal={ arXiv preprint arXiv:2210.14152v3 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2210.14152v3 },
  abstract={ The availability of commercial wearable trackers equipped with features to monitor sleep duration and quality has enabled more useful sleep health monitoring applications and analyses. However, much research has reported the challenge of long-term user retention in sleep monitoring through these modalities. Since modern Internet users own multiple mobile devices, our work explores the possibility of employing ubiquitous mobile devices and passive WiFi sensing techniques to predict sleep duration as the fundamental measure for complementing long-term sleep monitoring initiatives. In this paper, we propose SleepMore, an accurate and easy-to-deploy sleep-tracking approach based on machine learning over the user's WiFi network activity. It first employs a semi-personalized random forest model with an infinitesimal jackknife variance estimation method to classify a user's network activity behavior into sleep and awake states per minute granularity. Through a moving average technique, the system uses these state sequences to estimate the user's nocturnal sleep period and its uncertainty rate. Uncertainty quantification enables SleepMore to overcome the impact of noisy WiFi data that can yield large prediction errors. We validate SleepMore using data from a month-long user study involving 46 college students and draw comparisons with the Oura Ring wearable. Beyond the college campus, we evaluate SleepMore on non-student users of different housing profiles. Our results demonstrate that SleepMore produces statistically indistinguishable sleep statistics from the Oura ring baseline for predictions made within a 5\% uncertainty rate. These errors range between 15-28 minutes for determining sleep time and 7-29 minutes for determining wake time, proving statistically significant improvements over prior work. Our in-depth analysis explains the sources of errors. }
}

@article{221106330v1,
  title={ Health Guardian Platform: A technology stack to accelerate discovery in   Digital Health research },
  author={ Bo Wen and Vince S. Siu and Italo Buleje and Kuan Yu Hsieh and Takashi Itoh and Lukas Zimmerli and Nigel Hinds and Elif Eyigoz and Bing Dang and Stefan von Cavallar and Jeffrey L. Rogers },
  journal={ arXiv preprint arXiv:2211.06330v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2211.06330v1 },
  abstract={ This paper highlights the design philosophy and architecture of the Health Guardian, a platform developed by the IBM Digital Health team to accelerate discoveries of new digital biomarkers and development of digital health technologies. The Health Guardian allows for rapid translation of artificial intelligence (AI) research into cloud-based microservices that can be tested with data from clinical cohorts to understand disease and enable early prevention. The platform can be connected to mobile applications, wearables, or Internet of things (IoT) devices to collect health-related data into a secure database. When the analytics are created, the researchers can containerize and deploy their code on the cloud using pre-defined templates, and validate the models using the data collected from one or more sensing devices. The Health Guardian platform currently supports time-series, text, audio, and video inputs with 70+ analytic capabilities and is used for non-commercial scientific research. We provide an example of the Alzheimer's disease (AD) assessment microservice which uses AI methods to extract linguistic features from audio recordings to evaluate an individual's mini-mental state, the likelihood of having AD, and to predict the onset of AD before turning the age of 85. Today, IBM research teams across the globe use the Health Guardian internally as a test bed for early-stage research ideas, and externally with collaborators to support and enhance AI model development and clinical study efforts. }
}

@article{220200711v2,
  title={ A fully Bayesian semi-parametric scalar-on-function regression (SoFR)   with measurement error using instrumental variables },
  author={ Roger S. Zoh and Yuanyuan Luan and Carmen Tekwe },
  journal={ arXiv preprint arXiv:2202.00711v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2202.00711v2 },
  abstract={ Wearable devices such as the ActiGraph are now commonly used in health studies to monitor or track physical activity. This trend aligns well with the growing need to accurately assess the effects of physical activity on health outcomes such as obesity. When accessing the association between these device-based physical activity measures with health outcomes such as body mass index, the device-based data is considered functions, while the outcome is a scalar-valued. The regression model applied in these settings is the scalar-on-function regression (SoFR). Most estimation approaches in SoFR assume that the functional covariates are precisely observed, or the measurement errors are considered random errors. Violation of this assumption can lead to both under-estimation of the model parameters and sub-optimal analysis. The literature on a measurement corrected approach in SoFR is sparse in the non-Bayesian literature and virtually non-existent in the Bayesian literature. This paper considers a fully nonparametric Bayesian measurement error corrected SoFR model that relaxes all the constraining assumptions often made in these models. Our estimation relies on an instrumental variable (IV) to identify the measurement error model. Finally, we introduce an IV quality scalar parameter that is jointly estimated along with all model parameters. Our method is easy to implement, and we demonstrate its finite sample properties through an extensive simulation. Finally, the developed methods are applied to the National Health and Examination Survey to assess the relationship between wearable-device-based measures of physical activity and body mass index among adults living in the United States. }
}

@article{221110371v1,
  title={ Heterogeneous Hidden Markov Models for Sleep Activity Recognition from   Multi-Source Passively Sensed Data },
  author={ Fernando Moreno-Pino and María Martínez-García and Pablo M. Olmos and Antonio Artés-Rodríguez },
  journal={ arXiv preprint arXiv:2211.10371v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2211.10371v1 },
  abstract={ Psychiatric patients' passive activity monitoring is crucial to detect behavioural shifts in real-time, comprising a tool that helps clinicians supervise patients' evolution over time and enhance the associated treatments' outcomes. Frequently, sleep disturbances and mental health deterioration are closely related, as mental health condition worsening regularly entails shifts in the patients' circadian rhythms. Therefore, Sleep Activity Recognition constitutes a behavioural marker to portray patients' activity cycles and to detect behavioural changes among them. Moreover, mobile passively sensed data captured from smartphones, thanks to these devices' ubiquity, constitute an excellent alternative to profile patients' biorhythm.   In this work, we aim to identify major sleep episodes based on passively sensed data. To do so, a Heterogeneous Hidden Markov Model is proposed to model a discrete latent variable process associated with the Sleep Activity Recognition task in a self-supervised way. We validate our results against sleep metrics reported by clinically tested wearables, proving the effectiveness of the proposed approach. }
}

@article{221009499v1,
  title={ Enabling Heterogeneous Domain Adaptation in Multi-inhabitants Smart Home   Activity Learning },
  author={ Md Mahmudur Rahman and Mahta Mousavi and Peri Tarr and Mohammad Arif Ul Alam },
  journal={ arXiv preprint arXiv:2210.09499v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2210.09499v1 },
  abstract={ Domain adaptation for sensor-based activity learning is of utmost importance in remote health monitoring research. However, many domain adaptation algorithms suffer with failure to operate adaptation in presence of target domain heterogeneity (which is always present in reality) and presence of multiple inhabitants dramatically hinders their generalizability producing unsatisfactory results for semi-supervised and unseen activity learning tasks. We propose \\textbackslash{}emph\{AEDA\}, a novel deep auto-encoder-based model to enable semi-supervised domain adaptation in the existence of target domain heterogeneity and how to incorporate it to empower heterogeneity to any homogeneous deep domain adaptation architecture for cross-domain activity learning. Experimental evaluation on 18 different heterogeneous and multi-inhabitants use-cases of 8 different domains created from 2 publicly available human activity datasets (wearable and ambient smart homes) shows that \\textbackslash{}emph\{AEDA\} outperforms (max. 12.8\\textbackslash{}\% and 8.9\\textbackslash{}\% improvements for ambient smart home and wearables) over existing domain adaptation techniques for both seen and unseen activity learning in a heterogeneous setting. }
}

@article{221007475v1,
  title={ Latent Temporal Flows for Multivariate Analysis of Wearables Data },
  author={ Magda Amiridi and Gregory Darnell and Sean Jewell },
  journal={ arXiv preprint arXiv:2210.07475v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2210.07475v1 },
  abstract={ Increased use of sensor signals from wearable devices as rich sources of physiological data has sparked growing interest in developing health monitoring systems to identify changes in an individual's health profile. Indeed, machine learning models for sensor signals have enabled a diverse range of healthcare related applications including early detection of abnormalities, fertility tracking, and adverse drug effect prediction. However, these models can fail to account for the dependent high-dimensional nature of the underlying sensor signals. In this paper, we introduce Latent Temporal Flows, a method for multivariate time-series modeling tailored to this setting. We assume that a set of sequences is generated from a multivariate probabilistic model of an unobserved time-varying low-dimensional latent vector. Latent Temporal Flows simultaneously recovers a transformation of the observed sequences into lower-dimensional latent representations via deep autoencoder mappings, and estimates a temporally-conditioned probabilistic model via normalizing flows. Using data from the Apple Heart and Movement Study (AH\&MS), we illustrate promising forecasting performance on these challenging signals. Additionally, by analyzing two and three dimensional representations learned by our model, we show that we can identify participants' \$\\textbackslash{}text\{VO\}\_2\\textbackslash{}text\{max\}\$, a main indicator and summary of cardio-respiratory fitness, using only lower-level signals. Finally, we show that the proposed method consistently outperforms the state-of-the-art in multi-step forecasting benchmarks (achieving at least a \$10\\textbackslash{}\%\$ performance improvement) on several real-world datasets, while enjoying increased computational efficiency. }
}

@article{221005881v1,
  title={ Deterioration Prediction using Time-Series of Three Vital Signs and   Current Clinical Features Amongst COVID-19 Patients },
  author={ Sarmad Mehrdad and Farah E. Shamout and Yao Wang and S. Farokh Atashzar },
  journal={ arXiv preprint arXiv:2210.05881v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2210.05881v1 },
  abstract={ Unrecognized patient deterioration can lead to high morbidity and mortality. Most existing deterioration prediction models require a large number of clinical information, typically collected in hospital settings, such as medical images or comprehensive laboratory tests. This is infeasible for telehealth solutions and highlights a gap in deterioration prediction models that are based on minimal data, which can be recorded at a large scale in any clinic, nursing home, or even at the patient's home. In this study, we propose and develop a prognostic model that predicts if a patient will experience deterioration in the forthcoming 3-24 hours. The model sequentially processes routine triadic vital signs: (a) oxygen saturation, (b) heart rate, and (c) temperature. The model is also provided with basic patient information, including sex, age, vaccination status, vaccination date, and status of obesity, hypertension, or diabetes. We train and evaluate the model using data collected from 37,006 COVID-19 patients at NYU Langone Health in New York, USA. The model achieves an area under the receiver operating characteristic curve (AUROC) of 0.808-0.880 for 3-24 hour deterioration prediction. We also conduct occlusion experiments to evaluate the importance of each input feature, where the results reveal the significance of continuously monitoring the variations of the vital signs. Our results show the prospect of accurate deterioration forecast using a minimum feature set that can be relatively easily obtained using wearable devices and self-reported patient information. }
}

@article{220809819v1,
  title={ Robust Tests in Online Decision-Making },
  author={ Gi-Soo Kim and Hyun-Joon Yang and Jane P. Kim },
  journal={ arXiv preprint arXiv:2208.09819v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2208.09819v1 },
  abstract={ Bandit algorithms are widely used in sequential decision problems to maximize the cumulative reward. One potential application is mobile health, where the goal is to promote the user's health through personalized interventions based on user specific information acquired through wearable devices. Important considerations include the type of, and frequency with which data is collected (e.g. GPS, or continuous monitoring), as such factors can severely impact app performance and users' adherence. In order to balance the need to collect data that is useful with the constraint of impacting app performance, one needs to be able to assess the usefulness of variables. Bandit feedback data are sequentially correlated, so traditional testing procedures developed for independent data cannot apply. Recently, a statistical testing procedure was developed for the actor-critic bandit algorithm. An actor-critic algorithm maintains two separate models, one for the actor, the action selection policy, and the other for the critic, the reward model. The performance of the algorithm as well as the validity of the test are guaranteed only when the critic model is correctly specified. However, misspecification is frequent in practice due to incorrect functional form or missing covariates. In this work, we propose a modified actor-critic algorithm which is robust to critic misspecification and derive a novel testing procedure for the actor parameters in this case. }
}

@article{220801095v1,
  title={ Efficient Personalized Learning for Wearable Health Applications using   HyperDimensional Computing },
  author={ Sina Shahhosseini and Yang Ni and Hamidreza Alikhani and Emad Kasaeyan Naeini and Mohsen Imani and Nikil Dutt and Amir M. Rahmani },
  journal={ arXiv preprint arXiv:2208.01095v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2208.01095v1 },
  abstract={ Health monitoring applications increasingly rely on machine learning techniques to learn end-user physiological and behavioral patterns in everyday settings. Considering the significant role of wearable devices in monitoring human body parameters, on-device learning can be utilized to build personalized models for behavioral and physiological patterns, and provide data privacy for users at the same time. However, resource constraints on most of these wearable devices prevent the ability to perform online learning on them. To address this issue, it is required to rethink the machine learning models from the algorithmic perspective to be suitable to run on wearable devices. Hyperdimensional computing (HDC) offers a well-suited on-device learning solution for resource-constrained devices and provides support for privacy-preserving personalization. Our HDC-based method offers flexibility, high efficiency, resilience, and performance while enabling on-device personalization and privacy protection. We evaluate the efficacy of our approach using three case studies and show that our system improves the energy efficiency of training by up to \$45.8\\textbackslash{}times\$ compared with the state-of-the-art Deep Neural Network (DNN) algorithms while offering a comparable accuracy. }
}

@article{220714640v1,
  title={ EmoSens: Emotion Recognition based on Sensor data analysis using   LightGBM },
  author={ Gayathri S and Akshat Anand and Astha Vijayvargiya and Pushpalatha M and Vaishnavi Moorthy and Sumit Kumar and Harichandana B S S },
  journal={ arXiv preprint arXiv:2207.14640v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2207.14640v1 },
  abstract={ Smart wearables have played an integral part in our day to day life. From recording ECG signals to analysing body fat composition, the smart wearables can do it all. The smart devices encompass various sensors which can be employed to derive meaningful information regarding the user's physical and psychological conditions. Our approach focuses on employing such sensors to identify and obtain the variations in the mood of a user at a given instance through the use of supervised machine learning techniques. The study examines the performance of various supervised learning models such as Decision Trees, Random Forests, XGBoost, LightGBM on the dataset. With our proposed model, we obtained a high recognition rate of 92.5\% using XGBoost and LightGBM for 9 different emotion classes. By utilizing this, we aim to improvise and suggest methods to aid emotion recognition for better mental health analysis and mood monitoring. }
}

@article{220601339v1,
  title={ A peristaltic soft, wearable robot for compression and massage therapy },
  author={ Mengjia Zhu and Adrian Ferstera and Stejara Dinulescu and Nikolas Kastor and Max Linnander and Elliot W. Hawkes and Yon Visell },
  journal={ arXiv preprint arXiv:2206.01339v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2206.01339v1 },
  abstract={ Soft robotics is attractive for wearable applications that require conformal interactions with the human body. Soft wearable robotic garments hold promise for supplying dynamic compression or massage therapies, such as are applied for disorders affecting lymphatic and blood circulation. In this paper, we present a wearable robot capable of supplying dynamic compression and massage therapy via peristaltic motion of finger-sized soft, fluidic actuators. We show that this peristaltic wearable robot can supply dynamic compression pressures exceeding 22 kPa at frequencies of 14 Hz or more, meeting requirements for compression and massage therapy. A large variety of software-programmable compression wave patterns can be generated by varying frequency, amplitude, phase delay, and duration parameters. We first demonstrate the utility of this peristaltic wearable robot for compression therapy, showing fluid transport in a laboratory model of the upper limb. We theoretically and empirically identify driving regimes that optimize fluid transport. We second demonstrate the utility of this garment for dynamic massage therapy. These findings show the potential of such a wearable robot for the treatment of several health disorders associated with lymphatic and blood circulation, such as lymphedema and blood clots. }
}

@article{220508577v1,
  title={ PROLIFIC: Projection-based Test for Lack of Importance of Smooth   Functional Effect in Crossover Design },
  author={ Salil Koner and Ana-Maria Staicu and Arnab Maity },
  journal={ arXiv preprint arXiv:2205.08577v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2205.08577v1 },
  abstract={ Wearable devices for continuous monitoring of electronic health increased attention due to their richness in information. Often, inference is drawn from features that quantify some summary of the data, leading to a loss of information that could be useful when one utilizes the functional nature of the response. When functional trajectories are observed repeated over time, it is termed longitudinal functional data. This work is motivated by the interest to assess the efficacy of a noninflammatory medication, meloxicam, on the daily activity levels of household cats with a pre-existing condition of osteoarthritis under a crossover design. These activity profiles are recorded at a minute level by accelerometer over the entire study period. To this aspect, we propose an orthogonal projection-based test pseudo generalized F test for significance of the functional treatment effect under a functional additive crossover model after adjusting for the carryover effect and other baseline covariates. Under mild conditions, we derive the asymptotic null distribution of the test statistic when the projection function for the underlying Hilbert space is estimated from the data. In finite sample numerical studies, the proposed test maintains the size, is powerful to detect the significance of the smooth effect of meloxicam, and is very efficient compared to bootstrap-based alternatives. }
}

@article{220507800v1,
  title={ Design and Evaluation of an Invariant Extended Kalman Filter for Trunk   Motion Estimation with Sensor Misalignment },
  author={ Zenan Zhu and Seyed Mostafa Rezayat Sorkhabadi and Yan Gu and Wenlong Zhang },
  journal={ arXiv preprint arXiv:2205.07800v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2205.07800v1 },
  abstract={ Understanding human motion is of critical importance for health monitoring and control of assistive robots, yet many human kinematic variables cannot be directly or accurately measured by wearable sensors. In recent years, invariant extended Kalman filtering (InEKF) has shown a great potential in nonlinear state estimation, but its applications to human poses new challenges, including imperfect placement of wearable sensors and inaccurate measurement models. To address these challenges, this paper proposes an augmented InEKF design which considers the misalignment of the inertial sensor at the trunk as part of the states and preserves the group affine property for the process model. Personalized lower-extremity forward kinematic models are built and employed as the measurement model for the augmented InEKF. Observability analysis for the new InEKF design is presented. The filter is evaluated with three subjects in squatting, rolling-foot walking, and ladder-climbing motions. Experimental results validate the superior performance of the proposed InEKF over the state-of-the-art InEKF. Improved accuracy and faster convergence in estimating the velocity and orientation of human, in all three motions, are achieved despite the significant initial estimation errors and the uncertainties associated with the forward kinematic measurement model. }
}

@article{220508409v1,
  title={ Automated Mobility Context Detection with Inertial Signals },
  author={ Antonio Bevilacqua and Lisa Alcock and Brian Caulfield and Eran Gazit and Clint Hansen and Neil Ireson and Georgiana Ifrim },
  journal={ arXiv preprint arXiv:2205.08409v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2205.08409v1 },
  abstract={ Remote monitoring of motor functions is a powerful approach for health assessment, especially among the elderly population or among subjects affected by pathologies that negatively impact their walking capabilities. This is further supported by the continuous development of wearable sensor devices, which are getting progressively smaller, cheaper, and more energy efficient. The external environment and mobility context have an impact on walking performance, hence one of the biggest challenges when remotely analysing gait episodes is the ability to detect the context within which those episodes occurred. The primary goal of this paper is the investigation of context detection for remote monitoring of daily motor functions. We aim to understand whether inertial signals sampled with wearable accelerometers, provide reliable information to classify gait-related activities as either indoor or outdoor. We explore two different approaches to this task: (1) using gait descriptors and features extracted from the input inertial signals sampled during walking episodes, together with classic machine learning algorithms, and (2) treating the input inertial signals as time series data and leveraging end-to-end state-of-the-art time series classifiers. We directly compare the two approaches through a set of experiments based on data collected from 9 healthy individuals. Our results indicate that the indoor/outdoor context can be successfully derived from inertial data streams. We also observe that time series classification models achieve better accuracy than any other feature-based models, while preserving efficiency and ease of use. }
}

@article{220205735v4,
  title={ SleepPPG-Net: a deep learning algorithm for robust sleep staging from   continuous photoplethysmography },
  author={ Kevin Kotzen and Peter H. Charlton and Sharon Salabi and Lea Amar and Amir Landesberg and Joachim A. Behar },
  journal={ arXiv preprint arXiv:2202.05735v4 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2202.05735v4 },
  abstract={ Introduction: Sleep staging is an essential component in the diagnosis of sleep disorders and management of sleep health. It is traditionally measured in a clinical setting and requires a labor-intensive labeling process. We hypothesize that it is possible to perform robust 4-class sleep staging using the raw photoplethysmography (PPG) time series and modern advances in deep learning (DL). Methods: We used two publicly available sleep databases that included raw PPG recordings, totalling 2,374 patients and 23,055 hours. We developed SleepPPG-Net, a DL model for 4-class sleep staging from the raw PPG time series. SleepPPG-Net was trained end-to-end and consists of a residual convolutional network for automatic feature extraction and a temporal convolutional network to capture long-range contextual information. We benchmarked the performance of SleepPPG-Net against models based on the best-reported state-of-the-art (SOTA) algorithms. Results: When benchmarked on a held-out test set, SleepPPG-Net obtained a median Cohen's Kappa (\$\\textbackslash{}kappa\$) score of 0.75 against 0.69 for the best SOTA approach. SleepPPG-Net showed good generalization performance to an external database, obtaining a \$\\textbackslash{}kappa\$ score of 0.74 after transfer learning. Perspective: Overall, SleepPPG-Net provides new SOTA performance. In addition, performance is high enough to open the path to the development of wearables that meet the requirements for usage in clinical applications such as the diagnosis and monitoring of obstructive sleep apnea. }
}

@article{220104039v2,
  title={ MobilePhys: Personalized Mobile Camera-Based Contactless Physiological   Sensing },
  author={ Xin Liu and Yuntao Wang and Sinan Xie and Xiaoyu Zhang and Zixian Ma and Daniel McDuff and Shwetak Patel },
  journal={ arXiv preprint arXiv:2201.04039v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2201.04039v2 },
  abstract={ Camera-based contactless photoplethysmography refers to a set of popular techniques for contactless physiological measurement. The current state-of-the-art neural models are typically trained in a supervised manner using videos accompanied by gold standard physiological measurements. However, they often generalize poorly out-of-domain examples (i.e., videos that are unlike those in the training set). Personalizing models can help improve model generalizability, but many personalization techniques still require some gold standard data. To help alleviate this dependency, in this paper, we present a novel mobile sensing system called MobilePhys, the first mobile personalized remote physiological sensing system, that leverages both front and rear cameras on a smartphone to generate high-quality self-supervised labels for training personalized contactless camera-based PPG models. To evaluate the robustness of MobilePhys, we conducted a user study with 39 participants who completed a set of tasks under different mobile devices, lighting conditions/intensities, motion tasks, and skin types. Our results show that MobilePhys significantly outperforms the state-of-the-art on-device supervised training and few-shot adaptation methods. Through extensive user studies, we further examine how does MobilePhys perform in complex real-world settings. We envision that calibrated or personalized camera-based contactless PPG models generated from our proposed dual-camera mobile sensing system will open the door for numerous future applications such as smart mirrors, fitness and mobile health applications. }
}

@article{220407038v1,
  title={ OMAD: On-device Mental Anomaly Detection for Substance and Non-Substance   Users },
  author={ Emon Dey and Nirmalya Roy },
  journal={ arXiv preprint arXiv:2204.07038v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2204.07038v1 },
  abstract={ Stay at home order during the COVID-19 helps flatten the curve but ironically, instigate mental health problems among the people who have Substance Use Disorders. Measuring the electrical activity signals in brain using off-the-shelf consumer wearable devices such as smart wristwatch and mapping them in real time to underlying mood, behavioral and emotional changes play striking roles in postulating mental health anomalies. In this work, we propose to implement a wearable, \{\\textbackslash{}it On-device Mental Anomaly Detection (OMAD)\} system to detect anomalous behaviors and activities that render to mental health problems and help clinicians to design effective intervention strategies. We propose an intrinsic artifact removal model on Electroencephalogram (EEG) signal to better correlate the fine-grained behavioral changes. We design model compression technique on the artifact removal and activity recognition (main) modules. We implement a magnitude-based weight pruning technique both on convolutional neural network and Multilayer Perceptron to employ the inference phase on Nvidia Jetson Nano; one of the tightest resource-constrained devices for wearables. We experimented with three different combinations of feature extractions and artifact removal approaches. We evaluate the performance of \{\\textbackslash{}it OMAD\} in terms of accuracy, F1 score, memory usage and running time for both unpruned and compressed models using EEG data from both control and treatment (alcoholic) groups for different object recognition tasks. Our artifact removal model and main activity detection model achieved about \$\\textbackslash{}approx\$ 93\\textbackslash{}\% and 90\\textbackslash{}\% accuracy, respectively with significant reduction in model size (70\\textbackslash{}\%) and inference time (31\\textbackslash{}\%). }
}

@article{220403573v1,
  title={ An optimized hybrid solution for IoT based lifestyle disease   classification using stress data },
  author={ Sadhana Tiwari and Sonali Agarwal },
  journal={ arXiv preprint arXiv:2204.03573v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2204.03573v1 },
  abstract={ Stress, anxiety, and nervousness are all high-risk health states in everyday life. Previously, stress levels were determined by speaking with people and gaining insight into what they had experienced recently or in the past. Typically, stress is caused by an incidence that occurred a long time ago, but sometimes it is triggered by unknown factors. This is a challenging and complex task, but recent research advances have provided numerous opportunities to automate it. The fundamental features of most of these techniques are electro dermal activity (EDA) and heart rate values (HRV). We utilized an accelerometer to measure body motions to solve this challenge. The proposed novel method employs a test that measures a subject's electrocardiogram (ECG), galvanic skin values (GSV), HRV values, and body movements in order to provide a low-cost and time-saving solution for detecting stress lifestyle disease in modern times using cyber physical systems. This study provides a new hybrid model for lifestyle disease classification that decreases execution time while picking the best collection of characteristics and increases classification accuracy. The developed approach is capable of dealing with the class imbalance problem by using WESAD (wearable stress and affect dataset) dataset. The new model uses the Grid search (GS) method to select an optimized set of hyper parameters, and it uses a combination of the Correlation coefficient based Recursive feature elimination (CoC-RFE) method for optimal feature selection and gradient boosting as an estimator to classify the dataset, which achieves high accuracy and helps to provide smart, accurate, and high-quality healthcare systems. To demonstrate the validity and utility of the proposed methodology, its performance is compared to those of other well-established machine learning models. }
}

@article{220315183v1,
  title={ Visualizations of Complex Sequences of Family-Infant Vocalizations Using   Bag-of-Audio-Words Approach Based on Wav2vec 2.0 Features },
  author={ Jialu Li and Mark Hasegawa-Johnson and Nancy L. McElwain },
  journal={ arXiv preprint arXiv:2203.15183v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2203.15183v1 },
  abstract={ In the U.S., approximately 15-17\% of children 2-8 years of age are estimated to have at least one diagnosed mental, behavioral or developmental disorder. However, such disorders often go undiagnosed, and the ability to evaluate and treat disorders in the first years of life is limited. To analyze infant developmental changes, previous studies have shown advanced ML models excel at classifying infant and/or parent vocalizations collected using cell phone, video, or audio-only recording device like LENA. In this study, we pilot test the audio component of a new infant wearable multi-modal device that we have developed called LittleBeats (LB). LB audio pipeline is advanced in that it provides reliable labels for both speaker diarization and vocalization classification tasks, compared with other platforms that only record audio and/or provide speaker diarization labels. We leverage wav2vec 2.0 to obtain superior and more nuanced results with the LB family audio stream. We use a bag-of-audio-words method with wav2vec 2.0 features to create high-level visualizations to understand family-infant vocalization interactions. We demonstrate that our high-quality visualizations capture major types of family vocalization interactions, in categories indicative of mental, behavioral, and developmental health, for both labeled and unlabeled LB audio. }
}

@article{220312200v1,
  title={ Privacy-Preserving Personalized Fitness Recommender System (P3FitRec): A   Multi-level Deep Learning Approach },
  author={ Xiao Liu and Bonan Gao and Basem Suleiman and Han You and Zisu Ma and Yu Liu and Ali Anaissi },
  journal={ arXiv preprint arXiv:2203.12200v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2203.12200v1 },
  abstract={ Recommender systems have been successfully used in many domains with the help of machine learning algorithms. However, such applications tend to use multi-dimensional user data, which has raised widespread concerns about the breach of users privacy. Meanwhile, wearable technologies have enabled users to collect fitness-related data through embedded sensors to monitor their conditions or achieve personalized fitness goals. In this paper, we propose a novel privacy-aware personalized fitness recommender system. We introduce a multi-level deep learning framework that learns important features from a large-scale real fitness dataset that is collected from wearable IoT devices to derive intelligent fitness recommendations. Unlike most existing approaches, our approach achieves personalization by inferring the fitness characteristics of users from sensory data and thus minimizing the need for explicitly collecting user identity or biometric information, such as name, age, height, weight. In particular, our proposed models and algorithms predict (a) personalized exercise distance recommendations to help users to achieve target calories, (b) personalized speed sequence recommendations to adjust exercise speed given the nature of the exercise and the chosen route, and (c) personalized heart rate sequence to guide the user of the potential health status for future exercises. Our experimental evaluation on a real-world Fitbit dataset demonstrated high accuracy in predicting exercise distance, speed sequence, and heart rate sequence compared to similar studies. Furthermore, our approach is novel compared to existing studies as it does not require collecting and using users sensitive information, and thus it preserves the users privacy. }
}

@article{220311294v1,
  title={ Automated detection of foreground speech with wearable sensing in   everyday home environments: A transfer learning approach },
  author={ Dawei Liang and Zifan Xu and Yinuo Chen and Rebecca Adaimi and David Harwath and Edison Thomaz },
  journal={ arXiv preprint arXiv:2203.11294v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2203.11294v1 },
  abstract={ Acoustic sensing has proved effective as a foundation for numerous applications in health and human behavior analysis. In this work, we focus on the problem of detecting in-person social interactions in naturalistic settings from audio captured by a smartwatch. As a first step towards detecting social interactions, it is critical to distinguish the speech of the individual wearing the watch from all other sounds nearby, such as speech from other individuals and ambient sounds. This is very challenging in realistic settings, where interactions take place spontaneously and supervised models cannot be trained apriori to recognize the full complexity of dynamic social environments. In this paper, we introduce a transfer learning-based approach to detect foreground speech of users wearing a smartwatch. A highlight of the method is that it does not depend on the collection of voice samples to build user-specific models. Instead, the approach is based on knowledge transfer from general-purpose speaker representations derived from public datasets. Our experiments demonstrate that our approach performs comparably to a fully supervised model, with 80\% F1 score. To evaluate the method, we collected a dataset of 31 hours of smartwatch-recorded audio in 18 homes with a total of 39 participants performing various semi-controlled tasks. }
}

@article{220312595v1,
  title={ PhysioMTL: Personalizing Physiological Patterns using Optimal Transport   Multi-Task Regression },
  author={ Jiacheng Zhu and Gregory Darnell and Agni Kumar and Ding Zhao and Bo Li and Xuanlong Nguyen and Shirley You Ren },
  journal={ arXiv preprint arXiv:2203.12595v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2203.12595v1 },
  abstract={ Heart rate variability (HRV) is a practical and noninvasive measure of autonomic nervous system activity, which plays an essential role in cardiovascular health. However, using HRV to assess physiology status is challenging. Even in clinical settings, HRV is sensitive to acute stressors such as physical activity, mental stress, hydration, alcohol, and sleep. Wearable devices provide convenient HRV measurements, but the irregularity of measurements and uncaptured stressors can bias conventional analytical methods. To better interpret HRV measurements for downstream healthcare applications, we learn a personalized diurnal rhythm as an accurate physiological indicator for each individual. We develop Physiological Multitask-Learning (PhysioMTL) by harnessing Optimal Transport theory within a Multitask-learning (MTL) framework. The proposed method learns an individual-specific predictive model from heterogeneous observations, and enables estimation of an optimal transport map that yields a push forward operation onto the demographic features for each task. Our model outperforms competing MTL methodologies on unobserved predictive tasks for synthetic and two real-world datasets. Specifically, our method provides remarkable prediction results on unseen held-out subjects given only \$20\\textbackslash{}\%\$ of the subjects in real-world observational studies. Furthermore, our model enables a counterfactual engine that generates the effect of acute stressors and chronic conditions on HRV rhythms. }
}

@article{220309669v1,
  title={ Analysing the Performance of Stress Detection Models on Consumer-Grade   Wearable Devices },
  author={ Van-Tu Ninh and Sinéad Smyth and Minh-Triet Tran and Cathal Gurrin },
  journal={ arXiv preprint arXiv:2203.09669v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2203.09669v1 },
  abstract={ Identifying stress levels can provide valuable data for mental health analytics as well as labels for annotation systems. Although much research has been conducted into stress detection models using heart rate variability at a higher cost of data collection, there is a lack of research on the potential of using low-resolution Electrodermal Activity (EDA) signals from consumer-grade wearable devices to identify stress patterns. In this paper, we concentrate on performing statistical analyses on the stress detection capability of two popular approaches of training stress detection models with stress-related biometric signals: user-dependent and user-independent models. Our research manages to show that user-dependent models are statistically more accurate for stress detection. In terms of effectiveness assessment, the balanced accuracy (BA) metric is employed to evaluate the capability of distinguishing stress and non-stress conditions of the models trained on either low-resolution or high-resolution Electrodermal Activity (EDA) signals. The results from the experiment show that training the model with (comparatively low-cost) low-resolution EDA signal does not affect the stress detection accuracy of the model significantly compared to using a high-resolution EDA signal. Our research results demonstrate the potential of attaching the user-dependent stress detection model trained on personal low-resolution EDA signal recorded to collect data in daily life to provide users with personal stress level insight and analysis. }
}

@article{220212582v2,
  title={ A Blockchain-Based Consent Mechanism for Access to Fitness Data in the   Healthcare Context },
  author={ May Alhajri and Carsten Rudolph and Ahmad Salehi Shahraki },
  journal={ arXiv preprint arXiv:2202.12582v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2202.12582v2 },
  abstract={ Wearable fitness devices are widely used to track an individual's health and physical activities to improve the quality of health services. These devices sense a considerable amount of sensitive data processed by a centralized third party. While many researchers have thoroughly evaluated privacy issues surrounding wearable fitness trackers, no study has addressed privacy issues in trackers by giving control of the data to the user. Blockchain is an emerging technology with outstanding advantages in resolving consent management privacy concerns. As there are no fully transparent, legally compliant solutions for sharing personal fitness data, this study introduces an architecture for a human-centric, legally compliant, decentralized and dynamic consent system based on blockchain and smart contracts. Algorithms and sequence diagrams of the proposed system's activities show consent-related data flow among various agents, which are used later to prove the system's trustworthiness by formalizing the security requirements. The security properties of the proposed system were evaluated using the formal security modeling framework SeMF, which demonstrates the feasibility of the solution at an abstract level based on formal language theory. As a result, we have empirically proven that blockchain technology is suitable for mitigating the privacy issues of fitness providers by recording individuals' consent using blockchain and smart contracts. }
}

@article{220300503v1,
  title={ Gait Events Prediction using Hybrid CNN-RNN-based Deep Learning models   through a Single Waist-worn Wearable Sensor },
  author={ Muhammad Zeeshan Arshad and Ankhzaya Jamsrandorj and Jinwook Kim and Kyung-Ryoul Mun },
  journal={ arXiv preprint arXiv:2203.00503v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2203.00503v1 },
  abstract={ Elderly gait is a source of rich information about their physical and mental health condition. As an alternative to the multiple sensors on the lower body parts, a single sensor on the pelvis has a positional advantage and an abundance of information acquirable. This study aimed to explore a way of improving the accuracy of gait event detection in the elderly using a single sensor on the waist and deep learning models. Data was gathered from elderly subjects equipped with three IMU sensors while they walked. The input was taken only from the waist sensor was used to train 16 deep-learning models including CNN, RNN, and CNN-RNN hybrid with or without the Bidirectional and Attention mechanism. The groundtruth was extracted from foot IMU sensors. Fairly high accuracy of 99.73\% and 93.89\% was achieved by the CNN-BiGRU-Att model at the tolerance window of \$\\textbackslash{}pm\$6TS (\$\\textbackslash{}pm\$6ms) and \$\\textbackslash{}pm\$1TS (\$\\textbackslash{}pm\$1ms) respectively. Advancing from the previous studies exploring gait event detection, the model showed a great improvement in terms of its prediction error having an MAE of 6.239ms and 5.24ms for HS and TO events respectively at the tolerance window of \$\\textbackslash{}pm\$1TS. The results showed that the use of CNN-RNN hybrid models with Attention and Bidirectional mechanisms is promising for accurate gait event detection using a single waist sensor. The study can contribute to reducing the burden of gait detection and increase its applicability in future wearable devices that can be used for remote health monitoring (RHM) or diagnosis based thereon. }
}

@article{220208267v1,
  title={ More to Less (M2L): Enhanced Health Recognition in the Wild with Reduced   Modality of Wearable Sensors },
  author={ Huiyuan Yang and Han Yu and Kusha Sridhar and Thomas Vaessen and Inez Myin-Germeys and Akane Sano },
  journal={ arXiv preprint arXiv:2202.08267v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2202.08267v1 },
  abstract={ Accurately recognizing health-related conditions from wearable data is crucial for improved healthcare outcomes. To improve the recognition accuracy, various approaches have focused on how to effectively fuse information from multiple sensors. Fusing multiple sensors is a common scenario in many applications, but may not always be feasible in real-world scenarios. For example, although combining bio-signals from multiple sensors (i.e., a chest pad sensor and a wrist wearable sensor) has been proved effective for improved performance, wearing multiple devices might be impractical in the free-living context. To solve the challenges, we propose an effective more to less (M2L) learning framework to improve testing performance with reduced sensors through leveraging the complementary information of multiple modalities during training. More specifically, different sensors may carry different but complementary information, and our model is designed to enforce collaborations among different modalities, where positive knowledge transfer is encouraged and negative knowledge transfer is suppressed, so that better representation is learned for individual modalities. Our experimental results show that our framework achieves comparable performance when compared with the full modalities. Our code and results will be available at https://github.com/compwell-org/More2Less.git. }
}

@article{220200606v1,
  title={ Signal Quality Assessment of Photoplethysmogram Signals using Quantum   Pattern Recognition and lightweight CNN Architecture },
  author={ Tamaghno Chatterjee and Aayushman Ghosh and Sayan Sarkar },
  journal={ arXiv preprint arXiv:2202.00606v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2202.00606v1 },
  abstract={ Photoplethysmography (PPG) signal comprises physiological information related to cardiorespiratory health. However, while recording, these PPG signals are easily corrupted by motion artifacts and body movements, leading to noise enriched, poor quality signals. Therefore ensuring high-quality signals is necessary to extract cardiorespiratory information accurately. Although there exists several rule-based and Machine-Learning (ML) - based approaches for PPG signal quality estimation, those algorithms' efficacy is questionable. Thus, this work proposes a lightweight CNN architecture for signal quality assessment employing a novel Quantum pattern recognition (QPR) technique. The proposed algorithm is validated on manually annotated data obtained from the University of Queensland database. A total of 28366, 5s signal segments are preprocessed and transformed into image files of 20 x 500 pixels. The image files are treated as an input to the 2D CNN architecture. The developed model classifies the PPG signal as `good' or `bad' with an accuracy of 98.3\% with 99.3\% sensitivity, 94.5\% specificity and 98.9\% F1-score. Finally, the performance of the proposed framework is validated against the noisy `Welltory app' collected PPG database. Even in a noisy environment, the proposed architecture proved its competence. Experimental analysis concludes that a slim architecture along with a novel Spatio-temporal pattern recognition technique improve the system's performance. Hence, the proposed approach can be useful to classify good and bad PPG signals for a resource-constrained wearable implementation. }
}

@article{220110083v1,
  title={ A Wearable ECG Monitor for Deep Learning Based Real-Time Cardiovascular   Disease Detection },
  author={ Peng Wang and Zihuai Lin and Xucun Yan and Zijiao Chen and Ming Ding and Yang Song and Lu Meng },
  journal={ arXiv preprint arXiv:2201.10083v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2201.10083v1 },
  abstract={ Cardiovascular disease has become one of the most significant threats endangering human life and health. Recently, Electrocardiogram (ECG) monitoring has been transformed into remote cardiac monitoring by Holter surveillance. However, the widely used Holter can bring a great deal of discomfort and inconvenience to the individuals who carry them. We developed a new wireless ECG patch in this work and applied a deep learning framework based on the Convolutional Neural Network (CNN) and Long Short-term Memory (LSTM) models. However, we find that the models using the existing techniques are not able to differentiate two main heartbeat types (Supraventricular premature beat and Atrial fibrillation) in our newly obtained dataset, resulting in low accuracy of 58.0 \%. We proposed a semi-supervised method to process the badly labelled data samples with using the confidence-level-based training. The experiment results conclude that the proposed method can approach an average accuracy of 90.2 \%, i.e., 5.4 \% higher than the accuracy of conventional ECG classification methods. }
}

@article{220107888v1,
  title={ Adaptive Energy Management for Self-Sustainable Wearables in Mobile   Health },
  author={ Dina Hussein and Ganapati Bhat and Janardhan Rao Doppa },
  journal={ arXiv preprint arXiv:2201.07888v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2201.07888v1 },
  abstract={ Wearable devices that integrate multiple sensors, processors, and communication technologies have the potential to transform mobile health for remote monitoring of health parameters. However, the small form factor of the wearable devices limits the battery size and operating lifetime. As a result, the devices require frequent recharging, which has limited their widespread adoption. Energy harvesting has emerged as an effective method towards sustainable operation of wearable devices. Unfortunately, energy harvesting alone is not sufficient to fulfill the energy requirements of wearable devices. This paper studies the novel problem of adaptive energy management towards the goal of self-sustainable wearables by using harvested energy to supplement the battery energy and to reduce manual recharging by users. To solve this problem, we propose a principled algorithm referred as AdaEM. There are two key ideas behind AdaEM. First, it uses machine learning (ML) methods to learn predictive models of user activity and energy usage patterns. These models allow us to estimate the potential of energy harvesting in a day as a function of the user activities. Second, it reasons about the uncertainty in predictions and estimations from the ML models to optimize the energy management decisions using a dynamic robust optimization (DyRO) formulation. We propose a light-weight solution for DyRO to meet the practical needs of deployment. We validate the AdaEM approach on a wearable device prototype consisting of solar and motion energy harvesting using real-world data of user activities. Experiments show that AdaEM achieves solutions that are within 5\% of the optimal with less than 0.005\% execution time and energy overhead. }
}

@article{200601169v2,
  title={ RNNs on Monitoring Physical Activity Energy Expenditure in Older People },
  author={ Stylianos Paraschiakos and Cláudio Rebelo de Sá and Jeremiah Okai and Eline P. Slagboom and Marian Beekman and Arno Knobbe },
  journal={ arXiv preprint arXiv:2006.01169v2 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2006.01169v2 },
  abstract={ Through the quantification of physical activity energy expenditure (PAEE), health care monitoring has the potential to stimulate vital and healthy ageing, inducing behavioural changes in older people and linking these to personal health gains. To be able to measure PAEE in a monitoring environment, methods from wearable accelerometers have been developed, however, mainly targeted towards younger people. Since elderly subjects differ in energy requirements and range of physical activities, the current models may not be suitable for estimating PAEE among the elderly. Because past activities influence present PAEE, we propose a modeling approach known for its ability to model sequential data, the Recurrent Neural Network (RNN). To train the RNN for an elderly population, we used the GOTOV dataset with 34 healthy participants of 60 years and older (mean 65 years old), performing 16 different activities. We used accelerometers placed on wrist and ankle, and measurements of energy counts by means of indirect calorimetry. After optimization, we propose an architecture consisting of an RNN with 3 GRU layers and a feedforward network combining both accelerometer and participant-level data. In this paper, we describe our efforts to go beyond the standard facilities of a GRU-based RNN, with the aim of achieving accuracy surpassing the state of the art. These efforts include switching aggregation function from mean to dispersion measures (SD, IQR, ...), combining temporal and static data (person-specific details such as age, weight, BMI) and adding symbolic activity data as predicted by a previously trained ML model. The resulting architecture manages to increase its performance by approximatelly 10\% while decreasing training input by a factor of 10. It can thus be employed to investigate associations of PAEE with vitality parameters related to metabolic and cognitive health and mental well-being. }
}

@article{211213755v1,
  title={ Self-supervision of wearable sensors time-series data for influenza   detection },
  author={ Arinbjörn Kolbeinsson and Piyusha Gade and Raghu Kainkaryam and Filip Jankovic and Luca Foschini },
  journal={ arXiv preprint arXiv:2112.13755v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2112.13755v1 },
  abstract={ Self-supervision may boost model performance in downstream tasks. However, there is no principled way of selecting the self-supervised objectives that yield the most adaptable models. Here, we study this problem on daily time-series data generated from wearable sensors used to detect onset of influenza-like illness (ILI). We first show that using self-supervised learning to predict next-day time-series values allows us to learn rich representations which can be adapted to perform accurate ILI prediction. Second, we perform an empirical analysis of three different self-supervised objectives to assess their adaptability to ILI prediction. Our results show that predicting the next day's resting heart rate or time-in-bed during sleep provides better representations for ILI prediction. These findings add to previous work demonstrating the practical application of self-supervised learning from activity data to improve health predictions. }
}

@article{211203539v1,
  title={ A Function-Based Approach to Model the Measurement Error in Wearable   Devices },
  author={ Sneha Jadhav and Carmen D. Tekwe and Yuanyuan Luan },
  journal={ arXiv preprint arXiv:2112.03539v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2112.03539v1 },
  abstract={ Physical activity (PA) is an important risk factor for many health outcomes. Wearable-devices such as accelerometers are increasingly used in biomedical studies to understand the associations between PA and health outcomes. Statistical analyses involving accelerometer data are challenging due to the following three characteristics: (i) high-dimensionality, (ii) temporal dependence, and (iii) measurement error. To address these challenges we treat accelerometer-based measures of physical activity as a single function-valued covariate prone to measurement error. Specifically, in order to determine the relationship between PA and a health outcome of interest, we propose a regression model with a functional covariate that accounts for measurement error. Using regression calibration, we develop a two-step estimation method for the model parameters and establish their consistency. A test is also proposed to test the significance of the estimated model parameters. Simulation studies are conducted to compare the proposed methods with existing alternative approaches under varying scenarios. Finally, the developed methods are used to assess the relationship between PA intensity and BMI obtained from the National Health and Nutrition Examination Survey data. }
}

@article{211111789v1,
  title={ End-to-End Optimized Arrhythmia Detection Pipeline using Machine   Learning for Ultra-Edge Devices },
  author={ Sideshwar J B and Sachin Krishan T and Vishal Nagarajan and Shanthakumar S and Vineeth Vijayaraghavan },
  journal={ arXiv preprint arXiv:2111.11789v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2111.11789v1 },
  abstract={ Atrial fibrillation (AF) is the most prevalent cardiac arrhythmia worldwide, with 2\% of the population affected. It is associated with an increased risk of strokes, heart failure and other heart-related complications. Monitoring at-risk individuals and detecting asymptomatic AF could result in considerable public health benefits, as individuals with asymptomatic AF could take preventive measures with lifestyle changes. With increasing affordability to wearables, personalized health care is becoming more accessible. These personalized healthcare solutions require accurate classification of bio-signals while being computationally inexpensive. By making inferences on-device, we avoid issues inherent to cloud-based systems such as latency and network connection dependency. We propose an efficient pipeline for real-time Atrial Fibrillation Detection with high accuracy that can be deployed in ultra-edge devices. The feature engineering employed in this research catered to optimizing the resource-efficient classifier used in the proposed pipeline, which was able to outperform the best performing standard ML model by \$10\^{}5\\textbackslash{}times\$ in terms of memory footprint with a mere trade-off of 2\% classification accuracy. We also obtain higher accuracy of approximately 6\% while consuming 403\$\\textbackslash{}times\$ lesser memory and being 5.2\$\\textbackslash{}times\$ faster compared to the previous state-of-the-art (SoA) embedded implementation. }
}

@article{211107089v1,
  title={ Evaluating Contrastive Learning on Wearable Timeseries for Downstream   Clinical Outcomes },
  author={ Kevalee Shah and Dimitris Spathis and Chi Ian Tang and Cecilia Mascolo },
  journal={ arXiv preprint arXiv:2111.07089v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2111.07089v1 },
  abstract={ Vast quantities of person-generated health data (wearables) are collected but the process of annotating to feed to machine learning models is impractical. This paper discusses ways in which self-supervised approaches that use contrastive losses, such as SimCLR and BYOL, previously applied to the vision domain, can be applied to high-dimensional health signals for downstream classification tasks of various diseases spanning sleep, heart, and metabolic conditions. To this end, we adapt the data augmentation step and the overall architecture to suit the temporal nature of the data (wearable traces) and evaluate on 5 downstream tasks by comparing other state-of-the-art methods including supervised learning and an adversarial unsupervised representation learning method. We show that SimCLR outperforms the adversarial method and a fully-supervised method in the majority of the downstream evaluation tasks, and that all self-supervised methods outperform the fully-supervised methods. This work provides a comprehensive benchmark for contrastive methods applied to the wearable time-series domain, showing the promise of task-agnostic representations for downstream clinical outcomes. }
}

@article{211106175v1,
  title={ Training neural networks with synthetic electrocardiograms },
  author={ Matti Kaisti and Juho Laitala and Antti Airola },
  journal={ arXiv preprint arXiv:2111.06175v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2111.06175v1 },
  abstract={ We present a method for training neural networks with synthetic electrocardiograms that mimic signals produced by a wearable single lead electrocardiogram monitor. We use domain randomization where the synthetic signal properties such as the waveform shape, RR-intervals and noise are varied for every training example. Models trained with synthetic data are compared to their counterparts trained with real data. Detection of r-waves in electrocardiograms recorded during different physical activities and in atrial fibrillation is used to compare the models. By allowing the randomization to increase beyond what is typically observed in the real-world data the performance is on par or superseding the performance of networks trained with real data. Experiments show robust performance with different seeds and training examples on different test sets without any test set specific tuning. The method makes possible to train neural networks using practically free-to-collect data with accurate labels without the need for manual annotations and it opens up the possibility of extending the use of synthetic data on cardiac disease classification when disease specific a priori information is used in the electrocardiogram generation. Additionally the distribution of data can be controlled eliminating class imbalances that are typically observed in health related data and additionally the generated data is inherently private. }
}

@article{211103731v1,
  title={ Frugal Machine Learning },
  author={ Mikhail Evchenko and Joaquin Vanschoren and Holger H. Hoos and Marc Schoenauer and Michèle Sebag },
  journal={ arXiv preprint arXiv:2111.03731v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2111.03731v1 },
  abstract={ Machine learning, already at the core of increasingly many systems and applications, is set to become even more ubiquitous with the rapid rise of wearable devices and the Internet of Things. In most machine learning applications, the main focus is on the quality of the results achieved (e.g., prediction accuracy), and hence vast amounts of data are being collected, requiring significant computational resources to build models. In many scenarios, however, it is infeasible or impractical to set up large centralized data repositories. In personal health, for instance, privacy issues may inhibit the sharing of detailed personal data. In such cases, machine learning should ideally be performed on wearable devices themselves, which raises major computational limitations such as the battery capacity of smartwatches. This paper thus investigates frugal learning, aimed to build the most accurate possible models using the least amount of resources. A wide range of learning algorithms is examined through a frugal lens, analyzing their accuracy/runtime performance on a wide range of data sets. The most promising algorithms are thereafter assessed in a real-world scenario by implementing them in a smartwatch and letting them learn activity recognition models on the watch itself. }
}

@article{211014848v1,
  title={ V2iFi: in-Vehicle Vital Sign Monitoring via Compact RF Sensing },
  author={ Tianyue Zheng and Zhe Chen and Chao Cai and Jun Luo and Xu Zhang },
  journal={ arXiv preprint arXiv:2110.14848v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2110.14848v1 },
  abstract={ Given the significant amount of time people spend in vehicles, health issues under driving condition have become a major concern. Such issues may vary from fatigue, asthma, stroke, to even heart attack, yet they can be adequately indicated by vital signs and abnormal activities. Therefore, in-vehicle vital sign monitoring can help us predict and hence prevent these issues. Whereas existing sensor-based (including camera) methods could be used to detect these indicators, privacy concern and system complexity both call for a convenient yet effective and robust alternative. This paper aims to develop V2iFi, an intelligent system performing monitoring tasks using a COTS impulse radio mounted on the windshield. V2iFi is capable of reliably detecting driver's vital signs under driving condition and with the presence of passengers, thus allowing for potentially inferring corresponding health issues. Compared with prior work based on Wi-Fi CSI, V2iFi is able to distinguish reflected signals from multiple users, and hence provide finer-grained measurements under more realistic settings. We evaluate V2iFi both in lab environments and during real-life road tests; the results demonstrate that respiratory rate, heart rate, and heart rate variability can all be estimated accurately. Based on these estimation results, we further discuss how machine learning models can be applied on top of V2iFi so as to improve both physiological and psychological wellbeing in driving environments. }
}

@article{210210783v2,
  title={ Distributional data analysis via quantile functions and its application   to modelling digital biomarkers of gait in Alzheimer's Disease },
  author={ Rahul Ghosal and Vijay R. Varma and Dmitri Volfson and Inbar Hillel and Jacek Urbanek and Jeffrey M. Hausdorff and Amber Watts and Vadim Zipunnikov },
  journal={ arXiv preprint arXiv:2102.10783v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2102.10783v2 },
  abstract={ With the advent of continuous health monitoring with wearable devices, users now generate their unique streams of continuous data such as minute-level step counts or heartbeats. Summarizing these streams via scalar summaries often ignores the distributional nature of wearable data and almost unavoidably leads to the loss of critical information. We propose to capture the distributional nature of wearable data via user-specific quantile functions (QF) and use these QFs as predictors in scalar-on-quantile-function-regression (SOQFR). As an alternative approach, we also propose to represent QFs via user-specific L-moments, robust rank-based analogs of traditional moments, and use L-moments as predictors in SOQFR (SOQFR-L). These two approaches provide two mutually consistent interpretations: in terms of quantile levels by SOQFR and in terms of L-moments by SOQFR-L. We also demonstrate how to deal with multi-modal distributional data via Joint and Individual Variation Explained (JIVE) using L-moments. The proposed methods are illustrated in a study of association of digital gait biomarkers with cognitive function in Alzheimer's disease (AD). Our analysis shows that the proposed methods demonstrate higher predictive performance and attain much stronger associations with clinical cognitive scales compared to simple distributional summaries. }
}

@article{210913705v2,
  title={ Opportunistic Multi-Modal User Authentication for Health-Tracking IoT   Wearables },
  author={ Alexa Muratyan and William Cheung and Sayanton V. Dibbo and Sudip Vhaduri },
  journal={ arXiv preprint arXiv:2109.13705v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2109.13705v2 },
  abstract={ With the advancement of technologies, market wearables are becoming increasingly popular with a range of services, including providing access to bank accounts, accessing cars, monitoring patients remotely, among several others. However, often these wearables collect various sensitive personal information of a user with no to limited authentication, e.g., knowledge-based external authentication techniques, such as PINs. While most of these external authentication techniques suffer from multiple limitations, including recall burden, human errors, or biases, researchers have started using various physiological and behavioral data, such as gait and heart rate, collected by the wearables to authenticate a wearable user implicitly with a limited accuracy due to sensing and computing constraints of wearables. In this work, we explore the usefulness of blood oxygen saturation SpO2 values collected from the Oximeter device to distinguish a user from others. From a cohort of 25 subjects, we find that 92\% of the cases SpO2 can distinguish pairs of users. From detailed modeling and performance analysis, we observe that while SpO2 alone can obtain an average accuracy of 0.69 and F1 score of 0.69, the addition of heart rate (HR) can improve the average identification accuracy by 15\% and F1 score by 13\%. These results show promise in using SpO2 along with other biometrics to develop implicit continuous authentications for wearables. }
}

@article{210914743v2,
  title={ Posttraumatic Stress Disorder Hyperarousal Event Detection Using   Smartwatch Physiological and Activity Data },
  author={ Mahnoosh Sadeghi and Anthony D McDonald and Farzan Sasangohar },
  journal={ arXiv preprint arXiv:2109.14743v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2109.14743v2 },
  abstract={ Posttraumatic Stress Disorder (PTSD) is a psychiatric condition affecting nearly a quarter of the United States war veterans who return from war zones. Treatment for PTSD typically consists of a combination of in-session therapy and medication. However; patients often experience their most severe PTSD symptoms outside of therapy sessions. Mobile health applications may address this gap, but their effectiveness is limited by the current gap in continuous monitoring and detection capabilities enabling timely intervention. The goal of this article is to develop a novel method to detect hyperarousal events using physiological and activity-based machine learning algorithms. Physiological data including heart rate and body acceleration as well as self-reported hyperarousal events were collected using a tool developed for commercial off-the-shelf wearable devices from 99 United States veterans diagnosed with PTSD over several days. The data were used to develop four machine learning algorithms: Random Forest, Support Vector Machine, Logistic Regression and XGBoost. The XGBoost model had the best performance in detecting onset of PTSD symptoms with over 83\% accuracy and an AUC of 0.70. Post-hoc SHapley Additive exPlanations (SHAP) additive explanation analysis showed that algorithm predictions were correlated with average heart rate, minimum heart rate and average body acceleration. Findings show promise in detecting onset of PTSD symptoms which could be the basis for developing remote and continuous monitoring systems for PTSD. Such systems may address a vital gap in just-in-time interventions for PTSD self-management outside of scheduled clinical appointments. }
}

@article{210915036v1,
  title={ Automated Workers Ergonomic Risk Assessment in Manual Material Handling   using sEMG Wearable Sensors and Machine Learning },
  author={ Srimantha E. Mudiyanselage and Phuong H. D. Nguyen and Mohammad Sadra Rajabi and Reza Akhavian },
  journal={ arXiv preprint arXiv:2109.15036v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2109.15036v1 },
  abstract={ Manual material handling tasks have the potential to be highly unsafe from an ergonomic viewpoint. Safety inspections to monitor body postures can help mitigate ergonomic risks of material handling. However, the real effect of awkward muscle movements, strains, and excessive forces that may result in an injury may not be identified by external cues. This paper evaluates the ability of surface electromyogram (EMG)-based systems together with machine learning algorithms to automatically detect body movements that may harm muscles in material handling. The analysis utilized a lifting equation developed by the U.S. National Institute for Occupational Safety and Health (NIOSH). This equation determines a Recommended Weight Limit, which suggests the maximum acceptable weight that a healthy worker can lift and carry as well as a Lifting Index value to assess the risk extent. Four different machine learning models, namely Decision Tree, Support Vector Machine, K-Nearest Neighbor, and Random Forest are developed to classify the risk assessments calculated based on the NIOSH lifting equation. The sensitivity of the models to various parameters is also evaluated to find the best performance using each algorithm. Results indicate that Decision Tree models have the potential to predict the risk level with close to 99.35\% accuracy. }
}

@article{210804022v1,
  title={ Towards Automated Fatigue Assessment using Wearable Sensing and   Mixed-Effects Models },
  author={ Yang Bai and Yu Guan and Jian Qing Shi and Wan-Fai Ng },
  journal={ arXiv preprint arXiv:2108.04022v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2108.04022v1 },
  abstract={ Fatigue is a broad, multifactorial concept that includes the subjective perception of reduced physical and mental energy levels. It is also one of the key factors that strongly affect patients' health-related quality of life. To date, most fatigue assessment methods were based on self-reporting, which may suffer from many factors such as recall bias. To address this issue, in this work, we recorded multi-modal physiological data (including ECG, accelerometer, skin temperature and respiratory rate, as well as demographic information such as age, BMI) in free-living environments and developed automated fatigue assessment models. Specifically, we extracted features from each modality and employed the random forest-based mixed-effects models, which can take advantage of the demographic information for improved performance. We conducted experiments on our collected dataset, and very promising preliminary results were achieved. Our results suggested ECG played an important role in the fatigue assessment tasks. }
}

@article{210800144v1,
  title={ Personalized Stress Monitoring using Wearable Sensors in Everyday   Settings },
  author={ Ali Tazarv and Sina Labbaf and Stephanie M. Reich and Nikil Dutt and Amir M. Rahmani and Marco Levorato },
  journal={ arXiv preprint arXiv:2108.00144v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2108.00144v1 },
  abstract={ Since stress contributes to a broad range of mental and physical health problems, the objective assessment of stress is essential for behavioral and physiological studies. Although several studies have evaluated stress levels in controlled settings, objective stress assessment in everyday settings is still largely under-explored due to challenges arising from confounding contextual factors and limited adherence for self-reports. In this paper, we explore the objective prediction of stress levels in everyday settings based on heart rate (HR) and heart rate variability (HRV) captured via low-cost and easy-to-wear photoplethysmography (PPG) sensors that are widely available on newer smart wearable devices. We present a layered system architecture for personalized stress monitoring that supports a tunable collection of data samples for labeling, and present a method for selecting informative samples from the stream of real-time data for labeling. We captured the stress levels of fourteen volunteers through self-reported questionnaires over periods of between 1-3 months, and explored binary stress detection based on HR and HRV using Machine Learning Methods. We observe promising preliminary results given that the dataset is collected in the challenging environments of everyday settings. The binary stress detector is fairly accurate and can detect stressful vs non-stressful samples with a macro-F1 score of up to \\textbackslash{}\%76. Our study lays the groundwork for more sophisticated labeling strategies that generate context-aware, personalized models that will empower health professionals to provide personalized interventions. }
}

@article{210714028v1,
  title={ Estimating Respiratory Rate From Breath Audio Obtained Through Wearable   Microphones },
  author={ Agni Kumar and Vikramjit Mitra and Carolyn Oliver and Adeeti Ullal and Matt Biddulph and Irida Mance },
  journal={ arXiv preprint arXiv:2107.14028v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2107.14028v1 },
  abstract={ Respiratory rate (RR) is a clinical metric used to assess overall health and physical fitness. An individual's RR can change from their baseline due to chronic illness symptoms (e.g., asthma, congestive heart failure), acute illness (e.g., breathlessness due to infection), and over the course of the day due to physical exhaustion during heightened exertion. Remote estimation of RR can offer a cost-effective method to track disease progression and cardio-respiratory fitness over time. This work investigates a model-driven approach to estimate RR from short audio segments obtained after physical exertion in healthy adults. Data was collected from 21 individuals using microphone-enabled, near-field headphones before, during, and after strenuous exercise. RR was manually annotated by counting perceived inhalations and exhalations. A multi-task Long-Short Term Memory (LSTM) network with convolutional layers was implemented to process mel-filterbank energies, estimate RR in varying background noise conditions, and predict heavy breathing, indicated by an RR of more than 25 breaths per minute. The multi-task model performs both classification and regression tasks and leverages a mixture of loss functions. It was observed that RR can be estimated with a concordance correlation coefficient (CCC) of 0.76 and a mean squared error (MSE) of 0.2, demonstrating that audio can be a viable signal for approximating RR. }
}

@article{210705666v1,
  title={ Stress Classification and Personalization: Getting the most out of the   least },
  author={ Ramesh Kumar Sah and Hassan Ghasemzadeh },
  journal={ arXiv preprint arXiv:2107.05666v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2107.05666v1 },
  abstract={ Stress detection and monitoring is an active area of research with important implications for the personal, professional, and social health of an individual. Current approaches for affective state classification use traditional machine learning algorithms with features computed from multiple sensor modalities. These methods are data-intensive and rely on hand-crafted features which impede the practical applicability of these sensor systems in daily lives. To overcome these shortcomings, we propose a novel Convolutional Neural Network (CNN) based stress detection and classification framework without any feature computation using data from only one sensor modality. Our method is competitive and outperforms current state-of-the-art techniques and achieves a classification accuracy of \$92.85\\textbackslash{}\%\$ and an \$f1\$ score of \$0.89\$. Through our leave-one-subject-out analysis, we also show the importance of personalizing stress models. }
}

@article{210710399v1,
  title={ Quantifying machine learning-induced overdiagnosis in sepsis },
  author={ Anna Fedyukova and Douglas Pires and Daniel Capurro },
  journal={ arXiv preprint arXiv:2107.10399v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2107.10399v1 },
  abstract={ The proliferation of early diagnostic technologies, including self-monitoring systems and wearables, coupled with the application of these technologies on large segments of healthy populations may significantly aggravate the problem of overdiagnosis. This can lead to unwanted consequences such as overloading health care systems and overtreatment, with potential harms to healthy individuals. The advent of machine-learning tools to assist diagnosis -- while promising rapid and more personalised patient management and screening -- might contribute to this issue. The identification of overdiagnosis is usually post hoc and demonstrated after long periods (from years to decades) and costly randomised control trials. In this paper, we present an innovative approach that allows us to preemptively detect potential cases of overdiagnosis during predictive model development. This approach is based on the combination of labels obtained from a prediction model and clustered medical trajectories, using sepsis in adults as a test case. This is one of the first attempts to quantify machine-learning induced overdiagnosis and we believe will serves as a platform for further development, leading to guidelines for safe deployment of computational diagnostic tools. }
}

@article{210700693v1,
  title={ Inter-Beat Interval Estimation with Tiramisu Model: A Novel Approach   with Reduced Error },
  author={ Asiful Arefeen and Ali Akbari and Seyed Iman Mirzadeh and Roozbeh Jafari and Behrooz A. Shirazi and Hassan Ghasemzadeh },
  journal={ arXiv preprint arXiv:2107.00693v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2107.00693v1 },
  abstract={ Inter-beat interval (IBI) measurement enables estimation of heart-rate variability (HRV) which, in turns, can provide early indication of potential cardiovascular diseases. However, extracting IBIs from noisy signals is challenging since the morphology of the signal is distorted in the presence of the noise. Electrocardiogram (ECG) of a person in heavy motion is highly corrupted with noise, known as motion-artifact, and IBI extracted from it is inaccurate. As a part of remote health monitoring and wearable system development, denoising ECG signals and estimating IBIs correctly from them have become an emerging topic among signal-processing researchers. Apart from conventional methods, deep-learning techniques have been successfully used in signal denoising recently, and diagnosis process has become easier, leading to accuracy levels that were previously unachievable. We propose a deep-learning approach leveraging tiramisu autoencoder model to suppress motion-artifact noise and make the R-peaks of the ECG signal prominent even in the presence of high-intensity motion. After denoising, IBIs are estimated more accurately expediting diagnosis tasks. Results illustrate that our method enables IBI estimation from noisy ECG signals with SNR up to -30dB with average root mean square error (RMSE) of 13 milliseconds for estimated IBIs. At this noise level, our error percentage remains below 8\% and outperforms other state of the art techniques. }
}

@article{210703924v1,
  title={ Smart Healthcare in the Age of AI: Recent Advances, Challenges, and   Future Prospects },
  author={ Mahmoud Nasr and MD. Milon Islam and Shady Shehata and Fakhri Karray and Yuri Quintana },
  journal={ arXiv preprint arXiv:2107.03924v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2107.03924v1 },
  abstract={ The significant increase in the number of individuals with chronic ailments (including the elderly and disabled) has dictated an urgent need for an innovative model for healthcare systems. The evolved model will be more personalized and less reliant on traditional brick-and-mortar healthcare institutions such as hospitals, nursing homes, and long-term healthcare centers. The smart healthcare system is a topic of recently growing interest and has become increasingly required due to major developments in modern technologies, especially in artificial intelligence (AI) and machine learning (ML). This paper is aimed to discuss the current state-of-the-art smart healthcare systems highlighting major areas like wearable and smartphone devices for health monitoring, machine learning for disease diagnosis, and the assistive frameworks, including social robots developed for the ambient assisted living environment. Additionally, the paper demonstrates software integration architectures that are very significant to create smart healthcare systems, integrating seamlessly the benefit of data analytics and other tools of AI. The explained developed systems focus on several facets: the contribution of each developed framework, the detailed working procedure, the performance as outcomes, and the comparative merits and limitations. The current research challenges with potential future directions are addressed to highlight the drawbacks of existing systems and the possible methods to introduce novel frameworks, respectively. This review aims at providing comprehensive insights into the recent developments of smart healthcare systems to equip experts to contribute to the field. }
}

@article{210612081v1,
  title={ Forecasting Health and Wellbeing for Shift Workers Using Job-role Based   Deep Neural Network },
  author={ Han Yu and Asami Itoh and Ryota Sakamoto and Motomu Shimaoka and Akane Sano },
  journal={ arXiv preprint arXiv:2106.12081v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2106.12081v1 },
  abstract={ Shift workers who are essential contributors to our society, face high risks of poor health and wellbeing. To help with their problems, we collected and analyzed physiological and behavioral wearable sensor data from shift working nurses and doctors, as well as their behavioral questionnaire data and their self-reported daily health and wellbeing labels, including alertness, happiness, energy, health, and stress. We found the similarities and differences between the responses of nurses and doctors. According to the differences in self-reported health and wellbeing labels between nurses and doctors, and the correlations among their labels, we proposed a job-role based multitask and multilabel deep learning model, where we modeled physiological and behavioral data for nurses and doctors simultaneously to predict participants' next day's multidimensional self-reported health and wellbeing status. Our model showed significantly better performances than baseline models and previous state-of-the-art models in the evaluations of binary/3-class classification and regression prediction tasks. We also found features related to heart rate, sleep, and work shift contributed to shift workers' health and wellbeing. }
}

@article{210611900v1,
  title={ Person Re-identification Attack on Wearable Sensing },
  author={ Mohammad Arif Ul Alam },
  journal={ arXiv preprint arXiv:2106.11900v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2106.11900v1 },
  abstract={ Person re-identification is a critical privacy attack in publicly shared healthcare data as per Health Insurance Portability and Accountability Act (HIPAA) privacy rule. In this paper, we investigate the possibility of a new type of privacy attack, Person Re-identification Attack (PRI-attack) on publicly shared privacy insensitive wearable data. We investigate user's specific biometric signature in terms of two contextual biometric traits, physiological (photoplethysmography and electrodermal activity) and physical (accelerometer) contexts. In this regard, we develop a Multi-Modal Siamese Convolutional Neural Network (mmSNN) model. The framework learns the spatial and temporal information individually and combines them together in a modified weighted cost with an objective of predicting a person's identity. We evaluated our proposed model using real-time collected data from 3 collected datasets and one publicly available dataset. Our proposed framework shows that PPG-based breathing rate and heart rate in conjunction with hand gesture contexts can be utilized by attackers to re-identify user's identity (max. 71\%) from HIPAA compliant wearable data. Given publicly placed camera can estimate heart rate and breathing rate along with hand gestures remotely, person re-identification using them imposes a significant threat to future HIPAA compliant server which requires a better encryption method to store wearable healthcare data. }
}

@article{210611844v1,
  title={ Detecting Anomalous User Behavior in Remote Patient Monitoring },
  author={ Deepti Gupta and Maanak Gupta and Smriti Bhatt and Ali Saman Tosun },
  journal={ arXiv preprint arXiv:2106.11844v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2106.11844v1 },
  abstract={ The growth in Remote Patient Monitoring (RPM) services using wearable and non-wearable Internet of Medical Things (IoMT) promises to improve the quality of diagnosis and facilitate timely treatment for a gamut of medical conditions. At the same time, the proliferation of IoMT devices increases the potential for malicious activities that can lead to catastrophic results including theft of personal information, data breach, and compromised medical devices, putting human lives at risk. IoMT devices generate tremendous amount of data that reflect user behavior patterns including both personal and day-to-day social activities along with daily routine health monitoring. In this context, there are possibilities of anomalies generated due to various reasons including unexpected user behavior, faulty sensor, or abnormal values from malicious/compromised devices. To address this problem, there is an imminent need to develop a framework for securing the smart health care infrastructure to identify and mitigate anomalies. In this paper, we present an anomaly detection model for RPM utilizing IoMT and smart home devices. We propose Hidden Markov Model (HMM) based anomaly detection that analyzes normal user behavior in the context of RPM comprising both smart home and smart health devices, and identifies anomalous user behavior. We design a testbed with multiple IoMT devices and home sensors to collect data and use the HMM model to train using network and user behavioral data. Proposed HMM based anomaly detection model achieved over 98\% accuracy in identifying the anomalies in the context of RPM. }
}

@article{210709509v1,
  title={ Wearable Health Monitoring System for Older Adults in a Smart Home   Environment },
  author={ Rajdeep Kumar Nath and Himanshu Thapliyal },
  journal={ arXiv preprint arXiv:2107.09509v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2107.09509v1 },
  abstract={ The advent of IoT has enabled the design of connected and integrated smart health monitoring systems. These smart health monitoring systems could be realized in a smart home context to render long-term care to the elderly population. In this paper, we present the design of a wearable health monitoring system suitable for older adults in a smart home context. The proposed system offers solutions to monitor the stress, blood pressure, and location of an individual within a smart home environment. The stress detection model proposed in this work uses Electrodermal Activity (EDA), Photoplethysmogram (PPG), and Skin Temperature (ST) sensors embedded in a smart wristband for detecting physiological stress. The stress detection model is trained and tested using stress labels obtained from salivary cortisol which is a clinically established biomarker for physiological stress. A voice-based prototype is also implemented and the feasibility of the proposed system for integration in a smart home environment is analyzed by simulating a data acquisition and streaming scenario. We have also proposed a blood pressure estimation model using PPG signal and advanced regression techniques for integration with the stress detection model in the wearable health monitoring system. Finally, the design of a voice-assisted indoor location system is proposed for integration with the proposed system within a smart home environment. The proposed wearable health monitoring system is an important direction to realize a smart home environment with extensive diagnostic capabilities so that such a system could be useful for rendering long-term and personalized care to the aging population in the comfort of their home. }
}

@article{210603979v1,
  title={ Scalar on time-by-distribution regression and its application for   modelling associations between daily-living physical activity and cognitive   functions in Alzheimer's Disease },
  author={ Rahul Ghosal and Vijay R. Varma and Dmitri Volfson and Jacek Urbanek and Jeffrey M. Hausdorff and Amber Watts and Vadim Zipunnikov },
  journal={ arXiv preprint arXiv:2106.03979v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2106.03979v1 },
  abstract={ Wearable data is a rich source of information that can provide deeper understanding of links between human behaviours and human health. Existing modelling approaches use wearable data summarized at subject level via scalar summaries using regression techniques, temporal (time-of-day) curves using functional data analysis (FDA), and distributions using distributional data analysis (DDA). We propose to capture temporally local distributional information in wearable data using subject-specific time-by-distribution (TD) data objects. Specifically, we propose scalar on time-by-distribution regression (SOTDR) to model associations between scalar response of interest such as health outcomes or disease status and TD predictors. We show that TD data objects can be parsimoniously represented via a collection of time-varying L-moments that capture distributional changes over the time-of-day. The proposed method is applied to the accelerometry study of mild Alzheimer's disease (AD). Mild AD is found to be significantly associated with reduced maximal level of physical activity, particularly during morning hours. It is also demonstrated that TD predictors attain much stronger associations with clinical cognitive scales of attention, verbal memory, and executive function when compared to predictors summarized via scalar total activity counts, temporal functional curves, and quantile functions. Taken together, the present results suggest that the SOTDR analysis provides novel insights into cognitive function and AD. }
}

@article{210502808v1,
  title={ Wearable and Continuous Prediction of Passage of Time Perception for   Monitoring Mental Health },
  author={ Lara Orlandic and Adriana Arza Valdes and David Atienza },
  journal={ arXiv preprint arXiv:2105.02808v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2105.02808v1 },
  abstract={ A person's passage of time perception (POTP) is strongly linked to their mental state and stress response, and can therefore provide an easily quantifiable means of continuous mental health monitoring. In this work, we develop a custom experiment and Machine Learning (ML) models for predicting POTP from biomarkers acquired from wearable biosensors. We first confirm that individuals experience time passing slower than usual during fear or sadness (p = 0.046) and faster than usual during cognitive tasks (p = 2 x 10\^{}-5). Then, we group together the experimental segments associated with fast, slow, and normal POTP, and train a ML model to classify between these states based on a person's biomarkers. The classifier had a weighted average F-1 score of 79\%, with the fast-passing time class having the highest F-1 score of 93\%. Next, we classify each individual's POTP regardless of the task at hand, achieving an F-1 score of 77.1\% when distinguishing time passing faster rather than slower than usual. In the two classifiers, biomarkers derived from the respiration, electrocardiogram, skin conductance, and skin temperature signals contributed most to the classifier output, thus enabling real-time POTP monitoring using noninvasive, wearable biosensors. }
}

@article{210500528v1,
  title={ A 1D-CNN Based Deep Learning Technique for Sleep Apnea Detection in IoT   Sensors },
  author={ Arlene John and Barry Cardiff and Deepu John },
  journal={ arXiv preprint arXiv:2105.00528v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2105.00528v1 },
  abstract={ Internet of Things (IoT) enabled wearable sensors for health monitoring are widely used to reduce the cost of personal healthcare and improve quality of life. The sleep apnea-hypopnea syndrome, characterized by the abnormal reduction or pause in breathing, greatly affects the quality of sleep of an individual. This paper introduces a novel method for apnea detection (pause in breathing) from electrocardiogram (ECG) signals obtained from wearable devices. The novelty stems from the high resolution of apnea detection on a second-by-second basis, and this is achieved using a 1-dimensional convolutional neural network for feature extraction and detection of sleep apnea events. The proposed method exhibits an accuracy of 99.56\% and a sensitivity of 96.05\%. This model outperforms several lower resolution state-of-the-art apnea detection methods. The complexity of the proposed model is analyzed. We also analyze the feasibility of model pruning and binarization to reduce the resource requirements on a wearable IoT device. The pruned model with 80\\textbackslash{}\% sparsity exhibited an accuracy of 97.34\% and a sensitivity of 86.48\%. The binarized model exhibited an accuracy of 75.59\% and sensitivity of 63.23\%. The performance of low complexity patient-specific models derived from the generic model is also studied to analyze the feasibility of retraining existing models to fit patient-specific requirements. The patient-specific models on average exhibited an accuracy of 97.79\% and sensitivity of 92.23\%. The source code for this work is made publicly available. }
}

@article{200412168v2,
  title={ Interspecies information systems },
  author={ Dirk van der Linden },
  journal={ arXiv preprint arXiv:2004.12168v2 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2004.12168v2 },
  abstract={ This article introduces a new class of socio-technical systems, interspecies information systems (IIS) by describing several examples of these systems emerging through the use of commercially available data-driven animal-centered technology. When animal-centered technology, such as pet wearables, cow health monitoring, or even wildlife drones captures animal data and inform humans of actions to take towards animals, interspecies information systems emerge. I discuss the importance of understanding them as information systems rather than isolated technology or technology-mediated interactions, and propose a conceptual model capturing the key components and information flow of a general interspecies information system. I conclude by proposing multiple practical challenges that are faced in the successful design, engineering and use of any interspecies information systems where animal data informs human actions. }
}

@article{210316490v1,
  title={ Human Activity Analysis and Recognition from Smartphones using Machine   Learning Techniques },
  author={ Jakaria Rabbi and Md. Tahmid Hasan Fuad and Md. Abdul Awal },
  journal={ arXiv preprint arXiv:2103.16490v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2103.16490v1 },
  abstract={ Human Activity Recognition (HAR) is considered a valuable research topic in the last few decades. Different types of machine learning models are used for this purpose, and this is a part of analyzing human behavior through machines. It is not a trivial task to analyze the data from wearable sensors for complex and high dimensions. Nowadays, researchers mostly use smartphones or smart home sensors to capture these data. In our paper, we analyze these data using machine learning models to recognize human activities, which are now widely used for many purposes such as physical and mental health monitoring. We apply different machine learning models and compare performances. We use Logistic Regression (LR) as the benchmark model for its simplicity and excellent performance on a dataset, and to compare, we take Decision Tree (DT), Support Vector Machine (SVM), Random Forest (RF), and Artificial Neural Network (ANN). Additionally, we select the best set of parameters for each model by grid search. We use the HAR dataset from the UCI Machine Learning Repository as a standard dataset to train and test the models. Throughout the analysis, we can see that the Support Vector Machine performed (average accuracy 96.33\%) far better than the other methods. We also prove that the results are statistically significant by employing statistical significance test methods. }
}

@article{210312456v1,
  title={ Health Status Prediction with Local-Global Heterogeneous Behavior Graph },
  author={ Xuan Ma and Xiaoshan Yang and Junyu Gao and Changsheng Xu },
  journal={ arXiv preprint arXiv:2103.12456v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2103.12456v1 },
  abstract={ Health management is getting increasing attention all over the world. However, existing health management mainly relies on hospital examination and treatment, which are complicated and untimely. The emerging of mobile devices provides the possibility to manage people's health status in a convenient and instant way. Estimation of health status can be achieved with various kinds of data streams continuously collected from wearable sensors. However, these data streams are multi-source and heterogeneous, containing complex temporal structures with local contextual and global temporal aspects, which makes the feature learning and data joint utilization challenging. We propose to model the behavior-related multi-source data streams with a local-global graph, which contains multiple local context sub-graphs to learn short term local context information with heterogeneous graph neural networks and a global temporal sub-graph to learn long term dependency with self-attention networks. Then health status is predicted based on the structure-aware representation learned from the local-global behavior graph. We take experiments on StudentLife dataset, and extensive results demonstrate the effectiveness of our proposed model. }
}

@article{210102873v1,
  title={ FENet: A Frequency Extraction Network for Obstructive Sleep Apnea   Detection },
  author={ Guanhua Ye and Hongzhi Yin and Tong Chen and Hongxu Chen and Lizhen Cui and Xiangliang Zhang },
  journal={ arXiv preprint arXiv:2101.02873v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2101.02873v1 },
  abstract={ Obstructive Sleep Apnea (OSA) is a highly prevalent but inconspicuous disease that seriously jeopardizes the health of human beings. Polysomnography (PSG), the gold standard of detecting OSA, requires multiple specialized sensors for signal collection, hence patients have to physically visit hospitals and bear the costly treatment for a single detection. Recently, many single-sensor alternatives have been proposed to improve the cost efficiency and convenience. Among these methods, solutions based on RR-interval (i.e., the interval between two consecutive pulses) signals reach a satisfactory balance among comfort, portability and detection accuracy. In this paper, we advance RR-interval based OSA detection by considering its real-world practicality from energy perspectives. As photoplethysmogram (PPG) pulse sensors are commonly equipped on smart wrist-worn wearable devices (e.g., smart watches and wristbands), the energy efficiency of the detection model is crucial to fully support an overnight observation on patients. This creates challenges as the PPG sensors are unable to keep collecting continuous signals due to the limited battery capacity on smart wrist-worn devices. Therefore, we propose a novel Frequency Extraction Network (FENet), which can extract features from different frequency bands of the input RR-interval signals and generate continuous detection results with downsampled, discontinuous RR-interval signals. With the help of the one-to-multiple structure, FENet requires only one-third of the operation time of the PPG sensor, thus sharply cutting down the energy consumption and enabling overnight diagnosis. Experimental results on real OSA datasets reveal the state-of-the-art performance of FENet. }
}

@article{201209131v1,
  title={ Personal Mental Health Navigator: Harnessing the Power of Data, Personal   Models, and Health Cybernetics to Promote Psychological Well-being },
  author={ Amir M. Rahmani and Jocelyn Lai and Salar Jafarlou and Asal Yunusova and Alex. P. Rivera and Sina Labbaf and Sirui Hu and Arman Anzanpour and Nikil Dutt and Ramesh Jain and Jessica L. Borelli },
  journal={ arXiv preprint arXiv:2012.09131v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2012.09131v1 },
  abstract={ Traditionally, the regime of mental healthcare has followed an episodic psychotherapy model wherein patients seek care from a provider through a prescribed treatment plan developed over multiple provider visits. Recent advances in wearable and mobile technology have generated increased interest in digital mental healthcare that enables individuals to address episodic mental health symptoms. However, these efforts are typically reactive and symptom-focused and do not provide comprehensive, wrap-around, customized treatments that capture an individual's holistic mental health model as it unfolds over time. Recognizing that each individual is unique, we present the notion of Personalized Mental Health Navigation (MHN): a therapist-in-the-loop, cybernetic goal-based system that deploys a continuous cyclic loop of measurement, estimation, guidance, to steer the individual's mental health state towards a healthy zone. We outline the major components of MHN that is premised on the development of an individual's personal mental health state, holistically represented by a high-dimensional cover of multiple knowledge layers such as emotion, biological patterns, sociology, behavior, and cognition. We demonstrate the feasibility of the personalized MHN approach via a 12-month pilot case study for holistic stress management in college students and highlight an instance of a therapist-in-the-loop intervention using MHN for monitoring, estimating, and proactively addressing moderately severe depression over a sustained period of time. We believe MHN paves the way to transform mental healthcare from the current passive, episodic, reactive process (where individuals seek help to address symptoms that have already manifested) to a continuous and navigational paradigm that leverages a personalized model of the individual, promising to deliver timely interventions to individuals in a holistic manner. }
}

@article{201208975v1,
  title={ Personalized Step Counting Using Wearable Sensors: A Domain Adapted LSTM   Network Approach },
  author={ Arvind Pillai and Halsey Lea and Faisal Khan and Glynn Dennis },
  journal={ arXiv preprint arXiv:2012.08975v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2012.08975v1 },
  abstract={ Activity monitors are widely used to measure various physical activities (PA) as an indicator of mobility, fitness and general health. Similarly, real-time monitoring of longitudinal trends in step count has significant clinical potential as a personalized measure of disease related changes in daily activity. However, inconsistent step count accuracy across vendors, body locations, and individual gait differences limits clinical utility. The tri-axial accelerometer inside PA monitors can be exploited to improve step count accuracy across devices and individuals. In this study, we hypothesize: (1) raw tri-axial sensor data can be modeled to create reliable and accurate step count, and (2) a generalized step count model can then be efficiently adapted to each unique gait pattern using very little new data. Firstly, open-source raw sensor data was used to construct a long short term memory (LSTM) deep neural network to model step count. Then we generated a new, fully independent data set using a different device and different subjects. Finally, a small amount of subject-specific data was domain adapted to produce personalized models with high individualized step count accuracy. These results suggest models trained using large freely available datasets can be adapted to patient populations where large historical data sets are rare. }
}

@article{201202702v1,
  title={ Bayesian Active Learning for Wearable Stress and Affect Detection },
  author={ Abhijith Ragav and Gautham Krishna Gudur },
  journal={ arXiv preprint arXiv:2012.02702v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2012.02702v1 },
  abstract={ In the recent past, psychological stress has been increasingly observed in humans, and early detection is crucial to prevent health risks. Stress detection using on-device deep learning algorithms has been on the rise owing to advancements in pervasive computing. However, an important challenge that needs to be addressed is handling unlabeled data in real-time via suitable ground truthing techniques (like Active Learning), which should help establish affective states (labels) while also selecting only the most informative data points to query from an oracle. In this paper, we propose a framework with capabilities to represent model uncertainties through approximations in Bayesian Neural Networks using Monte-Carlo (MC) Dropout. This is combined with suitable acquisition functions for active learning. Empirical results on a popular stress and affect detection dataset experimented on a Raspberry Pi 2 indicate that our proposed framework achieves a considerable efficiency boost during inference, with a substantially low number of acquired pool points during active learning across various acquisition functions. Variation Ratios achieves an accuracy of 90.38\% which is comparable to the maximum test accuracy achieved while training on about 40\% lesser data. }
}

@article{201200110v1,
  title={ Representing and Denoising Wearable ECG Recordings },
  author={ Jeffrey Chan and Andrew C. Miller and Emily B. Fox },
  journal={ arXiv preprint arXiv:2012.00110v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2012.00110v1 },
  abstract={ Modern wearable devices are embedded with a range of noninvasive biomarker sensors that hold promise for improving detection and treatment of disease. One such sensor is the single-lead electrocardiogram (ECG) which measures electrical signals in the heart. The benefits of the sheer volume of ECG measurements with rich longitudinal structure made possible by wearables come at the price of potentially noisier measurements compared to clinical ECGs, e.g., due to movement. In this work, we develop a statistical model to simulate a structured noise process in ECGs derived from a wearable sensor, design a beat-to-beat representation that is conducive for analyzing variation, and devise a factor analysis-based method to denoise the ECG. We study synthetic data generated using a realistic ECG simulator and a structured noise model. At varying levels of signal-to-noise, we quantitatively measure an upper bound on performance and compare estimates from linear and non-linear models. Finally, we apply our method to a set of ECGs collected by wearables in a mobile health study. }
}

@article{201112121v1,
  title={ Self-supervised transfer learning of physiological representations from   free-living wearable data },
  author={ Dimitris Spathis and Ignacio Perez-Pozuelo and Soren Brage and Nicholas J. Wareham and Cecilia Mascolo },
  journal={ arXiv preprint arXiv:2011.12121v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2011.12121v1 },
  abstract={ Wearable devices such as smartwatches are becoming increasingly popular tools for objectively monitoring physical activity in free-living conditions. To date, research has primarily focused on the purely supervised task of human activity recognition, demonstrating limited success in inferring high-level health outcomes from low-level signals. Here, we present a novel self-supervised representation learning method using activity and heart rate (HR) signals without semantic labels. With a deep neural network, we set HR responses as the supervisory signal for the activity data, leveraging their underlying physiological relationship. In addition, we propose a custom quantile loss function that accounts for the long-tailed HR distribution present in the general population.   We evaluate our model in the largest free-living combined-sensing dataset (comprising >280k hours of wrist accelerometer \& wearable ECG data). Our contributions are two-fold: i) the pre-training task creates a model that can accurately forecast HR based only on cheap activity sensors, and ii) we leverage the information captured through this task by proposing a simple method to aggregate the learnt latent representations (embeddings) from the window-level to user-level. Notably, we show that the embeddings can generalize in various downstream tasks through transfer learning with linear classifiers, capturing physiologically meaningful, personalized information. For instance, they can be used to predict variables associated with individuals' health, fitness and demographic characteristics, outperforming unsupervised autoencoders and common bio-markers. Overall, we propose the first multimodal self-supervised method for behavioral and physiological data with implications for large-scale health and lifestyle monitoring. }
}

@article{201107161v1,
  title={ Ambient heat and human sleep },
  author={ Kelton Minor and Andreas Bjerre-Nielsen and Sigga Svala Jonasdottir and Sune Lehmann and Nick Obradovich },
  journal={ arXiv preprint arXiv:2011.07161v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2011.07161v1 },
  abstract={ Ambient temperatures are rising globally, with the greatest increases recorded at night. Concurrently, the prevalence of insufficient sleep is increasing in many populations, with substantial costs to human health and well-being. Even though nearly a third of the human lifespan is spent asleep, it remains unknown whether temperature and weather impact objective measures of sleep in real-world settings, globally. Here we link billions of sleep measurements from wearable devices comprising over 7 million nighttime sleep records across 68 countries to local daily meteorological data from 2015 to 2017. Rising nighttime temperatures shorten within-person sleep duration primarily through delayed onset, increasing the probability of insufficient sleep. The effect of temperature on sleep loss is substantially larger for residents from lower income countries and older adults, and females are affected more than are males. Nighttime temperature increases inflict the greatest sleep loss during summer and fall months, and we do not find evidence of short-term acclimatization. Coupling historical behavioral measurements with output from climate models, we project that climate change will further erode human sleep, producing substantial geographic inequalities. Our findings have significant implications for adaptation planning and illuminate a pathway through which rising temperatures may globally impact public health. }
}

@article{201016052v2,
  title={ HHAR-net: Hierarchical Human Activity Recognition using Neural Networks },
  author={ Mehrdad Fazli and Kamran Kowsari and Erfaneh Gharavi and Laura Barnes and Afsaneh Doryab },
  journal={ arXiv preprint arXiv:2010.16052v2 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2010.16052v2 },
  abstract={ Activity recognition using built-in sensors in smart and wearable devices provides great opportunities to understand and detect human behavior in the wild and gives a more holistic view of individuals' health and well being. Numerous computational methods have been applied to sensor streams to recognize different daily activities. However, most methods are unable to capture different layers of activities concealed in human behavior. Also, the performance of the models starts to decrease with increasing the number of activities. This research aims at building a hierarchical classification with Neural Networks to recognize human activities based on different levels of abstraction. We evaluate our model on the Extrasensory dataset; a dataset collected in the wild and containing data from smartphones and smartwatches. We use a two-level hierarchy with a total of six mutually exclusive labels namely, ''lying down'', ''sitting'', ''standing in place'', ''walking'', ''running'', and ''bicycling'' divided into ''stationary'' and ''non-stationary''. The results show that our model can recognize low-level activities (stationary/non-stationary) with 95.8\% accuracy and overall accuracy of 92.8\% over six labels. This is 3\% above our best performing baseline. }
}

@article{201104601v1,
  title={ Learning Generalizable Physiological Representations from Large-scale   Wearable Data },
  author={ Dimitris Spathis and Ignacio Perez-Pozuelo and Soren Brage and Nicholas J. Wareham and Cecilia Mascolo },
  journal={ arXiv preprint arXiv:2011.04601v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2011.04601v1 },
  abstract={ To date, research on sensor-equipped mobile devices has primarily focused on the purely supervised task of human activity recognition (walking, running, etc), demonstrating limited success in inferring high-level health outcomes from low-level signals, such as acceleration. Here, we present a novel self-supervised representation learning method using activity and heart rate (HR) signals without semantic labels. With a deep neural network, we set HR responses as the supervisory signal for the activity data, leveraging their underlying physiological relationship.   We evaluate our model in the largest free-living combined-sensing dataset (comprising more than 280,000 hours of wrist accelerometer \& wearable ECG data) and show that the resulting embeddings can generalize in various downstream tasks through transfer learning with linear classifiers, capturing physiologically meaningful, personalized information. For instance, they can be used to predict (higher than 70 AUC) variables associated with individuals' health, fitness and demographic characteristics, outperforming unsupervised autoencoders and common bio-markers. Overall, we propose the first multimodal self-supervised method for behavioral and physiological data with implications for large-scale health and lifestyle monitoring. }
}

@article{201008866v1,
  title={ MyWear: A Smart Wear for Continuous Body Vital Monitoring and Emergency   Alert },
  author={ Sibi C. Sethuraman and Pranav Kompally and Saraju P. Mohanty and Uma Choppali },
  journal={ arXiv preprint arXiv:2010.08866v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2010.08866v1 },
  abstract={ Smart healthcare which is built as healthcare Cyber-Physical System (H-CPS) from Internet-of-Medical-Things (IoMT) is becoming more important than before. Medical devices and their connectivity through Internet with alongwith the electronics health record (EHR) and AI analytics making H-CPS possible. IoMT-end devices like wearables and implantables are key for H-CPS based smart healthcare. Smart garment is a specific wearable which can be used for smart healthcare. There are various smart garments that help users to monitor their body vitals in real-time. Many commercially available garments collect the vital data and transmit it to the mobile application for visualization. However, these don't perform real-time analysis for the user to comprehend their health conditions. Also, such garments are not included with an alert system to alert users and contacts in case of emergency. In MyWear, we propose a wearable body vital monitoring garment that captures physiological data and automatically analyses such heart rate, stress level, muscle activity to detect abnormalities. A copy of the physiological data is transmitted to the cloud for detecting any abnormalities in heart beats and predict any potential heart failure in future. We also propose a deep neural network (DNN) model that automatically classifies abnormal heart beat and potential heart failure. For immediate assistance in such a situation, we propose an alert system that sends an alert message to nearby medical officials. The proposed MyWear has an average accuracy of 96.9\% and precision of 97.3\% for detection of the abnormalities. }
}

@article{201007209v1,
  title={ HeartBees: Visualizing Crowd Affects },
  author={ Chao Ying Qin and Marios Constantinides and Luca Maria Aiello and Daniele Quercia },
  journal={ arXiv preprint arXiv:2010.07209v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2010.07209v1 },
  abstract={ Affective sharing within groups strengthens coordination and empathy, leads to better health outcomes, and increases productivity and performance. Existing tools for affective sharing face one main challenge: creating a representation of collective emotional states that is relatable and universally accessible. To overcome this challenge, we propose HeartBees, a bio-feedback system for visualizing collective emotional states, which maps a multi-dimensional emotion model into a metaphorical visualization of flocks of birds. Grounded on Affective Computing literature and physiological sensing, we mapped physiological indicators that could be obtained from wearable devices into a multi-dimensional emotion model, which, in turn, our HeartBees can make use of. We evaluated our nature-inspired interactive system with 353 online participants, whose responses showed good consensus in the way they subjectively perceived the visualizations. Last, we discuss practical applications of HeartBees. }
}

@article{200912983v1,
  title={ The Relationship between Major Depression Symptom Severity and Sleep   Collected Using a Wristband Wearable Device: Multi-centre Longitudinal   Observational Study },
  author={ Yuezhou Zhang and Amos A Folarin and Shaoxiong Sun and Nicholas Cummins and Rebecca Bendayan Yatharth Ranjan and Zulqarnain Rashid and Pauline Conde and Callum Stewart and Petroula Laiou and Faith Matcham and Katie White and Femke Lamers and Sara Siddi and Sara Simblett and Inez Myin-Germeys and Aki Rintala and Til Wykes and Josep Maria Haro and Brenda WJH Pennix and Vaibhav A Narayan and Matthew Hotopf and Richard JB Dobson },
  journal={ arXiv preprint arXiv:2009.12983v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2009.12983v1 },
  abstract={ Research in mental health has implicated sleep pathologies with depression. However, the gold standard for sleep assessment, polysomnography, is not suitable for long-term, continuous, monitoring of daily sleep, and methods such as sleep diaries rely on subjective recall, which is qualitative and inaccurate. Wearable devices, on the other hand, provide a low-cost and convenient means to monitor sleep in home settings. The main aim of this study was to devise and extract sleep features, from data collected using a wearable device, and analyse their correlation with depressive symptom severity and sleep quality, as measured by the self-assessed Patient Health Questionnaire 8-item. Daily sleep data were collected passively by Fitbit wristband devices, and depressive symptom severity was self-reported every two weeks by the PHQ-8. The data used in this paper included 2,812 PHQ-8 records from 368 participants recruited from three study sites in the Netherlands, Spain, and the UK.We extracted 21 sleep features from Fitbit data which describe sleep in the following five aspects: sleep architecture, sleep stability, sleep quality, insomnia, and hypersomnia. Linear mixed regression models were used to explore associations between sleep features and depressive symptom severity. The z-test was used to evaluate the significance of the coefficient of each feature. We tested our models on the entire dataset and individually on the data of three different study sites. We identified 16 sleep features that were significantly correlated with the PHQ-8 score on the entire dataset. Associations between sleep features and the PHQ-8 score varied across different sites, possibly due to the difference in the populations. }
}

@article{200913626v1,
  title={ Monitoring My Dehydration: A Non-Invasive Dehydration Alert System Using   Electrodermal Activity },
  author={ Nandan Kulkarni and Christopher Compton and Jooseppi Luna and Mohammad Arif Ul Alam },
  journal={ arXiv preprint arXiv:2009.13626v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2009.13626v1 },
  abstract={ Staying hydrated and drinking fluids is extremely crucial to stay healthy and maintaining even basic bodily functions. Studies have shown that dehydration leads to loss of productivity, cognitive impairment and mood in both men and women. However, there are no such an existing tool that can monitor dehydration continuously and provide alert to users before it affects on their health. In this paper, we propose to utilize wearable Electrodermal Activity (EDA) sensors in conjunction with signal processing and machine learning techniques to develop first time ever a dehydration self-monitoring tool, \\textbackslash{}emph\{Monitoring My Dehydration\} (MMD), that can instantly detect the hydration level of human skin. Moreover, we develop an Android application over Bluetooth to connect with wearable EDA sensor integrated wristband to track hydration levels of the users real-time and instantly alert to the users when the hydration level goes beyond the danger level. To validate our developed tool's performance, we recruit 5 users, carefully designed the water intake routines to annotate the dehydration ground truth and trained state-of-art machine learning models to predict instant hydration level i.e., well-hydrated, hydrated, dehydrated and very dehydrated. Our system provides an accuracy of 84.5\% in estimating dehydration level with an sensitivity of 87.5\% and a specificity of 90.3\% which provides us confidence of moving forward with our method for larger longitudinal study. }
}

@article{200802871v2,
  title={ Fatigue Assessment using ECG and Actigraphy Sensors },
  author={ Yang Bai and Yu Guan and Wan-Fai Ng },
  journal={ arXiv preprint arXiv:2008.02871v2 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2008.02871v2 },
  abstract={ Fatigue is one of the key factors in the loss of work efficiency and health-related quality of life, and most fatigue assessment methods were based on self-reporting, which may suffer from many factors such as recall bias. To address this issue, we developed an automated system using wearable sensing and machine learning techniques for objective fatigue assessment. ECG/Actigraphy data were collected from subjects in free-living environments. Preprocessing and feature engineering methods were applied, before interpretable solution and deep learning solution were introduced. Specifically, for interpretable solution, we proposed a feature selection approach which can select less correlated and high informative features for better understanding system's decision-making process. For deep learning solution, we used state-of-the-art self-attention model, based on which we further proposed a consistency self-attention (CSA) mechanism for fatigue assessment. Extensive experiments were conducted, and very promising results were achieved. }
}

@article{200801723v1,
  title={ Having a Bad Day? Detecting the Impact of Atypical Life Events Using   Wearable Sensors },
  author={ Keith Burghardt and Nazgol Tavabi and Emilio Ferrara and Shrikanth Narayanan and Kristina Lerman },
  journal={ arXiv preprint arXiv:2008.01723v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2008.01723v1 },
  abstract={ Life events can dramatically affect our psychological state and work performance. Stress, for example, has been linked to professional dissatisfaction, increased anxiety, and workplace burnout. We explore the impact of positive and negative life events on a number of psychological constructs through a multi-month longitudinal study of hospital and aerospace workers. Through causal inference, we demonstrate that positive life events increase positive affect, while negative events increase stress, anxiety and negative affect. While most events have a transient effect on psychological states, major negative events, like illness or attending a funeral, can reduce positive affect for multiple days. Next, we assess whether these events can be detected through wearable sensors, which can cheaply and unobtrusively monitor health-related factors. We show that these sensors paired with embedding-based learning models can be used ``in the wild'' to capture atypical life events in hundreds of workers across both datasets. Overall our results suggest that automated interventions based on physiological sensing may be feasible to help workers regulate the negative effects of life events. }
}

@article{200705831v1,
  title={ MFED: A System for Monitoring Family Eating Dynamics },
  author={ Md Abu Sayeed Mondol and Brooke Bell and Meiyi Ma and Ridwan Alam and Ifat Emi and Sarah Masud Preum and Kayla de la Haye and Donna Spruijt-Metz and John C. Lach and John A. Stankovic },
  journal={ arXiv preprint arXiv:2007.05831v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2007.05831v1 },
  abstract={ Obesity is a risk factor for many health issues, including heart disease, diabetes, osteoarthritis, and certain cancers. One of the primary behavioral causes, dietary intake, has proven particularly challenging to measure and track. Current behavioral science suggests that family eating dynamics (FED) have high potential to impact child and parent dietary intake, and ultimately the risk of obesity. Monitoring FED requires information about when and where eating events are occurring, the presence or absence of family members during eating events, and some person-level states such as stress, mood, and hunger. To date, there exists no system for real-time monitoring of FED. This paper presents MFED, the first of its kind of system for monitoring FED in the wild in real-time. Smart wearables and Bluetooth beacons are used to monitor and detect eating activities and the location of the users at home. A smartphone is used for the Ecological Momentary Assessment (EMA) of a number of behaviors, states, and situations. While the system itself is novel, we also present a novel and efficient algorithm for detecting eating events from wrist-worn accelerometer data. The algorithm improves eating gesture detection F1-score by 19\% with less than 20\% computation compared to the state-of-the-art methods. To date, the MFED system has been deployed in 20 homes with a total of 74 participants, and responses from 4750 EMA surveys have been collected. This paper describes the system components, reports on the eating detection results from the deployments, proposes two techniques for improving ground truth collection after the system is deployed, and provides an overview of the FED data, generated from the multi-component system, that can be used to model and more comprehensively understand insights into the monitoring of family eating dynamics. }
}

@article{200701413v1,
  title={ Wearable Respiration Monitoring: Interpretable Inference with Context   and Sensor Biomarkers },
  author={ Ridwan Alam and David B. Peden and John C. Lach },
  journal={ arXiv preprint arXiv:2007.01413v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2007.01413v1 },
  abstract={ Breathing rate (BR), minute ventilation (VE), and other respiratory parameters are essential for real-time patient monitoring in many acute health conditions, such as asthma. The clinical standard for measuring respiration, namely Spirometry, is hardly suitable for continuous use. Wearables can track many physiological signals, like ECG and motion, yet not respiration. Deriving respiration from other modalities has become an area of active research. In this work, we infer respiratory parameters from wearable ECG and wrist motion signals. We propose a modular and generalizable classification-regression pipeline to utilize available context information, such as physical activity, in learning context-conditioned inference models. Morphological and power domain novel features from the wearable ECG are extracted to use with these models. Exploratory feature selection methods are incorporated in this pipeline to discover application-specific interpretable biomarkers. Using data from 15 subjects, we evaluate two implementations of the proposed pipeline: for inferring BR and VE. Each implementation compares generalized linear model, random forest, support vector machine, Gaussian process regression, and neighborhood component analysis as contextual regression models. Permutation, regularization, and relevance determination methods are used to rank the ECG features to identify robust ECG biomarkers across models and activities. This work demonstrates the potential of wearable sensors not only in continuous monitoring, but also in designing biomarker-driven preventive measures. }
}

@article{200611904v1,
  title={ The CARP Mobile Sensing Framework -- A Cross-platform, Reactive,   Programming Framework and Runtime Environment for Digital Phenotyping },
  author={ Jakob E. Bardram },
  journal={ arXiv preprint arXiv:2006.11904v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2006.11904v1 },
  abstract={ Mobile sensing - i.e., the ability to unobtrusively collect sensor data from built-in phone sensors - has long been a core research topic in Ubicomp. A number of technological platforms for mobile sensing have been presented over the years and a lot of knowledge on how to facilitate mobile sensing has been accumulated. This paper presents the CARP Mobile Sensing (CAMS) framework, which is a modern cross-platform (Android / iOS) software architecture providing a reactive and unified programming model that emphasizes extensibility, maintainability, and adaptability. Moreover, the CAMS framework supports sensing from wearable devices such as an electrocardiography (ECG) monitor, and configuring data transformers. The latter allows to transform collected data to a standardized data format and to implement privacy-preserving data transformations. The paper presents the design, architecture, implementation, and evaluation of CAMS, and shows how the framework has been used in two real-world mobile sensing and mobile health (mHealth) applications. We conclude that CAMS provides a novel cross-platform application programming framework which has proved mature, stable, scalable, and flexible in the design of digital phenotyping and mHealth applications }
}

@article{200607472v1,
  title={ Learning-to-Learn Personalised Human Activity Recognition Models },
  author={ Anjana Wijekoon and Nirmalie Wiratunga },
  journal={ arXiv preprint arXiv:2006.07472v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2006.07472v1 },
  abstract={ Human Activity Recognition\~{}(HAR) is the classification of human movement, captured using one or more sensors either as wearables or embedded in the environment\~{}(e.g. depth cameras, pressure mats). State-of-the-art methods of HAR rely on having access to a considerable amount of labelled data to train deep architectures with many train-able parameters. This becomes prohibitive when tasked with creating models that are sensitive to personal nuances in human movement, explicitly present when performing exercises. In addition, it is not possible to collect training data to cover all possible subjects in the target population. Accordingly, learning personalised models with few data remains an interesting challenge for HAR research. We present a meta-learning methodology for learning to learn personalised HAR models for HAR; with the expectation that the end-user need only provides a few labelled data but can benefit from the rapid adaptation of a generic meta-model. We introduce two algorithms, Personalised MAML and Personalised Relation Networks inspired by existing Meta-Learning algorithms but optimised for learning HAR models that are adaptable to any person in health and well-being applications. A comparative study shows significant performance improvements against the state-of-the-art Deep Learning algorithms and the Few-shot Meta-Learning algorithms in multiple HAR domains. }
}

@article{200608364v1,
  title={ Jointly Predicting Job Performance, Personality, Cognitive Ability,   Affect, and Well-Being },
  author={ Pablo Robles-Granda and Suwen Lin and Xian Wu and Sidney D'Mello and Gonzalo J. Martinez and Koustuv Saha and Kari Nies and Gloria Mark and Andrew T. Campbell and Munmun De Choudhury and Anind D. Dey and Julie Gregg and Ted Grover and Stephen M. Mattingly and Shayan Mirjafari and Edward Moskal and Aaron Striegel and Nitesh V. Chawla },
  journal={ arXiv preprint arXiv:2006.08364v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2006.08364v1 },
  abstract={ Assessment of job performance, personalized health and psychometric measures are domains where data-driven and ubiquitous computing exhibits the potential of a profound impact in the future. Existing techniques use data extracted from questionnaires, sensors (wearable, computer, etc.), or other traits, to assess well-being and cognitive attributes of individuals. However, these techniques can neither predict individual's well-being and psychological traits in a global manner nor consider the challenges associated to processing the data available, that is incomplete and noisy. In this paper, we create a benchmark for predictive analysis of individuals from a perspective that integrates: physical and physiological behavior, psychological states and traits, and job performance. We design data mining techniques as benchmark and uses real noisy and incomplete data derived from wearable sensors to predict 19 constructs based on 12 standardized well-validated tests. The study included 757 participants who were knowledge workers in organizations across the USA with varied work roles. We developed a data mining framework to extract the meaningful predictors for each of the 19 variables under consideration. Our model is the first benchmark that combines these various instrument-derived variables in a single framework to understand people's behavior by leveraging real uncurated data from wearable, mobile, and social media sources. We verify our approach experimentally using the data obtained from our longitudinal study. The results show that our framework is consistently reliable and capable of predicting the variables under study better than the baselines when prediction is restricted to the noisy, incomplete data. }
}

@article{200413085v1,
  title={ Secure Non-public Health Enterprise Networks },
  author={ Mona Ghassemian and Max Smith-Creasey and Maziar Nekovee },
  journal={ arXiv preprint arXiv:2004.13085v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2004.13085v1 },
  abstract={ Increasing demand for secure remote operation in industry and technology advancements to support delivering efficient services and tele-mentoring have opened a new market in healthcare sector and emergency services based on 5G and Tactile internet capabilities. In a connected world, hospitals would benefit from providing the on-time availability either for continuous health monitoring or critical services to the citizens in need. In this paper, we propose a secure non-public health enterprise network concept to enable an end-to-end secure and location-agnostic communication between a patient and a healthcare service provider, and other contacts with patients consent either in case of an emergency or to be stored in the medical records. We present how applying non-public enterprise networks can address market demand in health care sector for improved end-to-end security and privacy when dealing with personal and critical information. We present the three tier architecture model describing continuous authentication mechanisms based on biometric collection as well as the dynamic network solutions in the healthcare domain. The biometric collection can be done using ambient or IoT sensors as well as wearable or implantable devices to monitor the patient unobtrusively. Furthermore, end-to-end security solutions should adapt dynamically based on the user profile and situation awareness to address the required level of security at the network side. We discuss the related research challenges for developing the presented non-public health enterprise platform and provide suggestions for future work based on the healthcare sector requirements and opportunities. }
}

@article{200507532v1,
  title={ 6G Communication Technology: A Vision on Intelligent Healthcare },
  author={ Sabuzima Nayak and Ripon Patgiri },
  journal={ arXiv preprint arXiv:2005.07532v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2005.07532v1 },
  abstract={ 6G is a promising communication technology that will dominate the entire health market from 2030 onward. It will dominate not only health sector but also diverse sectors. It is expected that 6G will revolutionize many sectors including healthcare. Healthcare will be fully AI-driven and dependent on 6G communication technology, which will change our perception of lifestyle. Currently, time and space are the key barriers to health care and 6G will be able to overcome these barriers. Also, 6G will be proven as a game changing technology for healthcare. Therefore, in this perspective, we envision healthcare system for the era of 6G communication technology. Also, various new methodologies have to be introduced to enhance our lifestyle, which is addressed in this perspective, including Quality of Life (QoL), Intelligent Wearable Devices (IWD), Intelligent Internet of Medical Things (IIoMT), Hospital-to-Home (H2H) services, and new business model. In addition, we expose the role of 6G communication technology in telesurgery, Epidemic and Pandemic. }
}

@article{200403047v1,
  title={ Probabilistic modelling of gait for robust passive monitoring in daily   life },
  author={ Yordan P. Raykov and Luc J. W. Evers and Reham Badawy and Bastiaan Bloem and Tom M. Heskes and Marjan Meinders and Kasper Claes and Max A. Little },
  journal={ arXiv preprint arXiv:2004.03047v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2004.03047v1 },
  abstract={ Passive monitoring in daily life may provide invaluable insights about a person's health throughout the day. Wearable sensor devices are likely to play a key role in enabling such monitoring in a non-obtrusive fashion. However, sensor data collected in daily life reflects multiple health and behavior related factors together. This creates the need for structured principled analysis to produce reliable and interpretable predictions that can be used to support clinical diagnosis and treatment. In this work we develop a principled modelling approach for free-living gait (walking) analysis. Gait is a promising target for non-obtrusive monitoring because it is common and indicative of various movement disorders such as Parkinson's disease (PD), yet its analysis has largely been limited to experimentally controlled lab settings. To locate and characterize stationary gait segments in free living using accelerometers, we present an unsupervised statistical framework designed to segment signals into differing gait and non-gait patterns. Our flexible probabilistic framework combines empirical assumptions about gait into a principled graphical model with all of its merits. We demonstrate the approach on a new video-referenced dataset including unscripted daily living activities of 25 PD patients and 25 controls, in and around their own houses. We evaluate our ability to detect gait and predict medication induced fluctuations in PD patients based on modelled gait. Our evaluation includes a comparison between sensors attached at multiple body locations including wrist, ankle, trouser pocket and lower back. }
}

@article{200313098v1,
  title={ Proximity-Based Active Learning on Streaming Data: A Personalized Eating   Moment Recognition },
  author={ Marjan Nourollahi and Seyed Ali Rokni and Hassan Ghasemzadeh },
  journal={ arXiv preprint arXiv:2003.13098v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2003.13098v1 },
  abstract={ Detecting when eating occurs is an essential step toward automatic dietary monitoring, medication adherence assessment, and diet-related health interventions. Wearable technologies play a central role in designing unubtrusive diet monitoring solutions by leveraging machine learning algorithms that work on time-series sensor data to detect eating moments. While much research has been done on developing activity recognition and eating moment detection algorithms, the performance of the detection algorithms drops substantially when the model trained with one user is utilized by a new user. To facilitate development of personalized models, we propose PALS, Proximity-based Active Learning on Streaming data, a novel proximity-based model for recognizing eating gestures with the goal of significantly decreasing the need for labeled data with new users. Particularly, we propose an optimization problem to perform active learning under limited query budget by leveraging unlabeled data. Our extensive analysis on data collected in both controlled and uncontrolled settings indicates that the F-score of PLAS ranges from 22\% to 39\% for a budget that varies from 10 to 60 query. Furthermore, compared to the state-of-the-art approaches, off-line PALS, on average, achieves to 40\% higher recall and 12\\textbackslash{}\% higher f-score in detecting eating gestures. }
}

@article{200204700v4,
  title={ A Single RGB Camera Based Gait Analysis with a Mobile Tele-Robot for   Healthcare },
  author={ Ziyang Wang },
  journal={ arXiv preprint arXiv:2002.04700v4 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2002.04700v4 },
  abstract={ With the increasing awareness of high-quality life, there is a growing need for health monitoring devices running robust algorithms in home environment. Health monitoring technologies enable real-time analysis of users' health status, offering long-term healthcare support and reducing hospitalization time. The purpose of this work is twofold, the software focuses on the analysis of gait, which is widely adopted for joint correction and assessing any lower limb or spinal problem. On the hardware side, we design a novel marker-less gait analysis device using a low-cost RGB camera mounted on a mobile tele-robot. As gait analysis with a single camera is much more challenging compared to previous works utilizing multi-cameras, a RGB-D camera or wearable sensors, we propose using vision-based human pose estimation approaches. More specifically, based on the output of two state-of-the-art human pose estimation models (Openpose and VNect), we devise measurements for four bespoke gait parameters: inversion/eversion, dorsiflexion/plantarflexion, ankle and foot progression angles. We thereby classify walking patterns into normal, supination, pronation and limp. We also illustrate how to run the purposed machine learning models in low-resource environments such as a single entry-level CPU. Experiments show that our single RGB camera method achieves competitive performance compared to state-of-the-art methods based on depth cameras or multi-camera motion capture system, at smaller hardware costs. }
}

@article{250511612v1,
  title={ Heart2Mind: Human-Centered Contestable Psychiatric Disorder Diagnosis   System using Wearable ECG Monitors },
  author={ Hung Nguyen and Alireza Rahimi and Veronica Whitford and Hélène Fournier and Irina Kondratova and René Richard and Hung Cao },
  journal={ arXiv preprint arXiv:2505.11612v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2505.11612v1 },
  abstract={ Psychiatric disorders affect millions globally, yet their diagnosis faces significant challenges in clinical practice due to subjective assessments and accessibility concerns, leading to potential delays in treatment. To help address this issue, we present Heart2Mind, a human-centered contestable psychiatric disorder diagnosis system using wearable electrocardiogram (ECG) monitors. Our approach leverages cardiac biomarkers, particularly heart rate variability (HRV) and R-R intervals (RRI) time series, as objective indicators of autonomic dysfunction in psychiatric conditions. The system comprises three key components: (1) a Cardiac Monitoring Interface (CMI) for real-time data acquisition from Polar H9/H10 devices; (2) a Multi-Scale Temporal-Frequency Transformer (MSTFT) that processes RRI time series through integrated time-frequency domain analysis; (3) a Contestable Diagnosis Interface (CDI) combining Self-Adversarial Explanations (SAEs) with contestable Large Language Models (LLMs). Our MSTFT achieves 91.7\% accuracy on the HRV-ACC dataset using leave-one-out cross-validation, outperforming state-of-the-art methods. SAEs successfully detect inconsistencies in model predictions by comparing attention-based and gradient-based explanations, while LLMs enable clinicians to validate correct predictions and contest erroneous ones. This work demonstrates the feasibility of combining wearable technology with Explainable Artificial Intelligence (XAI) and contestable LLMs to create a transparent, contestable system for psychiatric diagnosis that maintains clinical oversight while leveraging advanced AI capabilities. Our implementation is publicly available at: https://github.com/Analytics-Everywhere-Lab/heart2mind. }
}

@article{250503787v1,
  title={ ArrhythmiaVision: Resource-Conscious Deep Learning Models with Visual   Explanations for ECG Arrhythmia Classification },
  author={ Zuraiz Baig and Sidra Nasir and Rizwan Ahmed Khan and Muhammad Zeeshan Ul Haque },
  journal={ arXiv preprint arXiv:2505.03787v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2505.03787v1 },
  abstract={ Cardiac arrhythmias are a leading cause of life-threatening cardiac events, highlighting the urgent need for accurate and timely detection. Electrocardiography (ECG) remains the clinical gold standard for arrhythmia diagnosis; however, manual interpretation is time-consuming, dependent on clinical expertise, and prone to human error. Although deep learning has advanced automated ECG analysis, many existing models abstract away the signal's intrinsic temporal and morphological features, lack interpretability, and are computationally intensive-hindering their deployment on resource-constrained platforms. In this work, we propose two novel lightweight 1D convolutional neural networks, ArrhythmiNet V1 and V2, optimized for efficient, real-time arrhythmia classification on edge devices. Inspired by MobileNet's depthwise separable convolutional design, these models maintain memory footprints of just 302.18 KB and 157.76 KB, respectively, while achieving classification accuracies of 0.99 (V1) and 0.98 (V2) on the MIT-BIH Arrhythmia Dataset across five classes: Normal Sinus Rhythm, Left Bundle Branch Block, Right Bundle Branch Block, Atrial Premature Contraction, and Premature Ventricular Contraction. In order to ensure clinical transparency and relevance, we integrate Shapley Additive Explanations and Gradient-weighted Class Activation Mapping, enabling both local and global interpretability. These techniques highlight physiologically meaningful patterns such as the QRS complex and T-wave that contribute to the model's predictions. We also discuss performance-efficiency trade-offs and address current limitations related to dataset diversity and generalizability. Overall, our findings demonstrate the feasibility of combining interpretability, predictive accuracy, and computational efficiency in practical, wearable, and embedded ECG monitoring systems. }
}

@article{250412921v1,
  title={ IdentiARAT: Toward Automated Identification of Individual ARAT Items   from Wearable Sensors },
  author={ Daniel Homm and Patrick Carqueville and Christian Eichhorn and Thomas Weikert and Thomas Menard and David A. Plecher and Chris Awai Easthope },
  journal={ arXiv preprint arXiv:2504.12921v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2504.12921v1 },
  abstract={ This study explores the potential of using wrist-worn inertial sensors to automate the labeling of ARAT (Action Research Arm Test) items. While the ARAT is commonly used to assess upper limb motor function, its limitations include subjectivity and time consumption of clinical staff. By using IMU (Inertial Measurement Unit) sensors and MiniROCKET as a time series classification technique, this investigation aims to classify ARAT items based on sensor recordings. We test common preprocessing strategies to efficiently leverage included information in the data. Afterward, we use the best preprocessing to improve the classification. The dataset includes recordings of 45 participants performing various ARAT items. Results show that MiniROCKET offers a fast and reliable approach for classifying ARAT domains, although challenges remain in distinguishing between individual resembling items. Future work may involve improving classification through more advanced machine-learning models and data enhancements. }
}

@article{241119000v3,
  title={ An AI-driven multimodal smart home platform for continuous monitoring   and intelligent assistance in post-stroke patients },
  author={ Chenyu Tang and Ruizhi Zhang and Shuo Gao and Zihe Zhao and Zibo Zhang and Jiaqi Wang and Cong Li and Junliang Chen and Yanning Dai and Shengbo Wang and Ruoyu Juan and Qiaoying Li and Ruimou Xie and Xuhang Chen and Xinkai Zhou and Yunjia Xia and Jianan Chen and Fanghao Lu and Xin Li and Ninglli Wang and Peter Smielewski and Yu Pan and Hubin Zhao and Luigi G. Occhipinti },
  journal={ arXiv preprint arXiv:2411.19000v3 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2411.19000v3 },
  abstract={ At-home rehabilitation for post-stroke patients presents significant challenges, as continuous, personalized care is often limited outside clinical settings. Additionally, the absence of comprehensive solutions addressing diverse monitoring and assistance needs in home environments complicates recovery efforts. Here, we present a multimodal smart home platform designed for continuous, at-home rehabilitation of post-stroke patients, integrating wearable sensing, ambient monitoring, and adaptive automation. A plantar pressure insole equipped with a machine learning pipeline classifies users into motor recovery stages with up to 94\% accuracy, enabling quantitative tracking of walking patterns. A head-mounted eye-tracking module supports cognitive assessments and hands-free control of household devices, while ambient sensors ensure sub-second response times for interaction. These data streams are fused locally via a hierarchical Internet of Things (IoT) architecture, protecting privacy and minimizing latency. An embedded large language model (LLM) agent, Auto-Care, continuously interprets multimodal data to provide real-time interventions-issuing personalized reminders, adjusting environmental conditions, and notifying caregivers. Implemented in a post-stroke context, this integrated smart home platform increases overall user satisfaction by an average of 115\% (p<0.01) compared to traditional home environment. Beyond stroke, the system offers a scalable framework for patient-centered, long-term care in broader neurorehabilitation and aging-in-place applications. }
}

@article{250409299v1,
  title={ Beyond Glucose-Only Assessment: Advancing Nocturnal Hypoglycemia   Prediction in Children with Type 1 Diabetes },
  author={ Marco Voegeli and Sonia Laguna and Heike Leutheuser and Marc Pfister and Marie-Anne Burckhardt and Julia E Vogt },
  journal={ arXiv preprint arXiv:2504.09299v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2504.09299v1 },
  abstract={ The dead-in-bed syndrome describes the sudden and unexplained death of young individuals with Type 1 Diabetes (T1D) without prior long-term complications. One leading hypothesis attributes this phenomenon to nocturnal hypoglycemia (NH), a dangerous drop in blood glucose during sleep. This study aims to improve NH prediction in children with T1D by leveraging physiological data and machine learning (ML) techniques. We analyze an in-house dataset collected from 16 children with T1D, integrating physiological metrics from wearable sensors. We explore predictive performance through feature engineering, model selection, architectures, and oversampling. To address data limitations, we apply transfer learning from a publicly available adult dataset. Our results achieve an AUROC of 0.75 +- 0.21 on the in-house dataset, further improving to 0.78 +- 0.05 with transfer learning. This research moves beyond glucose-only predictions by incorporating physiological parameters, showcasing the potential of ML to enhance NH detection and improve clinical decision-making for pediatric diabetes management. }
}

@article{250408877v1,
  title={ The SERENADE project: Sensor-Based Explainable Detection of Cognitive   Decline },
  author={ Gabriele Civitarese and Michele Fiori and Andrea Arighi and Daniela Galimberti and Graziana Florio and Claudio Bettini },
  journal={ arXiv preprint arXiv:2504.08877v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2504.08877v1 },
  abstract={ Mild Cognitive Impairment (MCI) affects 12-18\% of individuals over 60. MCI patients exhibit cognitive dysfunctions without significant daily functional loss. While MCI may progress to dementia, predicting this transition remains a clinical challenge due to limited and unreliable indicators. Behavioral changes, like in the execution of Activities of Daily Living (ADLs), can signal such progression. Sensorized smart homes and wearable devices offer an innovative solution for continuous, non-intrusive monitoring ADLs for MCI patients. However, current machine learning models for detecting behavioral changes lack transparency, hindering clinicians' trust. This paper introduces the SERENADE project, a European Union-funded initiative that aims to detect and explain behavioral changes associated with cognitive decline using explainable AI methods. SERENADE aims at collecting one year of data from 30 MCI patients living alone, leveraging AI to support clinical decision-making and offering a new approach to early dementia detection. }
}

@article{241118266v3,
  title={ Wearable intelligent throat enables natural speech in stroke patients   with dysarthria },
  author={ Chenyu Tang and Shuo Gao and Cong Li and Wentian Yi and Yuxuan Jin and Xiaoxue Zhai and Sixuan Lei and Hongbei Meng and Zibo Zhang and Muzi Xu and Shengbo Wang and Xuhang Chen and Chenxi Wang and Hongyun Yang and Ningli Wang and Wenyu Wang and Jin Cao and Xiaodong Feng and Peter Smielewski and Yu Pan and Wenhui Song and Martin Birchall and Luigi G. Occhipinti },
  journal={ arXiv preprint arXiv:2411.18266v3 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2411.18266v3 },
  abstract={ Wearable silent speech systems hold significant potential for restoring communication in patients with speech impairments. However, seamless, coherent speech remains elusive, and clinical efficacy is still unproven. Here, we present an AI-driven intelligent throat (IT) system that integrates throat muscle vibrations and carotid pulse signal sensors with large language model (LLM) processing to enable fluent, emotionally expressive communication. The system utilizes ultrasensitive textile strain sensors to capture high-quality signals from the neck area and supports token-level processing for real-time, continuous speech decoding, enabling seamless, delay-free communication. In tests with five stroke patients with dysarthria, IT's LLM agents intelligently corrected token errors and enriched sentence-level emotional and logical coherence, achieving low error rates (4.2\% word error rate, 2.9\% sentence error rate) and a 55\% increase in user satisfaction. This work establishes a portable, intuitive communication platform for patients with dysarthria with the potential to be applied broadly across different neurological conditions and in multi-language support systems. }
}

@article{250308814v1,
  title={ An Iterative, User-Centered Design of a Clinical Decision Support System   for Critical Care Assessments: Co-Design Sessions with ICU Clinical Providers },
  author={ Andrea E. Davidson and Jessica M. Ray and Ayush K. Patel and Yulia Strekalova Levites and Parisa Rashidi and Azra Bihorac },
  journal={ arXiv preprint arXiv:2503.08814v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.08814v1 },
  abstract={ This study reports the findings of qualitative interview sessions conducted with ICU clinicians for the co-design of a system user interface of an artificial intelligence (AI)-driven clinical decision support (CDS) system. This system integrates medical record data with wearable sensor, video, and environmental data into a real-time dynamic model that quantifies patients' risk of clinical decompensation and risk of developing delirium, providing actionable alerts to augment clinical decision-making in the ICU setting. Co-design sessions were conducted as semi-structured focus groups and interviews with ICU clinicians, including physicians, mid-level practitioners, and nurses. Study participants were asked about their perceptions on AI-CDS systems, their system preferences, and were asked to provide feedback on the current user interface prototype. Session transcripts were qualitatively analyzed to identify key themes related to system utility, interface design features, alert preferences, and implementation considerations. Ten clinicians participated in eight sessions. The analysis identified five themes: (1) AI's computational utility, (2) workflow optimization, (3) effects on patient care, (4) technical considerations, and (5) implementation considerations. Clinicians valued the CDS system's multi-modal continuous monitoring and AI's capacity to process large volumes of data in real-time to identify patient risk factors and suggest action items. Participants underscored the system's unique value in detecting delirium and promoting non-pharmacological delirium prevention measures. The actionability and intuitive interpretation of the presented information was emphasized. ICU clinicians recognize the potential of an AI-driven CDS system for ICU delirium and acuity to improve patient outcomes and clinical workflows. }
}

@article{250307276v1,
  title={ A Systematic Review of ECG Arrhythmia Classification: Adherence to   Standards, Fair Evaluation, and Embedded Feasibility },
  author={ Guilherme Silva and Pedro Silva and Gladston Moreira and Vander Freitas and Jadson Gertrudes and Eduardo Luz },
  journal={ arXiv preprint arXiv:2503.07276v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.07276v1 },
  abstract={ The classification of electrocardiogram (ECG) signals is crucial for early detection of arrhythmias and other cardiac conditions. However, despite advances in machine learning, many studies fail to follow standardization protocols, leading to inconsistencies in performance evaluation and real-world applicability. Additionally, hardware constraints essential for practical deployment, such as in pacemakers, Holter monitors, and wearable ECG patches, are often overlooked. Since real-world impact depends on feasibility in resource-constrained devices, ensuring efficient deployment is critical for continuous monitoring. This review systematically analyzes ECG classification studies published between 2017 and 2024, focusing on those adhering to the E3C (Embedded, Clinical, and Comparative Criteria), which include inter-patient paradigm implementation, compliance with Association for the Advancement of Medical Instrumentation (AAMI) recommendations, and model feasibility for embedded systems. While many studies report high accuracy, few properly consider patient-independent partitioning and hardware limitations. We identify state-of-the-art methods meeting E3C criteria and conduct a comparative analysis of accuracy, inference time, energy consumption, and memory usage. Finally, we propose standardized reporting practices to ensure fair comparisons and practical applicability of ECG classification models. By addressing these gaps, this study aims to guide future research toward more robust and clinically viable ECG classification systems. }
}

@article{250203273v1,
  title={ Bayesian Covariate-Dependent Circadian Modeling of Rest-Activity Rhythms },
  author={ Beniamino Hadj-Amar and Vaishnav Krishnan and Marina Vannucci },
  journal={ arXiv preprint arXiv:2502.03273v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.03273v1 },
  abstract={ We propose a Bayesian covariate-dependent anti-logistic circadian model for analyzing activity data collected via wrist-worn wearable devices. The proposed approach integrates covariates into the modeling of the amplitude and phase parameters, facilitating cohort-level analysis with enhanced flexibility and interpretability. To promote model sparsity, we employ an l\_1-ball projection prior, enabling precise control over complexity while identifying significant predictors. We assess performances on simulated data and then apply the method to real-world actigraphy data from people with epilepsy. Our results demonstrate the model's effectiveness in uncovering complex relationships among demographic, psychological, and medical factors influencing rest-activity rhythms, offering insights for personalized clinical assessments and healthcare interventions. }
}

@article{240616901v3,
  title={ ECGrecover: a Deep Learning Approach for Electrocardiogram Signal   Completion },
  author={ Alex Lence and Federica Granese and Ahmad Fall and Blaise Hanczar and Joe-Elie Salem and Jean-Daniel Zucker and Edi Prifti },
  journal={ arXiv preprint arXiv:2406.16901v3 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2406.16901v3 },
  abstract={ In this work, we address the challenge of reconstructing the complete 12-lead ECG signal from its incomplete parts. We focus on two main scenarios: (i) reconstructing missing signal segments within an ECG lead and (ii) recovering entire leads from signal in another unique lead. Two emerging clinical applications emphasize the relevance of our work. The first is the increasing need to digitize paper-stored ECGs for utilization in AI-based applications, often limited to digital 12 lead 10s ECGs. The second is the widespread use of wearable devices that record ECGs but typically capture only one or a few leads. In both cases, a non-negligible amount of information is lost or not recorded. Our approach aims to recover this missing signal. We propose ECGrecover, a U-Net neural network model trained on a novel composite objective function to address the reconstruction problem. This function incorporates both spatial and temporal features of the ECG by combining the distance in amplitude and sycnhronization through time between the reconstructed and the real digital signals. We used real-life ECG datasets and through comprehensive assessments compared ECGrecover with three state-of-the-art methods based on generative adversarial networks (EKGAN, Pix2Pix) as well as the CopyPaste strategy. The results demonstrated that ECGrecover consistently outperformed state-of-the-art methods in standard distortion metrics as well as in preserving critical ECG characteristics, particularly the P, QRS, and T wave coordinates. }
}

@article{241211286v1,
  title={ Detecting Daily Living Gait Amid Huntington's Disease Chorea using a   Foundation Deep Learning Model },
  author={ Dafna Schwartz and Lori Quinn and Nora E. Fritz and Lisa M. Muratori and Jeffery M. Hausdorff and Ran Gilad Bachrach },
  journal={ arXiv preprint arXiv:2412.11286v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.11286v1 },
  abstract={ Wearable sensors offer a non-invasive way to collect physical activity (PA) data, with walking as a key component. Existing models often struggle to detect gait bouts in individuals with neurodegenerative diseases (NDDs) involving involuntary movements. We developed J-Net, a deep learning model inspired by U-Net, which uses a pre-trained self-supervised foundation model fine-tuned with Huntington`s disease (HD) in-lab data and paired with a segmentation head for gait detection. J-Net processes wrist-worn accelerometer data to detect gait during daily living. We evaluated J-Net on in-lab and daily-living data from HD, Parkinson`s disease (PD), and controls. J-Net achieved a 10-percentage point improvement in ROC-AUC for HD over existing methods, reaching 0.97 for in-lab data. In daily-living environments, J-Net estimates showed no significant differences in median daily walking time between HD and controls (p = 0.23), in contrast to other models, which indicated counterintuitive results (p < 0.005). Walking time measured by J-Net correlated with the UHDRS-TMS clinical severity score (r=-0.52; p=0.02), confirming its clinical relevance. Fine-tuning J-Net on PD data also improved gait detection over current methods. J-Net`s architecture effectively addresses the challenges of gait detection in severe chorea and offers robust performance in daily living. The dataset and J-Net model are publicly available, providing a resource for further research into NDD-related gait impairments. }
}

@article{241210846v1,
  title={ Detecting Activities of Daily Living in Egocentric Video to   Contextualize Hand Use at Home in Outpatient Neurorehabilitation Settings },
  author={ Adesh Kadambi and José Zariffa },
  journal={ arXiv preprint arXiv:2412.10846v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.10846v1 },
  abstract={ Wearable egocentric cameras and machine learning have the potential to provide clinicians with a more nuanced understanding of patient hand use at home after stroke and spinal cord injury (SCI). However, they require detailed contextual information (i.e., activities and object interactions) to effectively interpret metrics and meaningfully guide therapy planning. We demonstrate that an object-centric approach, focusing on what objects patients interact with rather than how they move, can effectively recognize Activities of Daily Living (ADL) in real-world rehabilitation settings. We evaluated our models on a complex dataset collected in the wild comprising 2261 minutes of egocentric video from 16 participants with impaired hand function. By leveraging pre-trained object detection and hand-object interaction models, our system achieves robust performance across different impairment levels and environments, with our best model achieving a mean weighted F1-score of 0.78 +/- 0.12 and maintaining an F1-score > 0.5 for all participants using leave-one-subject-out cross validation. Through qualitative analysis, we observe that this approach generates clinically interpretable information about functional object use while being robust to patient-specific movement variations, making it particularly suitable for rehabilitation contexts with prevalent upper limb impairment. }
}

@article{241114656v2,
  title={ mmWave Radar for Sit-to-Stand Analysis: A Comparative Study with   Wearables and Kinect },
  author={ Shuting Hu and Peggy Ackun and Xiang Zhang and Siyang Cao and Jennifer Barton and Melvin G. Hector and Mindy J. Fain and Nima Toosizadeh },
  journal={ arXiv preprint arXiv:2411.14656v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2411.14656v2 },
  abstract={ This study explores a novel approach for analyzing Sit-to-Stand (STS) movements using millimeter-wave (mmWave) radar technology. The goal is to develop a non-contact sensing, privacy-preserving, and all-day operational method for healthcare applications, including fall risk assessment. We used a 60GHz mmWave radar system to collect radar point cloud data, capturing STS motions from 45 participants. By employing a deep learning pose estimation model, we learned the human skeleton from Kinect built-in body tracking and applied Inverse Kinematics (IK) to calculate joint angles, segment STS motions, and extract commonly used features in fall risk assessment. Radar extracted features were then compared with those obtained from Kinect and wearable sensors. The results demonstrated the effectiveness of mmWave radar in capturing general motion patterns and large joint movements (e.g., trunk). Additionally, the study highlights the advantages and disadvantages of individual sensors and suggests the potential of integrated sensor technologies to improve the accuracy and reliability of motion analysis in clinical and biomedical research settings. }
}

@article{241115366v1,
  title={ Personalization of Wearable Sensor-Based Joint Kinematic Estimation   Using Computer Vision for Hip Exoskeleton Applications },
  author={ Changseob Song and Bogdan Ivanyuk-Skulskyi and Adrian Krieger and Kaitao Luo and Inseung Kang },
  journal={ arXiv preprint arXiv:2411.15366v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2411.15366v1 },
  abstract={ Accurate lower-limb joint kinematic estimation is critical for applications such as patient monitoring, rehabilitation, and exoskeleton control. While previous studies have employed wearable sensor-based deep learning (DL) models for estimating joint kinematics, these methods often require extensive new datasets to adapt to unseen gait patterns. Meanwhile, researchers in computer vision have advanced human pose estimation models, which are easy to deploy and capable of real-time inference. However, such models are infeasible in scenarios where cameras cannot be used. To address these limitations, we propose a computer vision-based DL adaptation framework for real-time joint kinematic estimation. This framework requires only a small dataset (i.e., 1-2 gait cycles) and does not depend on professional motion capture setups. Using transfer learning, we adapted our temporal convolutional network (TCN) to stiff knee gait data, allowing the model to further reduce root mean square error by 9.7\% and 19.9\% compared to a TCN trained on only able-bodied and stiff knee datasets, respectively. Our framework demonstrates a potential for smartphone camera-trained DL models to estimate real-time joint kinematics across novel users in clinical populations with applications in wearable robots. }
}

@article{241009643v1,
  title={ Multimodal Physical Activity Forecasting in Free-Living Clinical   Settings: Hunting Opportunities for Just-in-Time Interventions },
  author={ Abdullah Mamun and Krista S. Leonard and Megan E. Petrov and Matthew P. Buman and Hassan Ghasemzadeh },
  journal={ arXiv preprint arXiv:2410.09643v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.09643v1 },
  abstract={ Objective: This research aims to develop a lifestyle intervention system, called MoveSense, that forecasts a patient's activity behavior to allow for early and personalized interventions in real-world clinical environments. Methods: We conducted two clinical studies involving 58 prediabetic veterans and 60 patients with obstructive sleep apnea to gather multimodal behavioral data using wearable devices. We develop multimodal long short-term memory (LSTM) network models, which are capable of forecasting the number of step counts of a patient up to 24 hours in advance by examining data from activity and engagement modalities. Furthermore, we design goal-based forecasting models to predict whether a person's next-day steps will be over a certain threshold. Results: Multimodal LSTM with early fusion achieves 33\% and 37\% lower mean absolute errors than linear regression and ARIMA respectively on the prediabetes dataset. LSTM also outperforms linear regression and ARIMA with a margin of 13\% and 32\% on the sleep dataset. Multimodal forecasting models also perform with 72\% and 79\% accuracy on the prediabetes dataset and sleep dataset respectively on goal-based forecasting. Conclusion: Our experiments conclude that multimodal LSTM models with early fusion are better than multimodal LSTM with late fusion and unimodal LSTM models and also than ARIMA and linear regression models. Significance: We address an important and challenging task of time-series forecasting in uncontrolled environments. Effective forecasting of a person's physical activity can aid in designing adaptive behavioral interventions to keep the user engaged and adherent to a prescribed routine. }
}

@article{241005491v1,
  title={ Pre-Ictal Seizure Prediction Using Personalized Deep Learning },
  author={ Shriya Jaddu and Sidh Jaddu and Camilo Gutierrez and Quincy K. Tran },
  journal={ arXiv preprint arXiv:2410.05491v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.05491v1 },
  abstract={ Introduction: Approximately 23 million or 30\% of epilepsy patients worldwide suffer from drug-resistant epilepsy (DRE). The unpredictability of seizure occurrences, which causes safety issues as well as social concerns, restrict the lifestyles of DRE patients. Surgical solutions and EEG-based solutions are very expensive, unreliable, invasive or impractical. The goal of this research was to employ improved technologies and methods to epilepsy patient physiological data and predict seizures up to two hours before onset, enabling non-invasive, affordable seizure prediction for DRE patients.   Methods: This research used a 1D Convolutional Neural Network-Based Bidirectional Long Short-Term Memory network that was trained on a diverse set of epileptic patient physiological data to predict seizures. Transfer learning was further utilized to personalize and optimize predictions for specific patients. Clinical data was retrospectively obtained for nine epilepsy patients via wearable devices over a period of about three to five days from a prospectively maintained database. The physiological data included 54 seizure occurrences and included heart rate, blood volume pulse, accelerometry, body temperature, and electrodermal activity.   Results and Conclusion: A general deep-learning model trained on the physiological data with randomly sampled test data achieved an accuracy of 91.94\%. However, such a generalized deep learning model had varied performances on data from unseen patients. When the general model was personalized (further trained) with patient-specific data, the personalized model achieved significantly improved performance with accuracies as high as 97\%. This preliminary research shows that patient-specific personalization may be a viable approach to achieve affordable, non-invasive seizure prediction that can improve the quality of life for DRE patients. }
}

@article{230705385v4,
  title={ Sparse learned kernels for interpretable and efficient medical time   series processing },
  author={ Sully F. Chen and Zhicheng Guo and Cheng Ding and Xiao Hu and Cynthia Rudin },
  journal={ arXiv preprint arXiv:2307.05385v4 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2307.05385v4 },
  abstract={ Rapid, reliable, and accurate interpretation of medical time-series signals is crucial for high-stakes clinical decision-making. Deep learning methods offered unprecedented performance in medical signal processing but at a cost: they were compute-intensive and lacked interpretability. We propose Sparse Mixture of Learned Kernels (SMoLK), an interpretable architecture for medical time series processing. SMoLK learns a set of lightweight flexible kernels that form a single-layer sparse neural network, providing not only interpretability, but also efficiency, robustness, and generalization to unseen data distributions. We introduce a parameter reduction techniques to reduce the size of SMoLK's networks while maintaining performance. We test SMoLK on two important tasks common to many consumer wearables: photoplethysmography (PPG) artifact detection and atrial fibrillation detection from single-lead electrocardiograms (ECGs). We find that SMoLK matches the performance of models orders of magnitude larger. It is particularly suited for real-time applications using low-power devices, and its interpretability benefits high-stakes situations. }
}

@article{240909161v2,
  title={ Train-On-Request: An On-Device Continual Learning Workflow for Adaptive   Real-World Brain Machine Interfaces },
  author={ Lan Mei and Cristian Cioflan and Thorir Mar Ingolfsson and Victor Kartsch and Andrea Cossettini and Xiaying Wang and Luca Benini },
  journal={ arXiv preprint arXiv:2409.09161v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2409.09161v2 },
  abstract={ Brain-machine interfaces (BMIs) are expanding beyond clinical settings thanks to advances in hardware and algorithms. However, they still face challenges in user-friendliness and signal variability. Classification models need periodic adaptation for real-life use, making an optimal re-training strategy essential to maximize user acceptance and maintain high performance. We propose TOR, a train-on-request workflow that enables user-specific model adaptation to novel conditions, addressing signal variability over time. Using continual learning, TOR preserves knowledge across sessions and mitigates inter-session variability. With TOR, users can refine, on demand, the model through on-device learning (ODL) to enhance accuracy adapting to changing conditions. We evaluate the proposed methodology on a motor-movement dataset recorded with a non-stigmatizing wearable BMI headband, achieving up to 92\% accuracy and a re-calibration time as low as 1.6 minutes, a 46\% reduction compared to a naive transfer learning workflow. We additionally demonstrate that TOR is suitable for ODL in extreme edge settings by deploying the training procedure on a RISC-V ultra-low-power SoC (GAP9), resulting in 21.6 ms of latency and 1 mJ of energy consumption per training step. To the best of our knowledge, this work is the first demonstration of an online, energy-efficient, dynamic adaptation of a BMI model to the intrinsic variability of EEG signals in real-time settings. }
}

@article{240816885v1,
  title={ A Prototype Model of Zero-Trust Architecture Blockchain with   EigenTrust-Based Practical Byzantine Fault Tolerance Protocol to Manage   Decentralized Clinical Trials },
  author={ Ashok Kumar Peepliwall and Hari Mohan Pandey and Surya Prakash and Anand A Mahajan and Sudhinder Singh Chowhan and Vinesh Kumar and Rahul Sharma },
  journal={ arXiv preprint arXiv:2408.16885v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.16885v1 },
  abstract={ The COVID-19 pandemic necessitated the emergence of decentralized Clinical Trials (DCTs) due to patient retention, accelerate trials, improve data accessibility, enable virtual care, and facilitate seamless communication through integrated systems. However, integrating systems in DCTs exposes clinical data to potential security threats, making them susceptible to theft at any stage, a high risk of protocol deviations, and monitoring issues. To mitigate these challenges, blockchain technology serves as a secure framework, acting as a decentralized ledger, creating an immutable environment by establishing a zero-trust architecture, where data are deemed untrusted until verified. In combination with Internet of Things (IoT)-enabled wearable devices, blockchain secures the transfer of clinical trial data on private blockchains during DCT automation and operations. This paper proposes a prototype model of the Zero-Trust Architecture Blockchain (z-TAB) to integrate patient-generated clinical trial data during DCT operation management. The EigenTrust-based Practical Byzantine Fault Tolerance (T-PBFT) algorithm has been incorporated as a consensus protocol, leveraging Hyperledger Fabric. Furthermore, the Internet of Things (IoT) has been integrated to streamline data processing among stakeholders within the blockchain platforms. Rigorous evaluation has been done to evaluate the quality of the system. }
}

@article{240905891v1,
  title={ In-ear ECG Signal Enhancement with Denoising Convolutional Autoencoders },
  author={ Edoardo Occhipinti and Marek Zylinski and Harry J. Davies and Amir Nassibi and Matteo Bermond and Patrik Bachtiger and Nicholas S. Peters and Danilo P. Mandic },
  journal={ arXiv preprint arXiv:2409.05891v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2409.05891v1 },
  abstract={ The cardiac dipole has been shown to propagate to the ears, now a common site for consumer wearable electronics, enabling the recording of electrocardiogram (ECG) signals. However, in-ear ECG recordings often suffer from significant noise due to their small amplitude and the presence of other physiological signals, such as electroencephalogram (EEG), which complicates the extraction of cardiovascular features. This study addresses this issue by developing a denoising convolutional autoencoder (DCAE) to enhance ECG information from in-ear recordings, producing cleaner ECG outputs. The model is evaluated using a dataset of in-ear ECGs and corresponding clean Lead I ECGs from 45 healthy participants. The results demonstrate a substantial improvement in signal-to-noise ratio (SNR), with a median increase of 5.9 dB. Additionally, the model significantly improved heart rate estimation accuracy, reducing the mean absolute error by almost 70\% and increasing R-peak detection precision to a median value of 90\%. We also trained and validated the model using a synthetic dataset, generated from real ECG signals, including abnormal cardiac morphologies, corrupted by pink noise. The results obtained show effective removal of noise sources with clinically plausible waveform reconstruction ability. }
}

@article{240807817v1,
  title={ MyoGestic: EMG Interfacing Framework for Decoding Multiple Spared   Degrees of Freedom of the Hand in Individuals with Neural Lesions },
  author={ Raul C. Sîmpetru and Dominik I. Braun and Arndt U. Simon and Michael März and Vlad Cnejevici and Daniela Souza de Oliveira and Nico Weber and Jonas Walter and Jörg Franke and Daniel Höglinger and Cosima Prahm and Matthias Ponfick and Alessandro Del Vecchio },
  journal={ arXiv preprint arXiv:2408.07817v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.07817v1 },
  abstract={ Restoring limb motor function in individuals with spinal cord injury (SCI), stroke, or amputation remains a critical challenge, one which affects millions worldwide. Recent studies show through surface electromyography (EMG) that spared motor neurons can still be voluntarily controlled, even without visible limb movement . These signals can be decoded and used for motor intent estimation; however, current wearable solutions lack the necessary hardware and software for intuitive interfacing of the spared degrees of freedom after neural injuries. To address these limitations, we developed a wireless, high-density EMG bracelet, coupled with a novel software framework, MyoGestic. Our system allows rapid and tailored adaptability of machine learning models to the needs of the users, facilitating real-time decoding of multiple spared distinctive degrees of freedom. In our study, we successfully decoded the motor intent from two participants with SCI, two with spinal stroke , and three amputees in real-time, achieving several controllable degrees of freedom within minutes after wearing the EMG bracelet. We provide a proof-of-concept that these decoded signals can be used to control a digitally rendered hand, a wearable orthosis, a prosthesis, or a 2D cursor. Our framework promotes a participant-centered approach, allowing immediate feedback integration, thus enhancing the iterative development of myocontrol algorithms. The proposed open-source software framework, MyoGestic, allows researchers and patients to focus on the augmentation and training of the spared degrees of freedom after neural lesions, thus potentially bridging the gap between research and clinical application and advancing the development of intuitive EMG interfaces for diverse neural lesions. }
}

@article{240815272v1,
  title={ Estimating ECG Intervals from Lead-I Alone: External Validation of   Supervised Models },
  author={ Ridwan Alam and Collin Stultz },
  journal={ arXiv preprint arXiv:2408.15272v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.15272v1 },
  abstract={ The diagnosis, prognosis, and treatment of a number of cardiovascular disorders rely on ECG interval measurements, including the PR, QRS, and QT intervals. These quantities are measured from the 12-lead ECG, either manually or using automated algorithms, which are readily available in clinical settings. A number of wearable devices, however, can acquire the lead-I ECG in an outpatient setting, thereby raising the potential for out-of-hospital monitoring for disorders that involve clinically significant changes in ECG intervals. In this work, we therefore developed a series of deep learning models for estimating the PR, QRS, and QT intervals using lead-I ECG. From a corpus of 4.2 million ECGs from patients at the Massachusetts General Hospital, we train and validate each of the models. At internal holdout validation, we achieve mean absolute errors (MAE) of 6.3 ms for QRS durations and 11.9 ms for QT intervals, and an MAE of 9.2 ms for estimating PR intervals. Moreover, as a well-defined P-wave does not always exist in ECG tracings - for example, when there is atrial fibrillation - we trained a model that can identify when there is a P-wave, and consequently, a measurable PR interval. We validate our models on three large external healthcare datasets without any finetuning or retraining - 3.2 million ECG from the Brigham and Womens Hospital, 668 thousand from MIMIC-IV, and 20 thousand from PTB-XL - and achieve similar performance. Also, our models significantly outperform two publicly available baseline algorithms. This work demonstrates that ECG intervals can be tracked from only lead-I ECG using deep learning, and highlights the potential for out-of-hospital applications. }
}

@article{240705870v1,
  title={ Cervical Auscultation Machine Learning for Dysphagia Assessment },
  author={ An An Chia and Stacy Lum and Michelle Boo and Rex Tan and Balamurali B T and Jer-Ming Chen },
  journal={ arXiv preprint arXiv:2407.05870v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2407.05870v1 },
  abstract={ This study evaluates the use of machine learning, specifically the Random Forest Classifier, to differentiate normal and pathological swallowing sounds. Employing a commercially available wearable stethoscope, we recorded swallows from both healthy adults and patients with dysphagia. The analysis revealed statistically significant differences in acoustic features, such as spectral crest, and zero-crossing rate between normal and pathological swallows, while no discriminating differences were demonstrated between different fluidand diet consistencies. The system demonstrated fair sensitivity (mean plus or minus SD: 74\% plus or minus 8\%) and specificity (89\% plus or minus 6\%) for dysphagic swallows. The model attained an overall accuracy of 83\% plus or minus 3\%, and F1 score of 78\% plus or minus 5\%. These results demonstrate that machine learning can be a valuable tool in non-invasive dysphagia assessment, although challenges such as sampling rate limitations and variability in sensitivity and specificity in discriminating between normal and pathological sounds are noted. The study underscores the need for further research to optimize these techniques for clinical use. }
}

@article{240613136v1,
  title={ GVT2RPM: An Empirical Study for General Video Transformer Adaptation to   Remote Physiological Measurement },
  author={ Hao Wang and Euijoon Ahn and Jinman Kim },
  journal={ arXiv preprint arXiv:2406.13136v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2406.13136v1 },
  abstract={ Remote physiological measurement (RPM) is an essential tool for healthcare monitoring as it enables the measurement of physiological signs, e.g., heart rate, in a remote setting via physical wearables. Recently, with facial videos, we have seen rapid advancements in video-based RPMs. However, adopting facial videos for RPM in the clinical setting largely depends on the accuracy and robustness (work across patient populations). Fortunately, the capability of the state-of-the-art transformer architecture in general (natural) video understanding has resulted in marked improvements and has been translated to facial understanding, including RPM. However, existing RPM methods usually need RPM-specific modules, e.g., temporal difference convolution and handcrafted feature maps. Although these customized modules can increase accuracy, they are not demonstrated for their robustness across datasets. Further, due to their customization of the transformer architecture, they cannot use the advancements made in general video transformers (GVT). In this study, we interrogate the GVT architecture and empirically analyze how the training designs, i.e., data pre-processing and network configurations, affect the model performance applied to RPM. Based on the structure of video transformers, we propose to configure its spatiotemporal hierarchy to align with the dense temporal information needed in RPM for signal feature extraction. We define several practical guidelines and gradually adapt GVTs for RPM without introducing RPM-specific modules. Our experiments demonstrate favorable results to existing RPM-specific module counterparts. We conducted extensive experiments with five datasets using intra-dataset and cross-dataset settings. We highlight that the proposed guidelines GVT2RPM can be generalized to any video transformers and is robust to various datasets. }
}

@article{240519277v3,
  title={ Deep Latent Variable Modeling of Physiological Signals },
  author={ Khuong Vo },
  journal={ arXiv preprint arXiv:2405.19277v3 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2405.19277v3 },
  abstract={ A deep latent variable model is a powerful method for capturing complex distributions. These models assume that underlying structures, but unobserved, are present within the data. In this dissertation, we explore high-dimensional problems related to physiological monitoring using latent variable models. First, we present a novel deep state-space model to generate electrical waveforms of the heart using optically obtained signals as inputs. This can bring about clinical diagnoses of heart disease via simple assessment through wearable devices. Second, we present a brain signal modeling scheme that combines the strengths of probabilistic graphical models and deep adversarial learning. The structured representations can provide interpretability and encode inductive biases to reduce the data complexity of neural oscillations. The efficacy of the learned representations is further studied in epilepsy seizure detection formulated as an unsupervised learning problem. Third, we propose a framework for the joint modeling of physiological measures and behavior. Existing methods to combine multiple sources of brain data provided are limited. Direct analysis of the relationship between different types of physiological measures usually does not involve behavioral data. Our method can identify the unique and shared contributions of brain regions to behavior and can be used to discover new functions of brain regions. The success of these innovative computational methods would allow the translation of biomarker findings across species and provide insight into neurocognitive analysis in numerous biological studies and clinical diagnoses, as well as emerging consumer applications. }
}

@article{240513023v1,
  title={ A Machine Learning Approach for Predicting Upper Limb Motion Intentions   with Multimodal Data in Virtual Reality },
  author={ Pavan Uttej Ravva and Pinar Kullu and Mohammad Fahim Abrar and Roghayeh Leila Barmaki },
  journal={ arXiv preprint arXiv:2405.13023v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2405.13023v1 },
  abstract={ Over the last decade, there has been significant progress in the field of interactive virtual rehabilitation. Physical therapy (PT) stands as a highly effective approach for enhancing physical impairments. However, patient motivation and progress tracking in rehabilitation outcomes remain a challenge. This work addresses the gap through a machine learning-based approach to objectively measure outcomes of the upper limb virtual therapy system in a user study with non-clinical participants. In this study, we use virtual reality to perform several tracing tasks while collecting motion and movement data using a KinArm robot and a custom-made wearable sleeve sensor. We introduce a two-step machine learning architecture to predict the motion intention of participants. The first step predicts reaching task segments to which the participant-marked points belonged using gaze, while the second step employs a Long Short-Term Memory (LSTM) model to predict directional movements based on resistance change values from the wearable sensor and the KinArm. We specifically propose to transpose our raw resistance data to the time-domain which significantly improves the accuracy of the models by 34.6\%. To evaluate the effectiveness of our model, we compared different classification techniques with various data configurations. The results show that our proposed computational method is exceptional at predicting participant's actions with accuracy values of 96.72\% for diamond reaching task, and 97.44\% for circle reaching task, which demonstrates the great promise of using multimodal data, including eye-tracking and resistance change, to objectively measure the performance and intention in virtual rehabilitation settings. }
}

@article{240504707v1,
  title={ A wearable anti-gravity supplement to therapy does not improve arm   function in chronic stroke: a randomized trial },
  author={ Courtney Celian and Partha Ryali and Valentino Wilson and Adith Srivatsaa and James L. Patton },
  journal={ arXiv preprint arXiv:2405.04707v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2405.04707v1 },
  abstract={ Background: Gravity confounds arm movement ability in post-stroke hemiparesis. Reducing its influence allows effective practice leading to recovery. Yet, there is a scarcity of wearable devices suitable for personalized use across diverse therapeutic activities in the clinic. Objective: In this study, we investigated the safety, feasibility, and efficacy of anti-gravity therapy using the ExoNET device in post-stroke participants. Methods: Twenty chronic stroke survivors underwent six, 45-minute occupational therapy sessions while wearing the ExoNET, randomized into either the treatment (ExoNET tuned to gravity-support) or control group (ExoNET tuned to slack condition). Clinical outcomes were evaluated by a blinded-rater at baseline, post, and six-week follow-up sessions. Kinetic, kinematic, and patient experience outcomes were also assessed. Results: Mixed-effect models showed a significant improvement in Box and Blocks scores in the post-intervention session for the treatment group (effect size: 2.1, p = .04). No significant effects were found between the treatment and control groups for ARAT scores and other clinical metrics. Direct kinetic effects revealed a significant reduction in muscle activity during free exploration with an effect size of (-7.12\%, p< 005). There were no significant longitudinal kinetic or kinematic trends. Subject feedback suggested a generally positive perception of the anti-gravity therapy. Conclusions: Anti-gravity therapy with the ExoNET is a safe and feasible treatment for post-stroke rehabilitation. The device provided anti-gravity forces, did not encumber range of motion, and clinical metrics of anti-gravity therapy demonstrated improvements in gross manual dexterity. Further research is required to explore potential benefits in broader clinical metrics. }
}

@article{240502485v1,
  title={ A Survey of Few-Shot Learning for Biomedical Time Series },
  author={ Chenqi Li and Timothy Denison and Tingting Zhu },
  journal={ arXiv preprint arXiv:2405.02485v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2405.02485v1 },
  abstract={ Advancements in wearable sensor technologies and the digitization of medical records have contributed to the unprecedented ubiquity of biomedical time series data. Data-driven models have tremendous potential to assist clinical diagnosis and improve patient care by improving long-term monitoring capabilities, facilitating early disease detection and intervention, as well as promoting personalized healthcare delivery. However, accessing extensively labeled datasets to train data-hungry deep learning models encounters many barriers, such as long-tail distribution of rare diseases, cost of annotation, privacy and security concerns, data-sharing regulations, and ethical considerations. An emerging approach to overcome the scarcity of labeled data is to augment AI methods with human-like capabilities to leverage past experiences to learn new tasks with limited examples, called few-shot learning. This survey provides a comprehensive review and comparison of few-shot learning methods for biomedical time series applications. The clinical benefits and limitations of such methods are discussed in relation to traditional data-driven approaches. This paper aims to provide insights into the current landscape of few-shot learning for biomedical time series and its implications for future research and applications. }
}

@article{240209474v2,
  title={ Deciphering Heartbeat Signatures: A Vision Transformer Approach to   Explainable Atrial Fibrillation Detection from ECG Signals },
  author={ Aruna Mohan and Danne Elbers and Or Zilbershot and Fatemeh Afghah and David Vorchheimer },
  journal={ arXiv preprint arXiv:2402.09474v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2402.09474v2 },
  abstract={ Remote patient monitoring based on wearable single-lead electrocardiogram (ECG) devices has significant potential for enabling the early detection of heart disease, especially in combination with artificial intelligence (AI) approaches for automated heart disease detection. There have been prior studies applying AI approaches based on deep learning for heart disease detection. However, these models are yet to be widely accepted as a reliable aid for clinical diagnostics, in part due to the current black-box perception surrounding many AI algorithms. In particular, there is a need to identify the key features of the ECG signal that contribute toward making an accurate diagnosis, thereby enhancing the interpretability of the model. In the present study, we develop a vision transformer approach to identify atrial fibrillation based on single-lead ECG data. A residual network (ResNet) approach is also developed for comparison with the vision transformer approach. These models are applied to the Chapman-Shaoxing dataset to classify atrial fibrillation, as well as another common arrhythmia, sinus bradycardia, and normal sinus rhythm heartbeats. The models enable the identification of the key regions of the heartbeat that determine the resulting classification, and highlight the importance of P-waves and T-waves, as well as heartbeat duration and signal amplitude, in distinguishing normal sinus rhythm from atrial fibrillation and sinus bradycardia. }
}

@article{231111055v2,
  title={ Designing Interpretable ML System to Enhance Trust in Healthcare: A   Systematic Review to Proposed Responsible Clinician-AI-Collaboration   Framework },
  author={ Elham Nasarian and Roohallah Alizadehsani and U. Rajendra Acharya and Kwok-Leung Tsui },
  journal={ arXiv preprint arXiv:2311.11055v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2311.11055v2 },
  abstract={ This paper explores the significant impact of AI-based medical devices, including wearables, telemedicine, large language models, and digital twins, on clinical decision support systems. It emphasizes the importance of producing outcomes that are not only accurate but also interpretable and understandable to clinicians, addressing the risk that lack of interpretability poses in terms of mistrust and reluctance to adopt these technologies in healthcare. The paper reviews interpretable AI processes, methods, applications, and the challenges of implementation in healthcare, focusing on quality control to facilitate responsible communication between AI systems and clinicians. It breaks down the interpretability process into data pre-processing, model selection, and post-processing, aiming to foster a comprehensive understanding of the crucial role of a robust interpretability approach in healthcare and to guide future research in this area. with insights for creating responsible clinician-AI tools for healthcare, as well as to offer a deeper understanding of the challenges they might face. Our research questions, eligibility criteria and primary goals were identified using Preferred Reporting Items for Systematic reviews and Meta-Analyses guideline and PICO method; PubMed, Scopus and Web of Science databases were systematically searched using sensitive and specific search strings. In the end, 52 publications were selected for data extraction which included 8 existing reviews and 44 related experimental studies. The paper offers general concepts of interpretable AI in healthcare and discuss three-levels interpretability process. Additionally, it provides a comprehensive discussion of evaluating robust interpretability AI in healthcare. Moreover, this survey introduces a step-by-step roadmap for implementing responsible AI in healthcare. }
}

@article{231009203v2,
  title={ SiamAF: Learning Shared Information from ECG and PPG Signals for Robust   Atrial Fibrillation Detection },
  author={ Zhicheng Guo and Cheng Ding and Duc H. Do and Amit Shah and Randall J. Lee and Xiao Hu and Cynthia Rudin },
  journal={ arXiv preprint arXiv:2310.09203v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2310.09203v2 },
  abstract={ Atrial fibrillation (AF) is the most common type of cardiac arrhythmia. It is associated with an increased risk of stroke, heart failure, and other cardiovascular complications, but can be clinically silent. Passive AF monitoring with wearables may help reduce adverse clinical outcomes related to AF. Detecting AF in noisy wearable data poses a significant challenge, leading to the emergence of various deep learning techniques. Previous deep learning models learn from a single modality, either electrocardiogram (ECG) or photoplethysmography (PPG) signals. However, deep learning models often struggle to learn generalizable features and rely on features that are more susceptible to corruption from noise, leading to sub-optimal performances in certain scenarios, especially with low-quality signals. Given the increasing availability of ECG and PPG signal pairs from wearables and bedside monitors, we propose a new approach, SiamAF, leveraging a novel Siamese network architecture and joint learning loss function to learn shared information from both ECG and PPG signals. At inference time, the proposed model is able to predict AF from either PPG or ECG and outperforms baseline methods on three external test sets. It learns medically relevant features as a result of our novel architecture design. The proposed model also achieves comparable performance to traditional learning regimes while requiring much fewer training labels, providing a potential approach to reduce future reliance on manual labeling. }
}

@article{230704866v2,
  title={ Gait Event Detection and Travel Distance Using Waist-Worn Accelerometers   across a Range of Speeds: Automated Approach },
  author={ Albara Ah Ramli and Xin Liu and Kelly Berndt and Chen-Nee Chuah and Erica Goude and Lynea B. Kaethler and Amanda Lopez and Alina Nicorici and Corey Owens and David Rodriguez and Jane Wang and Daniel Aranki and Craig M. McDonald and Erik K. Henricson },
  journal={ arXiv preprint arXiv:2307.04866v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2307.04866v2 },
  abstract={ Estimation of temporospatial clinical features of gait (CFs), such as step count and length, step duration, step frequency, gait speed, and distance traveled, is an important component of community-based mobility evaluation using wearable accelerometers. However, accurate unsupervised computerized measurement of CFs of individuals with Duchenne muscular dystrophy (DMD) who have progressive loss of ambulatory mobility is difficult due to differences in patterns and magnitudes of acceleration across their range of attainable gait velocities. This paper proposes a novel calibration method. It aims to detect steps, estimate stride lengths, and determine travel distance. The approach involves a combination of clinical observation, machine-learning-based step detection, and regression-based stride length prediction. The method demonstrates high accuracy in children with DMD and typically developing controls (TDs) regardless of the participant's level of ability. Fifteen children with DMD and fifteen TDs underwent supervised clinical testing across a range of gait speeds using 10 m or 25 m run/walk (10 MRW, 25 MRW), 100 m run/walk (100 MRW), 6-min walk (6 MWT), and free-walk (FW) evaluations while wearing a mobile-phone-based accelerometer at the waist near the body's center of mass. Following calibration by a trained clinical evaluator, CFs were extracted from the accelerometer data using a multi-step machine-learning-based process and the results were compared to ground-truth observation data. Model predictions vs. observed values for step counts, distance traveled, and step length showed a strong correlation. Our study findings indicate that a single waist-worn accelerometer calibrated to an individual's stride characteristics using our methods accurately measures CFs and estimates travel distances across a common range of gait speeds in both DMD and TD peers. }
}

@article{230111399v3,
  title={ Distributional outcome regression via quantile functions and its   application to modelling continuously monitored heart rate and physical   activity },
  author={ Rahul Ghosal and Sujit K. Ghosh and Jennifer A. Schrack and Vadim Zipunnikov },
  journal={ arXiv preprint arXiv:2301.11399v3 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2301.11399v3 },
  abstract={ Modern clinical and epidemiological studies widely employ wearables to record parallel streams of real-time data on human physiology and behavior. With recent advances in distributional data analysis, these high-frequency data are now often treated as distributional observations resulting in novel regression settings. Motivated by these modelling setups, we develop a distributional outcome regression via quantile functions (DORQF) that expands existing literature with three key contributions: i) handling both scalar and distributional predictors, ii) ensuring jointly monotone regression structure without enforcing monotonicity on individual functional regression coefficients, iii) providing statistical inference via asymptotic projection-based joint confidence bands and a statistical test of global significance to quantify uncertainty of the estimated functional regression coefficients. The method is motivated by and applied to Actiheart component of Baltimore Longitudinal Study of Aging that collected one week of minute-level heart rate (HR) and physical activity (PA) data on 781 older adults to gain deeper understanding of age-related changes in daily life heart rate reserve, defined as a distribution of daily HR, while accounting for daily distribution of physical activity, age, gender, and body composition. Intriguingly, the results provide novel insights in epidemiology of daily life heart rate reserve. }
}

@article{240105378v1,
  title={ Detecting QT prolongation From a Single-lead ECG With Deep Learning },
  author={ Ridwan Alam and Aaron Aguirre and Collin Stultz },
  journal={ arXiv preprint arXiv:2401.05378v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2401.05378v1 },
  abstract={ For a number of antiarrhythmics, drug loading requires a 3 day hospitalization with monitoring for QT prolongation. Automated QT monitoring with wearable ECG monitors would facilitate out-of-hospital care. We develop a deep learning model that infers QT intervals from ECG lead-I - the lead most often acquired from ambulatory ECG monitors - and to use this model to detect clinically meaningful QT-prolongation episodes during Dofetilide drug loading. Using 4.22 million 12-lead ECG recordings from 903.6 thousand patients at the Massachusetts General Hospital, we develop a deep learning model, QTNet, that infers QT intervals from lead-I. Over 3 million ECGs from 653 thousand patients are used to train the model and an internal-test set containing 633 thousand ECGs from 135 thousand patients was used for testing. QTNet is further evaluated on an external-validation set containing 3.1 million ECGs from 667 thousand patients at another institution. QTNet was used to detect Dofetilide-induced QT prolongation in a publicly available database (ECGRDVQ-dataset) containing ECGs from subjects enrolled in a clinical trial evaluating the effects of antiarrhythmic drugs. QTNet achieves mean absolute errors of 12.63ms (internal-test) and 12.30ms (external-validation) for estimating absolute QT intervals. The associated Pearson correlation coefficients are 0.91 (internal-test) and 0.92 (external-validation). For the ECGRDVQ-dataset, QTNet detects Dofetilide-induced QTc prolongation with 87\% sensitivity and 77\% specificity. The negative predictive value of the model is greater than 95\% when the pre-test probability of drug-induced QTc prolongation is below 25\%. Drug-induced QT prolongation risk can be tracked from ECG lead-I using deep learning. }
}

@article{231209517v1,
  title={ A wearable Gait Assessment Method for Lumbar Disc Herniation Based on   Adaptive Kalman Filtering },
  author={ Yongsong Wang and Zhixin Li and Zhaohui Guo and Yin Ding and Zhan Huan and Lin Chen },
  journal={ arXiv preprint arXiv:2312.09517v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2312.09517v1 },
  abstract={ Lumbar disc herniation (LDH) is a prevalent orthopedic condition in clinical practice. Inertial measurement unit sensors (IMUs) are an effective tool for monitoring and assessing gait impairment in patients with lumbar disc herniation (LDH). However, the current gait assessment of LDH focuses solely on single-source acceleration signal data, without considering the diversity of sensor data. It also overlooks the individual differences in motor function deterioration between the healthy and affected lower limbs in patients with LDH. To address this issue, we developed an LDH gait feature model that relies on multi-source adaptive Kalman data fusion of acceleration and angular velocity. We utilized an adaptive Kalman data fusion algorithm for acceleration and angular velocity to estimate the attitude angle and segment the gait phase. Two Inertial Measurement Units (IMUs) were used to analyze the gait characteristics of patients with lumbar disc issues and healthy individuals. This analysis included 12 gait characteristics, such as gait spatiotemporal parameters, kinematic parameters, and expansibility index numbers. Statistical methods were employed to analyze the characteristic model and confirm the biological differences between the healthy affected side of LDH and healthy subjects. Finally, a classifier based on feature engineering was utilized to classify the gait patterns of the affected side of patients with lumbar disc disease and healthy subjects. This approach achieved a classification accuracy of 95.50\%, enhancing the recognition of LDH and healthy gait patterns. It also provided effective gait feature sets and methods for assessing LDH clinically. }
}

@article{231112781v1,
  title={ Quantifying Impairment and Disease Severity Using AI Models Trained on   Healthy Subjects },
  author={ Boyang Yu and Aakash Kaku and Kangning Liu and Avinash Parnandi and Emily Fokas and Anita Venkatesan and Natasha Pandit and Rajesh Ranganath and Heidi Schambra and Carlos Fernandez-Granda },
  journal={ arXiv preprint arXiv:2311.12781v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2311.12781v1 },
  abstract={ Automatic assessment of impairment and disease severity is a key challenge in data-driven medicine. We propose a novel framework to address this challenge, which leverages AI models trained exclusively on healthy individuals. The COnfidence-Based chaRacterization of Anomalies (COBRA) score exploits the decrease in confidence of these models when presented with impaired or diseased patients to quantify their deviation from the healthy population. We applied the COBRA score to address a key limitation of current clinical evaluation of upper-body impairment in stroke patients. The gold-standard Fugl-Meyer Assessment (FMA) requires in-person administration by a trained assessor for 30-45 minutes, which restricts monitoring frequency and precludes physicians from adapting rehabilitation protocols to the progress of each patient. The COBRA score, computed automatically in under one minute, is shown to be strongly correlated with the FMA on an independent test cohort for two different data modalities: wearable sensors (\$\\textbackslash{}rho = 0.845\$, 95\% CI [0.743,0.908]) and video (\$\\textbackslash{}rho = 0.746\$, 95\% C.I [0.594, 0.847]). To demonstrate the generalizability of the approach to other conditions, the COBRA score was also applied to quantify severity of knee osteoarthritis from magnetic-resonance imaging scans, again achieving significant correlation with an independent clinical assessment (\$\\textbackslash{}rho = 0.644\$, 95\% C.I [0.585,0.696]). }
}

@article{221012807v3,
  title={ HKF: Hierarchical Kalman Filtering with Online Learned Evolution Priors   for Adaptive ECG Denoising },
  author={ Guy Revach and Timur Locher and Nir Shlezinger and Ruud J. G. van Sloun and Rik Vullings },
  journal={ arXiv preprint arXiv:2210.12807v3 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2210.12807v3 },
  abstract={ Electrocardiography (ECG) signals play a pivotal role in many healthcare applications, especially in at-home monitoring of vital signs. Wearable technologies, which these applications often depend upon, frequently produce low-quality ECG signals. While several methods exist for ECG denoising to enhance signal quality and aid clinical interpretation, they often underperform with ECG data from wearable technology due to limited noise tolerance or inadequate flexibility in capturing ECG dynamics. This paper introduces HKF, a hierarchical and adaptive Kalman filter, which uses a proprietary state space model to effectively capture both intra- and inter-heartbeat dynamics for ECG signal denoising. HKF learns a patient-specific structured prior for the ECG signal's intra-heartbeat dynamics in an online manner, resulting in a filter that adapts to the specific ECG signal characteristics of each patient. In an empirical study, HKF demonstrated superior denoising performance (reduced mean-squared error) while preserving the unique properties of the waveform. In a comparative analysis, HKF outperformed previously proposed methods for ECG denoising, such as the model-based Kalman filter and data-driven autoencoders. This makes it a suitable candidate for applications in extramural healthcare settings. }
}

@article{230617188v1,
  title={ Decentralized Healthcare Systems with Federated Learning and Blockchain },
  author={ Abdulrezzak Zekiye and Öznur Özkasap },
  journal={ arXiv preprint arXiv:2306.17188v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2306.17188v1 },
  abstract={ Artificial intelligence (AI) and deep learning techniques have gained significant attraction in recent years, owing to their remarkable capability of achieving high performance across a broad range of applications. However, a crucial challenge in training such models is the acquisition of vast amounts of data, which is often limited in fields like healthcare. In this domain, medical data is typically scattered across various sources such as hospitals, clinics, and wearable devices. The aggregated data collected from multiple sources in the healthcare domain is sufficient for training advanced deep learning models. However, these sources are frequently hesitant to share such data due to privacy considerations. To address this challenge, researchers have proposed the integration of blockchain and federated learning to develop a system that facilitates the secure sharing of medical records. This work provides a succinct review of the current state of the art in the use of blockchain and federated learning in the decentralized healthcare domain. }
}

@article{230517629v1,
  title={ Multi-Modal Wireless Flexible Gel-Free Sensors with Edge Deep Learning   for Detecting and Alerting Freezing of Gait in Parkinson's Patients },
  author={ Yuhan Hou and Jack Ji and Yi Zhu and Thomas Dell and Xilin Liu },
  journal={ arXiv preprint arXiv:2305.17629v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2305.17629v1 },
  abstract={ Freezing of gait (FoG) is a debilitating symptom of Parkinson's disease (PD). This work develops flexible wearable sensors that can detect FoG and alert patients and companions to help prevent falls. FoG is detected on the sensors using a deep learning (DL) model with multi-modal sensory inputs collected from distributed wireless sensors. Two types of wireless sensors are developed, including: (1) a C-shape central node placed around the patient's ears, which collects electroencephalogram (EEG), detects FoG using an on-device DL model, and generates auditory alerts when FoG is detected; (2) a stretchable patch-type sensor attached to the patient's legs, which collects electromyography (EMG) and movement information from accelerometers. The patch-type sensors wirelessly send collected data to the central node through low-power ultra-wideband (UWB) transceivers. All sensors are fabricated on flexible printed circuit boards. Adhesive gel-free acetylene carbon black and polydimethylsiloxane electrodes are fabricated on the flexible substrate to allow conformal wear over the long term. Custom integrated circuits (IC) are developed in 180 nm CMOS technology and used in both types of sensors for signal acquisition, digitization, and wireless communication. A novel lightweight DL model is trained using multi-modal sensory data. The inference of the DL model is performed on a low-power microcontroller in the central node. The DL model achieves a high detection sensitivity of 0.81 and a specificity of 0.88. The developed wearable sensors are ready for clinical experiments and hold great promise in improving the quality of life of patients with PD. The proposed design methodologies can be used in wearable medical devices for the monitoring and treatment of a wide range of neurodegenerative diseases. }
}

@article{230509366v1,
  title={ Evaluation of self-supervised pre-training for automatic infant movement   classification using wearable movement sensors },
  author={ Einari Vaaras and Manu Airaksinen and Sampsa Vanhatalo and Okko Räsänen },
  journal={ arXiv preprint arXiv:2305.09366v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2305.09366v1 },
  abstract={ The recently-developed infant wearable MAIJU provides a means to automatically evaluate infants' motor performance in an objective and scalable manner in out-of-hospital settings. This information could be used for developmental research and to support clinical decision-making, such as detection of developmental problems and guiding of their therapeutic interventions. MAIJU-based analyses rely fully on the classification of infant's posture and movement; it is hence essential to study ways to increase the accuracy of such classifications, aiming to increase the reliability and robustness of the automated analysis. Here, we investigated how self-supervised pre-training improves performance of the classifiers used for analyzing MAIJU recordings, and we studied whether performance of the classifier models is affected by context-selective quality-screening of pre-training data to exclude periods of little infant movement or with missing sensors. Our experiments show that i) pre-training the classifier with unlabeled data leads to a robust accuracy increase of subsequent classification models, and ii) selecting context-relevant pre-training data leads to substantial further improvements in the classifier performance. }
}

@article{230414912v1,
  title={ Human Activity Recognition Using Self-Supervised Representations of   Wearable Data },
  author={ Maximilien Burq and Niranjan Sridhar },
  journal={ arXiv preprint arXiv:2304.14912v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2304.14912v1 },
  abstract={ Automated and accurate human activity recognition (HAR) using body-worn sensors enables practical and cost efficient remote monitoring of Activity of DailyLiving (ADL), which are shown to provide clinical insights across multiple therapeutic areas. Development of accurate algorithms for human activity recognition(HAR) is hindered by the lack of large real-world labeled datasets. Furthermore, algorithms seldom work beyond the specific sensor on which they are prototyped, prompting debate about whether accelerometer-based HAR is even possible [Tong et al., 2020]. Here we develop a 6-class HAR model with strong performance when evaluated on real-world datasets not seen during training. Our model is based on a frozen self-supervised representation learned on a large unlabeled dataset, combined with a shallow multi-layer perceptron with temporal smoothing. The model obtains in-dataset state-of-the art performance on the Capture24 dataset (\$\\textbackslash{}kappa= 0.86\$). Out-of-distribution (OOD) performance is \$\\textbackslash{}kappa = 0.7\$, with both the representation and the perceptron models being trained on data from a different sensor. This work represents a key step towards device-agnostic HAR models, which can help contribute to increased standardization of model evaluation in the HAR field. }
}

@article{230203731v2,
  title={ MMA-RNN: A Multi-level Multi-task Attention-based Recurrent Neural   Network for Discrimination and Localization of Atrial Fibrillation },
  author={ Yifan Sun and Jingyan Shen and Yunfan Jiang and Zhaohui Huang and Minsheng Hao and Xuegong Zhang },
  journal={ arXiv preprint arXiv:2302.03731v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2302.03731v2 },
  abstract={ The automatic detection of atrial fibrillation based on electrocardiograph (ECG) signals has received wide attention both clinically and practically. It is challenging to process ECG signals with cyclical pattern, varying length and unstable quality due to noise and distortion. Besides, there has been insufficient research on separating persistent atrial fibrillation from paroxysmal atrial fibrillation, and little discussion on locating the onsets and end points of AF episodes. It is even more arduous to perform well on these two distinct but interrelated tasks, while avoiding the mistakes inherent from stage-by-stage approaches. This paper proposes the Multi-level Multi-task Attention-based Recurrent Neural Network for three-class discrimination on patients and localization of the exact timing of AF episodes. Our model captures three-level sequential features based on a hierarchical architecture utilizing Bidirectional Long and Short-Term Memory Network (Bi-LSTM) and attention layers, and accomplishes the two tasks simultaneously with a multi-head classifier. The model is designed as an end-to-end framework to enhance information interaction and reduce error accumulation. Finally, we conduct experiments on CPSC 2021 dataset and the result demonstrates the superior performance of our method, indicating the potential application of MMA-RNN to wearable mobile devices for routine AF monitoring and early diagnosis. }
}

@article{220411795v3,
  title={ Performer: A Novel PPG-to-ECG Reconstruction Transformer for a Digital   Biomarker of Cardiovascular Disease Detection },
  author={ Ella Lan },
  journal={ arXiv preprint arXiv:2204.11795v3 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2204.11795v3 },
  abstract={ Electrocardiography (ECG), an electrical measurement which captures cardiac activities, is the gold standard for diagnosing cardiovascular disease (CVD). However, ECG is infeasible for continuous cardiac monitoring due to its requirement for user participation. By contrast, photoplethysmography (PPG) provides easy-to-collect data, but its limited accuracy constrains its clinical usage. To combine the advantages of both signals, recent studies incorporate various deep learning techniques for the reconstruction of PPG signals to ECG; however, the lack of contextual information as well as the limited abilities to denoise biomedical signals ultimately constrain model performance. In this research, we propose Performer, a novel Transformer-based architecture that reconstructs ECG from PPG and combines the PPG and reconstructed ECG as multiple modalities for CVD detection. This method is the first time that Transformer sequence-to-sequence translation has been performed on biomedical waveform reconstruction, combining the advantages of both PPG and ECG. We also create Shifted Patch-based Attention (SPA), an effective method to encode/decode the biomedical waveforms. Through fetching the various sequence lengths and capturing cross-patch connections, SPA maximizes the signal processing for both local features and global contextual representations. The proposed architecture generates a state-of-the-art performance of 0.29 RMSE for the reconstruction of PPG to ECG on the BIDMC database, surpassing prior studies. We also evaluated this model on the MIMIC-III dataset, achieving a 95.9\% accuracy in CVD detection, and on the PPG-BP dataset, achieving 75.9\% accuracy in related CVD diabetes detection, indicating its generalizability. As a proof of concept, an earring wearable named PEARL (prototype), was designed to scale up the point-of-care (POC) healthcare system. }
}

@article{221112778v1,
  title={ Monitoring and Improving Personalized Sleep Quality from Long-Term   Lifelogs },
  author={ Wenbin Gan and Minh-Son Dao and Koji Zettsu },
  journal={ arXiv preprint arXiv:2211.12778v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2211.12778v1 },
  abstract={ Sleep plays a vital role in our physical, cognitive, and psychological well-being. Despite its importance, long-term monitoring of personalized sleep quality (SQ) in real-world contexts is still challenging. Many sleep researches are still developing clinically and far from accessible to the general public. Fortunately, wearables and IoT devices provide the potential to explore the sleep insights from multimodal data, and have been used in some SQ researches. However, most of these studies analyze the sleep related data and present the results in a delayed manner (i.e., today's SQ obtained from last night's data), it is sill difficult for individuals to know how their sleep will be before they go to bed and how they can proactively improve it. To this end, this paper proposes a computational framework to monitor the individual SQ based on both the objective and subjective data from multiple sources, and moves a step further towards providing the personalized feedback to improve the SQ in a data-driven manner. The feedback is implemented by referring the insights from the PMData dataset based on the discovered patterns between life events and different levels of SQ. The deep learning based personal SQ model (PerSQ), using the long-term heterogeneous data and considering the carry-over effect, achieves higher prediction performance compared with baseline models. A case study also shows reasonable results for an individual to monitor and improve the SQ in the future. }
}

@article{221103773v1,
  title={ Objective dyspnea evaluation on COVID-19 patients learning from   exertion-induced dyspnea scores },
  author={ Zijing Zhang and Jianlin Zhou and Thomas B. Conroy and Samuel Chung and Justin Choi and Patrick Chau and Daniel B. Green and Ana C. Krieger and Edwin C. Kan },
  journal={ arXiv preprint arXiv:2211.03773v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2211.03773v1 },
  abstract={ Objective: Dyspnea is one of the most common symptoms for many pulmonary diseases including COVID-19. Clinical assessment of dyspnea is mainly performed by subjective self-report, which has limited accuracy and is challenging for continuous monitoring. The objective of this research study is to determine if dyspnea progression in COVID patients can be assessed using a non-invasive wearable sensor and if the findings are comparable to a learning model of physiologically induced dyspnea on healthy subjects. Methods: Non-invasive wearable respiratory sensors were employed to retrieve continuous respiratory characteristics with user comfort and convenience. Overnight (\~{}16h) respiratory waveforms were collected on 12 COVID-19 patients, and a benchmark on 13 healthy subjects with exertion-induced dyspnea were also performed for blind comparison. The learning model was built from the respiratory features with self report on 32 healthy subjects under exertion and airway blockage. Results: High similarity between dyspnea on COVID patients and physiologically induced dyspnea on healthy subjects was established. COVID patients have consistently high objective dyspnea scores in comparison with normal breathing of healthy subjects. We also exhibited continuous dyspnea scoring capability for 12-16 hours on patients. Conclusion: This paper validates the viability to use our objective dyspnea scoring for clinical dyspnea assessment on COVID patients. Significance: The proposed system can help the identification of dyspneic exacerbation in conditions such as COVID, leading to early intervention and possibly improving their outcome. This approach can be potentially applied to other pulmonary disorders such as asthma, emphysema, and pneumonia. }
}

@article{221103767v1,
  title={ Novel Muscle Monitoring by Radiomyography(RMG) and Application to Hand   Gesture Recognition },
  author={ Zijing Zhang and Edwin C. Kan },
  journal={ arXiv preprint arXiv:2211.03767v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2211.03767v1 },
  abstract={ Conventional electromyography (EMG) measures the continuous neural activity during muscle contraction, but lacks explicit quantification of the actual contraction. Mechanomyography (MMG) and accelerometers only measure body surface motion, while ultrasound, CT-scan and MRI are restricted to in-clinic snapshots. Here we propose a novel radiomyography (RMG) for continuous muscle actuation sensing that can be wearable and touchless, capturing both superficial and deep muscle groups. We verified RMG experimentally by a forearm wearable sensor for detailed hand gesture recognition. We first converted the radio sensing outputs to the time-frequency spectrogram, and then employed the vision transformer (ViT) deep learning network as the classification model, which can recognize 23 gestures with an average accuracy up to 99\% on 8 subjects. By transfer learning, high adaptivity to user difference and sensor variation were achieved at an average accuracy up to 97\%. We further demonstrated RMG to monitor eye and leg muscles and achieved high accuracy for eye movement and body postures tracking. RMG can be used with synchronous EMG to derive stimulation-actuation waveforms for many future applications in kinesiology, physiotherapy, rehabilitation, and human-machine interface. }
}

@article{220807088v1,
  title={ Enhancing Deep Learning-based 3-lead ECG Classification with Heartbeat   Counting and Demographic Data Integration },
  author={ Khiem H. Le and Hieu H. Pham and Thao B. T. Nguyen and Tu A. Nguyen and Cuong D. Do },
  journal={ arXiv preprint arXiv:2208.07088v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2208.07088v1 },
  abstract={ Nowadays, an increasing number of people are being diagnosed with cardiovascular diseases (CVDs), the leading cause of death globally. The gold standard for identifying these heart problems is via electrocardiogram (ECG). The standard 12-lead ECG is widely used in clinical practice and the majority of current research. However, using a lower number of leads can make ECG more pervasive as it can be integrated with portable or wearable devices. This article introduces two novel techniques to improve the performance of the current deep learning system for 3-lead ECG classification, making it comparable with models that are trained using standard 12-lead ECG. Specifically, we propose a multi-task learning scheme in the form of the number of heartbeats regression and an effective mechanism to integrate patient demographic data into the system. With these two advancements, we got classification performance in terms of F1 scores of 0.9796 and 0.8140 on two large-scale ECG datasets, i.e., Chapman and CPSC-2018, respectively, which surpassed current state-of-the-art ECG classification methods, even those trained on 12-lead data. To encourage further development, our source code is publicly available at https://github.com/lhkhiem28/LightX3ECG. }
}

@article{211205564v2,
  title={ Identification of Hip and Knee Joint Impedance During the Swing Phase of   Walking },
  author={ Herman van der Kooij and Simone S. Fricke and Ronald C. van 't Veld and Ander Vallinas Prieto and Arvid Q. L. Keemink and Alfred C. Schouten and Edwin H. F. van Asseldonk },
  journal={ arXiv preprint arXiv:2112.05564v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2112.05564v2 },
  abstract={ Knowledge on joint impedance during walking in various conditions is relevant for clinical decision-making and the development of robotic gait trainers, leg prostheses, leg orthotics and wearable exoskeletons. Whereas ankle impedance during walking has been experimentally assessed, knee and hip joint impedance during walking have not been identified yet. Here we developed and evaluated a lower limb perturbator to identify hip, knee and ankle joint impedance during treadmill walking. The lower limb perturbator (LOPER) consists of an actuator connected to the thigh via rods. The LOPER allows to apply force perturbations to a free-hanging leg, while standing on the contralateral leg, with a bandwidth of up to 39 Hz. While walking in minimal impedance mode, the interaction forces between LOPER and the thigh were low (<5 N) and the effect on the walking pattern was smaller than the within-subject variability during normal walking. Using a non-linear multibody dynamical model of swing leg dynamics, the hip, knee and ankle joint impedance were estimated at three time points during the swing phase for nine subjects walking at a speed of 0.5 m/s. The identified model was well able to predict the experimental responses for the hip and knee, since the mean variance accounted (VAF) for was 99\% and 96\%, respectively. The ankle lacked a consistent response and the mean VAF of the model fit was only 77\%, and therefore the estimated ankle impedance was not reliable. The averaged across-subjects stiffness varied between the three time points within 34-66 and 0-3.5 Nm/rad for the hip and knee joint respectively. The damping varied between 1.9-4.6 and 0.02-0.14 Nms/rad for hip and knee respectively. The developed LOPER has a negligible effect on the unperturbed walking pattern and allows to identify hip and knee impedance during the swing phase. }
}

@article{220200589v1,
  title={ Blind ECG Restoration by Operational Cycle-GANs },
  author={ Serkan Kiranyaz and Ozer Can Devecioglu and Turker Ince and Junaid Malik and Muhammad Chowdhury and Tahir Hamid and Rashid Mazhar and Amith Khandakar and Anas Tahir and Tawsifur Rahman and Moncef Gabbouj },
  journal={ arXiv preprint arXiv:2202.00589v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2202.00589v1 },
  abstract={ Continuous long-term monitoring of electrocardiography (ECG) signals is crucial for the early detection of cardiac abnormalities such as arrhythmia. Non-clinical ECG recordings acquired by Holter and wearable ECG sensors often suffer from severe artifacts such as baseline wander, signal cuts, motion artifacts, variations on QRS amplitude, noise, and other interferences. Usually, a set of such artifacts occur on the same ECG signal with varying severity and duration, and this makes an accurate diagnosis by machines or medical doctors extremely difficult. Despite numerous studies that have attempted ECG denoising, they naturally fail to restore the actual ECG signal corrupted with such artifacts due to their simple and naive noise model. In this study, we propose a novel approach for blind ECG restoration using cycle-consistent generative adversarial networks (Cycle-GANs) where the quality of the signal can be improved to a clinical level ECG regardless of the type and severity of the artifacts corrupting the signal. To further boost the restoration performance, we propose 1D operational Cycle-GANs with the generative neuron model. The proposed approach has been evaluated extensively using one of the largest benchmark ECG datasets from the China Physiological Signal Challenge (CPSC-2020) with more than one million beats. Besides the quantitative and qualitative evaluations, a group of cardiologists performed medical evaluations to validate the quality and usability of the restored ECG, especially for an accurate arrhythmia diagnosis. }
}

@article{211211903v1,
  title={ The utility of wearable devices in assessing ambulatory impairments of   people with multiple sclerosis in free-living conditions },
  author={ Shaoxiong Sun and Amos A Folarin and Yuezhou Zhang and Nicholas Cummins and Shuo Liu and Callum Stewart and Yatharth Ranjan and Zulqarnain Rashid and Pauline Conde and Petroula Laiou and Heet Sankesara and Gloria Dalla Costa and Letizia Leocani and Per Soelberg Sørensen and Melinda Magyari and Ana Isabel Guerrero and Ana Zabalza and Srinivasan Vairavan and Raquel Bailon and Sara Simblett and Inez Myin-Germeys and Aki Rintala and Til Wykes and Vaibhav A Narayan and Matthew Hotopf and Giancarlo Comi and Richard JB Dobson and RADAR-CNS consortium },
  journal={ arXiv preprint arXiv:2112.11903v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2112.11903v1 },
  abstract={ Multiple sclerosis (MS) is a progressive inflammatory and neurodegenerative disease of the central nervous system affecting over 2.5 million people globally. In-clinic six-minute walk test (6MWT) is a widely used objective measure to evaluate the progression of MS. Yet, it has limitations such as the need for a clinical visit and a proper walkway. The widespread use of wearable devices capable of depicting patients activity profiles has the potential to assess the level of MS-induced disability in free-living conditions. In this work, we extracted 96 activity features in different temporal granularities (from minute-level to day-level) and explored their utility in estimating 6MWT scores in a European (Italy, Spain, and Denmark) MS cohort of 337 participants over an average of 10-month duration. We combined these features with participant demographics using three regression models including elastic net, gradient boosted trees and random forest. In addition, we quantified the individual feature contribution using feature importance in these regression models, linear mixed-effects models, generalized estimating equations, and correlation-based feature selection (CFS). The results showed promising estimation performance with R2 of 0.30, which was derived using random forest after CFS. This model was able to distinguish the participants with low disability from those with high disability. Furthermore, we observed that the minute-level (no longer than 8 minutes) step count, particularly those capturing the upper end of the step count distribution, had a stronger association with 6MWT. The use of a walking aid was indicative of ambulatory function measured through 6MWT. This study provides a basis for future investigation into the clinical relevance and utility of wearables in assessing MS progression in free-living conditions. }
}

@article{211115569v1,
  title={ Scalable Machine Learning Architecture for Neonatal Seizure Detection on   Ultra-Edge Devices },
  author={ Vishal Nagarajan and Ashwini Muralidharan and Deekshitha Sriraman and Pravin Kumar S },
  journal={ arXiv preprint arXiv:2111.15569v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2111.15569v1 },
  abstract={ Neonatal seizures are a commonly encountered neurological condition. They are the first clinical signs of a serious neurological disorder. Thus, rapid recognition and treatment are necessary to prevent serious fatalities. The use of electroencephalography (EEG) in the field of neurology allows precise diagnosis of several medical conditions. However, interpreting EEG signals needs the attention of highly specialized staff since the infant brain is developmentally immature during the neonatal period. Detecting seizures on time could potentially prevent the negative effects on the neurocognitive development of the infants. In recent years, neonatal seizure detection using machine learning algorithms have been gaining traction. Since there is a need for the classification of bio-signals to be computationally inexpensive in the case of seizure detection, this research presents a machine learning (ML) based architecture that operates with comparable predictive performance as previous models but with minimum level configuration. The proposed classifier was trained and tested on a public dataset of NICU seizures recorded at the Helsinki University Hospital. Our architecture achieved a best sensitivity of 87\%, which is 6\% more than that of the standard ML model chosen in this study. The model size of the ML classifier is optimized to just 4.84 KB with minimum prediction time of 182.61 milliseconds, thus enabling it to be deployed on wearable ultra-edge devices for quick and accurate response and obviating the need for cloud-based and other such exhaustive computational methods. }
}

@article{210509987v2,
  title={ Temporal convolutional networks predict dynamic oxygen uptake response   from wearable sensors across exercise intensities },
  author={ Robert Amelard and Eric T Hedge and Richard L Hughson },
  journal={ arXiv preprint arXiv:2105.09987v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2105.09987v2 },
  abstract={ Oxygen consumption (VO\$\_2\$) provides established clinical and physiological indicators of cardiorespiratory function and exercise capacity. However, VO\$\_2\$ monitoring is largely limited to specialized laboratory settings, making its widespread monitoring elusive. Here, we investigate temporal prediction of VO\$\_2\$ from wearable sensors during cycle ergometer exercise using a temporal convolutional network (TCN). Cardiorespiratory signals were acquired from a smart shirt with integrated textile sensors alongside ground-truth VO\$\_2\$ from a metabolic system on twenty-two young healthy adults. Participants performed one ramp-incremental and three pseudorandom binary sequence exercise protocols to assess a range of VO\$\_2\$ dynamics. A TCN model was developed using causal convolutions across an effective history length to model the time-dependent nature of VO\$\_2\$. Optimal history length was determined through minimum validation loss across hyperparameter values. The best performing model encoded 218 s history length (TCN-VO\$\_2\$ A), with 187 s, 97 s, and 76 s yielding less than 3\% deviation from the optimal validation loss. TCN-VO\$\_2\$ A showed strong prediction accuracy (mean, 95\% CI) across all exercise intensities (-22 ml.min\$\^{}\{-1\}\$, [-262, 218]), spanning transitions from low-moderate (-23 ml.min\$\^{}\{-1\}\$, [-250, 204]), low-high (14 ml.min\$\^{}\{-1\}\$, [-252, 280]), ventilatory threshold-high (-49 ml.min\$\^{}\{-1\}\$, [-274, 176]), and maximal (-32 ml.min\$\^{}\{-1\}\$, [-261, 197]) exercise. Second-by-second classification of physical activity across 16090 s of predicted VO\$\_2\$ was able to discern between vigorous, moderate, and light activity with high accuracy (94.1\%). This system enables quantitative aerobic activity monitoring in non-laboratory settings across a range of exercise intensities using wearable sensors for monitoring exercise prescription adherence and personal fitness. }
}

@article{211006139v1,
  title={ Classification of anomalous gait using Machine Learning techniques and   embedded sensors },
  author={ T. R. D. Sa and C. M. S. Figueiredo },
  journal={ arXiv preprint arXiv:2110.06139v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2110.06139v1 },
  abstract={ Human gait can be a predictive factor for detecting pathologies that affect human locomotion according to studies. In addition, it is known that a high investment is demanded in order to raise a traditional clinical infrastructure able to provide human gait examinations, making them unaffordable for economically vulnerable patients. In face of this scenario, this work proposes an accessible and modern solution composed of a wearable device, to acquire 3D-accelerometer and 3D-gyroscope measurements, and machine learning techniques to classify between distinct categories of induced gait disorders. In order to develop the proposed research, it was created a dataset with the target label being 4 distinct and balanced categories of anomalous gait. The machine learning techniques that achieved the best performances (in terms of accuracy) in this dataset were through the application of Principal Component Analysis algorithm following of a Support Vector Machines classifier (94 \\textbackslash{}\%). Further, an architecture based on a Feedforward Neural Network yielded even better results (96 \\textbackslash{}\%). Finally, it is also presented computational performance comparison between the models implemented. }
}

@article{210902442v2,
  title={ Parkinson's Disease Diagnosis based on Gait Cycle Analysis Through an   Interpretable Interval Type-2 Neuro-Fuzzy System },
  author={ Armin Salimi-Badr and Mohammad Hashemi and Hamidreza Saffari },
  journal={ arXiv preprint arXiv:2109.02442v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2109.02442v2 },
  abstract={ In this paper, an interpretable classifier using an interval type-2 fuzzy neural network for detecting patients suffering from Parkinson's Disease (PD) based on analyzing the gait cycle is presented. The proposed method utilizes clinical features extracted from the vertical Ground Reaction Force (vGRF), measured by 16 wearable sensors placed in the soles of subjects' shoes and learns interpretable fuzzy rules. Therefore, experts can verify the decision made by the proposed method based on investigating the firing strength of interpretable fuzzy rules. Moreover, experts can utilize the extracted fuzzy rules for patient diagnosing or adjust them based on their knowledge. To improve the robustness of the proposed method against uncertainty and noisy sensor measurements, Interval Type-2 Fuzzy Logic is applied. To learn fuzzy rules, two paradigms are proposed: 1- A batch learning approach based on clustering available samples is applied to extract initial fuzzy rules, 2- A complementary online learning is proposed to improve the rule base encountering new labeled samples. The performance of the method is evaluated for classifying patients and healthy subjects in different conditions including the presence of noise or observing new instances. Moreover, the performance of the model is compared to some previous supervised and unsupervised machine learning approaches. The final Accuracy, Precision, Recall, and F1 Score of the proposed method are 88.74\%, 89.41\%, 95.10\%, and 92.16\%. Finally, the extracted fuzzy sets for each feature are reported. }
}

@article{210211895v2,
  title={ MGait: Model-Based Gait Analysis Using Wearable Bend and Inertial   Sensors },
  author={ Sizhe An and Yigit Tuncel and Toygun Basaklar and Gokul Krishna Krishnakumar and Ganapati Bhat and Umit Ogras },
  journal={ arXiv preprint arXiv:2102.11895v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2102.11895v2 },
  abstract={ Movement disorders, such as Parkinson's disease, affect more than 10 million people worldwide. Gait analysis is a critical step in the diagnosis and rehabilitation of these disorders. Specifically, step length provides valuable insights into the gait quality and rehabilitation process. However, traditional approaches for estimating step length are not suitable for continuous daily monitoring since they rely on special mats and clinical environments. To address this limitation, we present a novel and practical step-length estimation technique using low-power wearable bend and inertial sensors. Experimental results show that the proposed model estimates step length with 5.49\% mean absolute percentage error and provides accurate real-time feedback to the user. }
}

@article{210808975v1,
  title={ Assessing Cerebellar Disorders With Wearable Inertial Sensor Data Using   Time-Frequency and Autoregressive Hidden Markov Model Approaches },
  author={ Karin C. Knudson and Anoopum S. Gupta },
  journal={ arXiv preprint arXiv:2108.08975v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2108.08975v1 },
  abstract={ We use autoregressive hidden Markov models and a time-frequency approach to create meaningful quantitative descriptions of behavioral characteristics of cerebellar ataxias from wearable inertial sensor data gathered during movement. Wearable sensor data is relatively easily collected and provides direct measurements of movement that can be used to develop useful behavioral biomarkers. Sensitive and specific behavioral biomarkers for neurodegenerative diseases are critical to supporting early detection, drug development efforts, and targeted treatments. We create a flexible and descriptive set of features derived from accelerometer and gyroscope data collected from wearable sensors while participants perform clinical assessment tasks, and with them estimate disease status and severity. A short period of data collection (\$<\$ 5 minutes) yields enough information to effectively separate patients with ataxia from healthy controls with very high accuracy, to separate ataxia from other neurodegenerative diseases such as Parkinson's disease, and to give estimates of disease severity. }
}

@article{210611855v1,
  title={ Intuitive and Ubiquitous Fever Monitoring Using Smartphones and   Smartwatches },
  author={ Joseph Breda and Shwetak Patel },
  journal={ arXiv preprint arXiv:2106.11855v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2106.11855v1 },
  abstract={ Inside all smart devices, such as smartphones or smartwatches, there are thermally sensitive resistors known as thermistors which are used to monitor the temperature of the device. These thermistors are sensitive to temperature changes near their location on-device. While they are designed to measure the temperature of the device components such as the battery, they can also sense changes in the temperature of the ambient environment or thermal entities in contact with the device. We have developed a model to estimate core body temperature from signals sensed by these thermistors during a user interaction in which the user places the capacitive touchscreen of a smart device against a thermal site on their body such as their forehead. During the interaction, the device logs the temperature sensed by the thermistors as well as the raw capacitance seen by the touch screen to capture features describing the rate of heat transfer from the body to the device and device-to-skin contact respectively. These temperature and contact features are then used to model the rate of heat transferred from the user's body to the device and thus core-body temperature of the user for ubiquitous and accessible fever monitoring using only a smart device. We validate this system in a lab environment on a simulated skin-like heat source with a temperature estimate mean absolute error of 0.743\$\^{}\{\\textbackslash{}circ\}\$F (roughly 0.4\$\^{}\{\\textbackslash{}circ\}\$C) and limit of agreement of \$\\textbackslash{}pm2.374\^{}\{\\textbackslash{}circ\}\$F (roughly 1.3\$\^{}\{\\textbackslash{}circ\}\$C) which is comparable to some off-the-shelf peripheral and tympanic thermometers. We found a Pearson's correlation \$R\^{}2\$ of 0.837 between ground truth temperature and temperature estimated by our system. We also deploy this system in an ongoing clinical study on a population of 7 participants in a clinical environment to show the similarity between simulated and clinical trials. }
}

@article{210409305v2,
  title={ Tracking agitation in people living with dementia in a care environment },
  author={ Shehroz S. Khan and Thaejaesh Sooriyakumaran and Katherine Rich and Sofija Spasojevic and Bing Ye and Kristine Newman and Andrea Iaboni and Alex Mihailidis },
  journal={ arXiv preprint arXiv:2104.09305v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2104.09305v2 },
  abstract={ Agitation is a symptom that communicates distress in people living with dementia (PwD), and that can place them and others at risk. In a long term care (LTC) environment, care staff track and document these symptoms as a way to detect when there has been a change in resident status to assess risk, and to monitor for response to interventions. However, this documentation can be time-consuming, and due to staffing constraints, episodes of agitation may go unobserved. This brings into question the reliability of these assessments, and presents an opportunity for technology to help track and monitor behavioural symptoms in dementia. In this paper, we present the outcomes of a 2 year real-world study performed in a dementia unit, where a multi-modal wearable device was worn by \$20\$ PwD. In line with a commonly used clinical documentation tool, this large multi-modal time-series data was analyzed to track the presence of episodes of agitation in 8-hour nursing shifts. The development of a baseline classification model (AUC=0.717) on this dataset and subsequent improvement (AUC= 0.779) lays the groundwork for automating the process of annotating agitation events in nursing charts. }
}

@article{201107684v3,
  title={ Inferring COPD Severity from Tidal Breathing },
  author={ Kofi Odame and Graham Atkins and Maria Nyamukuru and Katherine Fearon },
  journal={ arXiv preprint arXiv:2011.07684v3 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2011.07684v3 },
  abstract={ Objective: To develop an algorithm that can infer the severity level of a COPD patient's airflow limitation from tidal breathing data that is collected by a wearable device.   Methods: Data was collected from 25 single visit adult volunteers with a confirmed or suspected diagnosis of chronic obstructive pulmonary disease (COPD). The ground truth airflow limitation severity of each subject was determined by applying the Global Initiative for Chronic Obstructive Lung Disease (GOLD) staging criteria to the subject's spirometry results. Spirometry was performed in a pulmonary function test laboratory under the supervision of trained clinical staff. Separately, the subjects' respiratory signal was measured during quiet breathing, and a classification model was built to infer the subjects' level of airflow limitation from the measured respiratory signal. The classification model was evaluated against the ground truth using leave-one-out testing.   Results: Severity of airway obstruction was classified as either mild/moderate or severe/very severe with an accuracy of 96.4\%.   Conclusion: Tidal breathing parameters that are measured with a wearable device can be used to distinguish between different levels of airflow limitation in COPD patients. }
}

@article{200308474v2,
  title={ TILES-2018, a longitudinal physiologic and behavioral data set of   hospital workers },
  author={ Karel Mundnich and Brandon M. Booth and Michelle L'Hommedieu and Tiantian Feng and Benjamin Girault and Justin L'Hommedieu and Mackenzie Wildman and Sophia Skaaden and Amrutha Nadarajan and Jennifer L. Villatte and Tiago H. Falk and Kristina Lerman and Emilio Ferrara and Shrikanth Narayanan },
  journal={ arXiv preprint arXiv:2003.08474v2 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2003.08474v2 },
  abstract={ We present a novel longitudinal multimodal corpus of physiological and behavioral data collected from direct clinical providers in a hospital workplace. We designed the study to investigate the use of off-the-shelf wearable and environmental sensors to understand individual-specific constructs such as job performance, interpersonal interaction, and well-being of hospital workers over time in their natural day-to-day job settings. We collected behavioral and physiological data from \$n = 212\$ participants through Internet-of-Things Bluetooth data hubs, wearable sensors (including a wristband, a biometrics-tracking garment, a smartphone, and an audio-feature recorder), together with a battery of surveys to assess personality traits, behavioral states, job performance, and well-being over time. Besides the default use of the data set, we envision several novel research opportunities and potential applications, including multi-modal and multi-task behavioral modeling, authentication through biometrics, and privacy-aware and privacy-preserving machine learning. }
}

@article{201008485v1,
  title={ A New Open-Access Platform for Measuring and Sharing mTBI Data },
  author={ August G. Domel and Samuel J. Raymond and Chiara Giordano and Yuzhe Liu and Seyed Abdolmajid Yousefsani and Michael Fanton and Ileana Pirozzi and Ali Kight and Brett Avery and Athanasia Boumis and Tyler Fetters and Simran Jandu and William M Mehring and Sam Monga and Nicole Mouchawar and India Rangel and Eli Rice and Pritha Roy and Sohrab Sami and Heer Singh and Lyndia Wu and Calvin Kuo and Michael Zeineh and Gerald Grant and David B. Camarillo },
  journal={ arXiv preprint arXiv:2010.08485v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2010.08485v1 },
  abstract={ Despite numerous research efforts, the precise mechanisms of concussion have yet to be fully uncovered. Clinical studies on high-risk populations, such as contact sports athletes, have become more common and give insight on the link between impact severity and brain injury risk through the use of wearable sensors and neurological testing. However, as the number of institutions operating these studies grows, there is a growing need for a platform to share these data to facilitate our understanding of concussion mechanisms and aid in the development of suitable diagnostic tools. To that end, this paper puts forth two contributions: 1) a centralized, open-source platform for storing and sharing head impact data, in collaboration with the Federal Interagency Traumatic Brain Injury Research informatics system (FITBIR), and 2) a deep learning impact detection algorithm (MiGNet) to differentiate between true head impacts and false positives for the previously biomechanically validated instrumented mouthguard sensor (MiG2.0), all of which easily interfaces with FITBIR. We report 96\% accuracy using MiGNet, based on a neural network model, improving on previous work based on Support Vector Machines achieving 91\% accuracy, on an out of sample dataset of high school and collegiate football head impacts. The integrated MiG2.0 and FITBIR system serve as a collaborative research tool to be disseminated across multiple institutions towards creating a standardized dataset for furthering the knowledge of concussion biomechanics. }
}

@article{200708920v1,
  title={ Vision-based Estimation of MDS-UPDRS Gait Scores for Assessing   Parkinson's Disease Motor Severity },
  author={ Mandy Lu and Kathleen Poston and Adolf Pfefferbaum and Edith V. Sullivan and Li Fei-Fei and Kilian M. Pohl and Juan Carlos Niebles and Ehsan Adeli },
  journal={ arXiv preprint arXiv:2007.08920v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2007.08920v1 },
  abstract={ Parkinson's disease (PD) is a progressive neurological disorder primarily affecting motor function resulting in tremor at rest, rigidity, bradykinesia, and postural instability. The physical severity of PD impairments can be quantified through the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS), a widely used clinical rating scale. Accurate and quantitative assessment of disease progression is critical to developing a treatment that slows or stops further advancement of the disease. Prior work has mainly focused on dopamine transport neuroimaging for diagnosis or costly and intrusive wearables evaluating motor impairments. For the first time, we propose a computer vision-based model that observes non-intrusive video recordings of individuals, extracts their 3D body skeletons, tracks them through time, and classifies the movements according to the MDS-UPDRS gait scores. Experimental results show that our proposed method performs significantly better than chance and competing methods with an F1-score of 0.83 and a balanced accuracy of 81\%. This is the first benchmark for classifying PD patients based on MDS-UPDRS gait severity and could be an objective biomarker for disease severity. Our work demonstrates how computer-assisted technologies can be used to non-intrusively monitor patients and their motor impairments. The code is available at https://github.com/mlu355/PD-Motor-Severity-Estimation. }
}

@article{200302501v2,
  title={ Detecting Attended Visual Targets in Video },
  author={ Eunji Chong and Yongxin Wang and Nataniel Ruiz and James M. Rehg },
  journal={ arXiv preprint arXiv:2003.02501v2 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2003.02501v2 },
  abstract={ We address the problem of detecting attention targets in video. Our goal is to identify where each person in each frame of a video is looking, and correctly handle the case where the gaze target is out-of-frame. Our novel architecture models the dynamic interaction between the scene and head features and infers time-varying attention targets. We introduce a new annotated dataset, VideoAttentionTarget, containing complex and dynamic patterns of real-world gaze behavior. Our experiments show that our model can effectively infer dynamic attention in videos. In addition, we apply our predicted attention maps to two social gaze behavior recognition tasks, and show that the resulting classifiers significantly outperform existing methods. We achieve state-of-the-art performance on three datasets: GazeFollow (static images), VideoAttentionTarget (videos), and VideoCoAtt (videos), and obtain the first results for automatically classifying clinically-relevant gaze behavior without wearable cameras or eye trackers. }
}

@article{250510864v1,
  title={ Anti-Sensing: Defense against Unauthorized Radar-based Human Vital Sign   Sensing with Physically Realizable Wearable Oscillators },
  author={ Md Farhan Tasnim Oshim and Nigel Doering and Bashima Islam and Tsui-Wei Weng and Tauhidur Rahman },
  journal={ arXiv preprint arXiv:2505.10864v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2505.10864v1 },
  abstract={ Recent advancements in Ultra-Wideband (UWB) radar technology have enabled contactless, non-line-of-sight vital sign monitoring, making it a valuable tool for healthcare. However, UWB radar's ability to capture sensitive physiological data, even through walls, raises significant privacy concerns, particularly in human-robot interactions and autonomous systems that rely on radar for sensing human presence and physiological functions. In this paper, we present Anti-Sensing, a novel defense mechanism designed to prevent unauthorized radar-based sensing. Our approach introduces physically realizable perturbations, such as oscillatory motion from wearable devices, to disrupt radar sensing by mimicking natural cardiac motion, thereby misleading heart rate (HR) estimations. We develop a gradient-based algorithm to optimize the frequency and spatial amplitude of these oscillations for maximal disruption while ensuring physiological plausibility. Through both simulations and real-world experiments with radar data and neural network-based HR sensing models, we demonstrate the effectiveness of Anti-Sensing in significantly degrading model accuracy, offering a practical solution for privacy preservation. }
}

@article{250509099v1,
  title={ Imitation Learning for Adaptive Control of a Virtual Soft Exoglove },
  author={ Shirui Lyu and Vittorio Caggiano and Matteo Leonetti and Dario Farina and Letizia Gionfrida },
  journal={ arXiv preprint arXiv:2505.09099v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2505.09099v1 },
  abstract={ The use of wearable robots has been widely adopted in rehabilitation training for patients with hand motor impairments. However, the uniqueness of patients' muscle loss is often overlooked. Leveraging reinforcement learning and a biologically accurate musculoskeletal model in simulation, we propose a customized wearable robotic controller that is able to address specific muscle deficits and to provide compensation for hand-object manipulation tasks. Video data of a same subject performing human grasping tasks is used to train a manipulation model through learning from demonstration. This manipulation model is subsequently fine-tuned to perform object-specific interaction tasks. The muscle forces in the musculoskeletal manipulation model are then weakened to simulate neurological motor impairments, which are later compensated by the actuation of a virtual wearable robotics glove. Results shows that integrating the virtual wearable robotic glove provides shared assistance to support the hand manipulator with weakened muscle forces. The learned exoglove controller achieved an average of 90.5\\textbackslash{}\% of the original manipulation proficiency. }
}

@article{250508953v1,
  title={ Multimodal Modeling of Ultradian Rhythms Using the Hankel Alternative   View of Koopman (HAVOK) Analysis },
  author={ Emmanuel Molefi and Billy C. Smith and Christopher Thornton and Peter N. Taylor and Yujiang Wang },
  journal={ arXiv preprint arXiv:2505.08953v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2505.08953v1 },
  abstract={ Ultradian rhythms - quasi-rhythmic fluctuations in behavior and physiology with periods shorter than 24 hours - are observed across various organisms, including humans. Despite their role in key biological processes such as sleep architecture and hormone regulation, their underlying mechanisms remain poorly understood. Here, we leveraged wearable sensor technology for continuous monitoring of physiological signals in 16 healthy participants over two weeks. By systematically removing circadian and longer-scale rhythms, we isolated ultradian dynamics and modeled them using the Hankel Alternative View of Koopman (HAVOK) framework,a data-driven approach based on Takens' embedding theorem and Koopman operator theory. This allowed us to characterize ultradian rhythms as an intermittently forced linear system and distinguish between regular oscillatory behavior and more complex dynamics. Across participants, ultradian fluctuations were well-described by the HAVOK model, with intermittent forcing consistently observed. The model demonstrated strong forecasting accuracy, with root mean squared error (RMSE) of \$0.0315 \\textbackslash{}pm 0.02\$, \$0.0306 \\textbackslash{}pm 0.02\$, and \$0.0218 \\textbackslash{}pm 0.02\$ in the leading time-delay coordinates. Notably, a significant sex difference in model rank (z = -2.06, p = 0.0396) suggests that sex hormones may play a key role in ultradian dynamics. These findings provide evidence for intermittently forced linear systems as a useful framework for understanding ultradian rhythms and their regulation. }
}

@article{250417696v3,
  title={ Hierarchical and Multimodal Data for Daily Activity Understanding },
  author={ Ghazal Kaviani and Yavuz Yarici and Seulgi Kim and Mohit Prabhushankar and Ghassan AlRegib and Mashhour Solh and Ameya Patil },
  journal={ arXiv preprint arXiv:2504.17696v3 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2504.17696v3 },
  abstract={ Daily Activity Recordings for Artificial Intelligence (DARai, pronounced ''Dahr-ree'') is a multimodal, hierarchically annotated dataset constructed to understand human activities in real-world settings. DARai consists of continuous scripted and unscripted recordings of 50 participants in 10 different environments, totaling over 200 hours of data from 20 sensors including multiple camera views, depth and radar sensors, wearable inertial measurement units (IMUs), electromyography (EMG), insole pressure sensors, biomonitor sensors, and gaze tracker.   To capture the complexity in human activities, DARai is annotated at three levels of hierarchy: (i) high-level activities (L1) that are independent tasks, (ii) lower-level actions (L2) that are patterns shared between activities, and (iii) fine-grained procedures (L3) that detail the exact execution steps for actions. The dataset annotations and recordings are designed so that 22.7\% of L2 actions are shared between L1 activities and 14.2\% of L3 procedures are shared between L2 actions. The overlap and unscripted nature of DARai allows counterfactual activities in the dataset.   Experiments with various machine learning models showcase the value of DARai in uncovering important challenges in human-centered applications. Specifically, we conduct unimodal and multimodal sensor fusion experiments for recognition, temporal localization, and future action anticipation across all hierarchical annotation levels. To highlight the limitations of individual sensors, we also conduct domain-variant experiments that are enabled by DARai's multi-sensor and counterfactual activity design setup.   The code, documentation, and dataset are available at the dedicated DARai website: https://alregib.ece.gatech.edu/software-and-datasets/darai-daily-activity-recordings-for-artificial-intelligence-and-machine-learning/ }
}

@article{250507668v1,
  title={ Intuitive Human-Robot Interfaces Leveraging on Autonomy Features for the   Control of Highly-redundant Robots },
  author={ Davide Torielli },
  journal={ arXiv preprint arXiv:2505.07668v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2505.07668v1 },
  abstract={ [...] With the TelePhysicalOperation interface, the user can teleoperate the different capabilities of a robot (e.g., single/double arm manipulation, wheel/leg locomotion) by applying virtual forces on selected robot body parts. This approach emulates the intuitiveness of physical human-robot interaction, but at the same time it permits to teleoperate the robot from a safe distance, in a way that resembles a ''Marionette'' interface. The system is further enhanced with wearable haptic feedback functions to align better with the ''Marionette'' metaphor, and a user study has been conducted to validate its efficacy with and without the haptic channel enabled. Considering the importance of robot independence, the TelePhysicalOperation interface incorporates autonomy modules to face, for example, the teleoperation of dual-arm mobile base robots for bimanual object grasping and transportation tasks.   With the laser-guided interface, the user can indicate points of interest to the robot through the utilization of a simple but effective laser emitter device. With a neural network-based vision system, the robot tracks the laser projection in real time, allowing the user to indicate not only fixed goals, like objects, but also paths to follow. With the implemented autonomous behavior, a mobile manipulator employs its locomanipulation abilities to follow the indicated goals. The behavior is modeled using Behavior Trees, exploiting their reactivity to promptly respond to changes in goal positions, and their modularity to adapt the motion planning to the task needs. The proposed laser interface has also been employed in an assistive scenario. In this case, users with upper limbs impairments can control an assistive manipulator by directing a head-worn laser emitter to the point of interests, to collaboratively address activities of everyday life. [...] }
}

@article{240614498v3,
  title={ LLaSA: A Multimodal LLM for Human Activity Analysis Through Wearable and   Smartphone Sensors },
  author={ Sheikh Asif Imran and Mohammad Nur Hossain Khan and Subrata Biswas and Bashima Islam },
  journal={ arXiv preprint arXiv:2406.14498v3 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2406.14498v3 },
  abstract={ Wearables generate rich motion data, yet current systems only classify what happened - failing to support natural questions about why it happened or what it means. We introduce LLaSA (Large Language and Sensor Assistant), a compact 13B model that enables ask-anything, open-ended question answering grounded in raw IMU data. LLaSA supports conversational, context-aware reasoning - explaining the causes of sensor-detected behaviors and answering free-form questions in real-world scenarios. It is tuned for scientific accuracy, coherence, and response reliability. To advance this new task of sensor-based QA, we release three large-scale datasets: SensorCaps, OpenSQA, and Tune-OpenSQA. Together, these resources define a new benchmark for sensor-language models. LLaSA consistently produces interpretable, causal answers and outperforms commercial LLMs across both public and real-world settings. Our code repository and datasets can be found at https://github.com/BASHLab/LLaSA. }
}

@article{250408907v2,
  title={ Spatial Audio Processing with Large Language Model on Wearable Devices },
  author={ Ayushi Mishra and Yang Bai and Priyadarshan Narayanasamy and Nakul Garg and Nirupam Roy },
  journal={ arXiv preprint arXiv:2504.08907v2 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2504.08907v2 },
  abstract={ Integrating spatial context into large language models (LLMs) has the potential to revolutionize human-computer interaction, particularly in wearable devices. In this work, we present a novel system architecture that incorporates spatial speech understanding into LLMs, enabling contextually aware and adaptive applications for wearable technologies. Our approach leverages microstructure-based spatial sensing to extract precise Direction of Arrival (DoA) information using a monaural microphone. To address the lack of existing dataset for microstructure-assisted speech recordings, we synthetically create a dataset called OmniTalk by using the LibriSpeech dataset. This spatial information is fused with linguistic embeddings from OpenAI's Whisper model, allowing each modality to learn complementary contextual representations. The fused embeddings are aligned with the input space of LLaMA-3.2 3B model and fine-tuned with lightweight adaptation technique LoRA to optimize for on-device processing. SING supports spatially-aware automatic speech recognition (ASR), achieving a mean error of \$25.72\^{}\\textbackslash{}circ\$-a substantial improvement compared to the 88.52\$\^{}\\textbackslash{}circ\$ median error in existing work-with a word error rate (WER) of 5.3. SING also supports soundscaping, for example, inference how many people were talking and their directions, with up to 5 people and a median DoA error of 16\$\^{}\\textbackslash{}circ\$. Our system demonstrates superior performance in spatial speech understanding while addressing the challenges of power efficiency, privacy, and hardware constraints, paving the way for advanced applications in augmented reality, accessibility, and immersive experiences. }
}

@article{250110917v2,
  title={ Decomposing and Fusing Intra- and Inter-Sensor Spatio-Temporal Signal   for Multi-Sensor Wearable Human Activity Recognition },
  author={ Haoyu Xie and Haoxuan Li and Chunyuan Zheng and Haonan Yuan and Guorui Liao and Jun Liao and Li Liu },
  journal={ arXiv preprint arXiv:2501.10917v2 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2501.10917v2 },
  abstract={ Wearable Human Activity Recognition (WHAR) is a prominent research area within ubiquitous computing. Multi-sensor synchronous measurement has proven to be more effective for WHAR than using a single sensor. However, existing WHAR methods use shared convolutional kernels for indiscriminate temporal feature extraction across each sensor variable, which fails to effectively capture spatio-temporal relationships of intra-sensor and inter-sensor variables. We propose the DecomposeWHAR model consisting of a decomposition phase and a fusion phase to better model the relationships between modality variables. The decomposition creates high-dimensional representations of each intra-sensor variable through the improved Depth Separable Convolution to capture local temporal features while preserving their unique characteristics. The fusion phase begins by capturing relationships between intra-sensor variables and fusing their features at both the channel and variable levels. Long-range temporal dependencies are modeled using the State Space Model (SSM), and later cross-sensor interactions are dynamically captured through a self-attention mechanism, highlighting inter-sensor spatial correlations. Our model demonstrates superior performance on three widely used WHAR datasets, significantly outperforming state-of-the-art models while maintaining acceptable computational efficiency. }
}

@article{250413370v1,
  title={ Multi-Sensor Fusion-Based Mobile Manipulator Remote Control for   Intelligent Smart Home Assistance },
  author={ Xiao Jin and Bo Xiao and Huijiang Wang and Wendong Wang and Zhenhua Yu },
  journal={ arXiv preprint arXiv:2504.13370v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2504.13370v1 },
  abstract={ This paper proposes a wearable-controlled mobile manipulator system for intelligent smart home assistance, integrating MEMS capacitive microphones, IMU sensors, vibration motors, and pressure feedback to enhance human-robot interaction. The wearable device captures forearm muscle activity and converts it into real-time control signals for mobile manipulation. The wearable device achieves an offline classification accuracy of 88.33\\textbackslash{}\%\\textbackslash{} across six distinct movement-force classes for hand gestures by using a CNN-LSTM model, while real-world experiments involving five participants yield a practical accuracy of 83.33\\textbackslash{}\%\\textbackslash{} with an average system response time of 1.2 seconds. In Human-Robot synergy in navigation and grasping tasks, the robot achieved a 98\\textbackslash{}\%\\textbackslash{} task success rate with an average trajectory deviation of only 3.6 cm. Finally, the wearable-controlled mobile manipulator system achieved a 93.3\\textbackslash{}\%\\textbackslash{} gripping success rate, a transfer success of 95.6\\textbackslash{}\%\\textbackslash{}, and a full-task success rate of 91.1\\textbackslash{}\%\\textbackslash{} during object grasping and transfer tests, in which a total of 9 object-texture combinations were evaluated. These three experiments' results validate the effectiveness of MEMS-based wearable sensing combined with multi-sensor fusion for reliable and intuitive control of assistive robots in smart home scenarios. }
}

@article{250412039v1,
  title={ RadMamba: Efficient Human Activity Recognition through Radar-based   Micro-Doppler-Oriented Mamba State-Space Model },
  author={ Yizhuo Wu and Francesco Fioranelli and Chang Gao },
  journal={ arXiv preprint arXiv:2504.12039v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2504.12039v1 },
  abstract={ Radar-based HAR has emerged as a promising alternative to conventional monitoring approaches, such as wearable devices and camera-based systems, due to its unique privacy preservation and robustness advantages. However, existing solutions based on convolutional and recurrent neural networks, although effective, are computationally demanding during deployment. This limits their applicability in scenarios with constrained resources or those requiring multiple sensors. Advanced architectures, such as ViT and SSM architectures, offer improved modeling capabilities and have made efforts toward lightweight designs. However, their computational complexity remains relatively high. To leverage the strengths of transformer architectures while simultaneously enhancing accuracy and reducing computational complexity, this paper introduces RadMamba, a parameter-efficient, radar micro-Doppler-oriented Mamba SSM specifically tailored for radar-based HAR. Across three diverse datasets, RadMamba matches the top-performing previous model's 99.8\% classification accuracy on Dataset DIAT with only 1/400 of its parameters and equals the leading models' 92.0\% accuracy on Dataset CI4R with merely 1/10 of their parameters. In scenarios with continuous sequences of actions evaluated on Dataset UoG2020, RadMamba surpasses other models with significantly higher parameter counts by at least 3\%, achieving this with only 6.7k parameters. Our code is available at: https://github.com/lab-emi/AIRHAR. }
}

@article{250413921v1,
  title={ Wireless Silent Speech Interface Using Multi-Channel Textile EMG Sensors   Integrated into Headphones },
  author={ Chenyu Tang and Josée Mallah and Dominika Kazieczko and Wentian Yi and Tharun Reddy Kandukuri and Edoardo Occhipinti and Bhaskar Mishra and Sunita Mehta and Luigi G. Occhipinti },
  journal={ arXiv preprint arXiv:2504.13921v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2504.13921v1 },
  abstract={ This paper presents a novel wireless silent speech interface (SSI) integrating multi-channel textile-based EMG electrodes into headphone earmuff for real-time, hands-free communication. Unlike conventional patch-based EMG systems, which require large-area electrodes on the face or neck, our approach ensures comfort, discretion, and wearability while maintaining robust silent speech decoding. The system utilizes four graphene/PEDOT:PSS-coated textile electrodes to capture speech-related neuromuscular activity, with signals processed via a compact ESP32-S3-based wireless readout module. To address the challenge of variable skin-electrode coupling, we propose a 1D SE-ResNet architecture incorporating squeeze-and-excitation (SE) blocks to dynamically adjust per-channel attention weights, enhancing robustness against motion-induced impedance variations. The proposed system achieves 96\% accuracy on 10 commonly used voice-free control words, outperforming conventional single-channel and non-adaptive baselines. Experimental validation, including XAI-based attention analysis and t-SNE feature visualization, confirms the adaptive channel selection capability and effective feature extraction of the model. This work advances wearable EMG-based SSIs, demonstrating a scalable, low-power, and user-friendly platform for silent communication, assistive technologies, and human-computer interaction. }
}

@article{241118822v5,
  title={ RelCon: Relative Contrastive Learning for a Motion Foundation Model for   Wearable Data },
  author={ Maxwell A. Xu and Jaya Narain and Gregory Darnell and Haraldur Hallgrimsson and Hyewon Jeong and Darren Forde and Richard Fineman and Karthik J. Raghuram and James M. Rehg and Shirley Ren },
  journal={ arXiv preprint arXiv:2411.18822v5 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2411.18822v5 },
  abstract={ We present RelCon, a novel self-supervised Relative Contrastive learning approach for training a motion foundation model from wearable accelerometry sensors. First, a learnable distance measure is trained to capture motif similarity and domain-specific semantic information such as rotation invariance. Then, the learned distance provides a measurement of semantic similarity between a pair of accelerometry time-series, which we use to train our foundation model to model relative relationships across time and across subjects. The foundation model is trained on 1 billion segments from 87,376 participants, and achieves state-of-the-art performance across multiple downstream tasks, including human activity recognition and gait metric regression. To our knowledge, we are the first to show the generalizability of a foundation model with motion data from wearables across distinct evaluation tasks. }
}

@article{240513955v2,
  title={ Decoding Brain Dynamics in Motor Planning Based on EEG Microstates for   Predicting Pedestrian Road-Crossing in Vehicle-to-Everything Architectures },
  author={ Xiaoshan Zhou and Carol C. Menassa and Vineet R. Kamat },
  journal={ arXiv preprint arXiv:2405.13955v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2405.13955v2 },
  abstract={ Pedestrians who cross roads, often emerge from occlusion or abruptly begin crossing from a standstill, frequently leading to unintended collisions with vehicular traffic that result in accidents and interruptions. Existing studies have predominantly relied on external network sensing and observational data to anticipate pedestrian motion. However, these methods are post hoc, reducing the vehicles' ability to respond in a timely manner. This study addresses these gaps by introducing a novel data stream and analytical framework derived from pedestrians' wearable electroencephalogram (EEG) signals to predict motor planning in road crossings. Experiments were conducted where participants were embodied in a visual avatar as pedestrians and interacted with varying traffic volumes, marked crosswalks, and traffic signals. To understand how human cognitive modules flexibly interplay with hemispheric asymmetries in functional specialization, we analyzed time-frequency representation and functional connectivity using collected EEG signals and constructed a Gaussian Hidden Markov Model to decompose EEG sequences into cognitive microstate transitions based on posterior probabilistic reasoning. Subsequently, datasets were constructed using a sliding window approach, and motor readiness was predicted using the K-nearest Neighbors algorithm combined with Dynamic Time Warping. Results showed that high-beta oscillations in the frontocentral cortex achieved an Area Under the Curve of 0.91 with approximately a 1-second anticipatory lead window before physical road crossing movement occurred. These preliminary results signify a transformative shift towards pedestrians proactively signaling their motor intentions to autonomous vehicles within intelligent V2X systems. The proposed framework is also adaptable to various human-robot interactions, enabling seamless collaboration in dynamic mobile environments. }
}

@article{250403334v1,
  title={ Data Augmentation of Time-Series Data in Human Movement Biomechanics: A   Scoping Review },
  author={ Christina Halmich and Lucas Höschler and Christoph Schranz and Christian Borgelt },
  journal={ arXiv preprint arXiv:2504.03334v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2504.03334v1 },
  abstract={ The integration of machine learning and deep learning has transformed data analytics in biomechanics, enabled by extensive wearable sensor data. However, the field faces challenges such as limited large-scale datasets and high data acquisition costs, which hinder the development of robust algorithms. Data augmentation techniques show promise in addressing these issues, but their application to biomechanical time-series data requires comprehensive evaluation.   This scoping review investigates data augmentation methods for time-series data in the biomechanics domain. It analyzes current approaches for augmenting and generating time-series datasets, evaluates their effectiveness, and offers recommendations for applying these techniques in biomechanics.   Four databases, PubMed, IEEE Xplore, Scopus, and Web of Science, were searched for studies published between 2013 and 2024. Following PRISMA-ScR guidelines, a two-stage screening identified 21 relevant publications.   Results show that there is no universally preferred method for augmenting biomechanical time-series data; instead, methods vary based on study objectives. A major issue identified is the absence of soft tissue artifacts in synthetic data, leading to discrepancies referred to as the synthetic gap. Moreover, many studies lack proper evaluation of augmentation methods, making it difficult to assess their effects on model performance and data quality.   This review highlights the critical role of data augmentation in addressing limited dataset availability and improving model generalization in biomechanics. Tailoring augmentation strategies to the characteristics of biomechanical data is essential for advancing predictive modeling. A better understanding of how different augmentation methods impact data quality and downstream tasks will be key to developing more effective and realistic techniques. }
}

@article{250402895v1,
  title={ UAC: Uncertainty-Aware Calibration of Neural Networks for Gesture   Detection },
  author={ Farida Al Haddad and Yuxin Wang and Malcolm Mielle },
  journal={ arXiv preprint arXiv:2504.02895v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2504.02895v1 },
  abstract={ Artificial intelligence has the potential to impact safety and efficiency in safety-critical domains such as construction, manufacturing, and healthcare. For example, using sensor data from wearable devices, such as inertial measurement units (IMUs), human gestures can be detected while maintaining privacy, thereby ensuring that safety protocols are followed. However, strict safety requirements in these domains have limited the adoption of AI, since accurate calibration of predicted probabilities and robustness against out-of-distribution (OOD) data is necessary.   This paper proposes UAC (Uncertainty-Aware Calibration), a novel two-step method to address these challenges in IMU-based gesture recognition. First, we present an uncertainty-aware gesture network architecture that predicts both gesture probabilities and their associated uncertainties from IMU data. This uncertainty is then used to calibrate the probabilities of each potential gesture. Second, an entropy-weighted expectation of predictions over multiple IMU data windows is used to improve accuracy while maintaining correct calibration.   Our method is evaluated using three publicly available IMU datasets for gesture detection and is compared to three state-of-the-art calibration methods for neural networks: temperature scaling, entropy maximization, and Laplace approximation. UAC outperforms existing methods, achieving improved accuracy and calibration in both OOD and in-distribution scenarios. Moreover, we find that, unlike our method, none of the state-of-the-art methods significantly improve the calibration of IMU-based gesture recognition models. In conclusion, our work highlights the advantages of uncertainty-aware calibration of neural networks, demonstrating improvements in both calibration and accuracy for gesture detection using IMU data. }
}

@article{250323725v1,
  title={ Exploring Temporal Dynamics in Event-based Eye Tracker },
  author={ Hongwei Ren and Xiaopeng Lin and Hongxiang Huang and Yue Zhou and Bojun Cheng },
  journal={ arXiv preprint arXiv:2503.23725v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.23725v1 },
  abstract={ Eye-tracking is a vital technology for human-computer interaction, especially in wearable devices such as AR, VR, and XR. The realization of high-speed and high-precision eye-tracking using frame-based image sensors is constrained by their limited temporal resolution, which impairs the accurate capture of rapid ocular dynamics, such as saccades and blinks. Event cameras, inspired by biological vision systems, are capable of perceiving eye movements with extremely low power consumption and ultra-high temporal resolution. This makes them a promising solution for achieving high-speed, high-precision tracking with rich temporal dynamics. In this paper, we propose TDTracker, an effective eye-tracking framework that captures rapid eye movements by thoroughly modeling temporal dynamics from both implicit and explicit perspectives. TDTracker utilizes 3D convolutional neural networks to capture implicit short-term temporal dynamics and employs a cascaded structure consisting of a Frequency-aware Module, GRU, and Mamba to extract explicit long-term temporal dynamics. Ultimately, a prediction heatmap is used for eye coordinate regression. Experimental results demonstrate that TDTracker achieves state-of-the-art (SOTA) performance on the synthetic SEET dataset and secured Third place in the CVPR event-based eye-tracking challenge 2025. Our code is available at https://github.com/rhwxmx/TDTracker. }
}

@article{250323537v1,
  title={ Redundant feature screening method for human activity recognition based   on attention purification mechanism },
  author={ Hanyu Liu and Xiaoyang Li and Yixuan Jiang and Haotian Tang and Dongchen Wu and Yameng Guo },
  journal={ arXiv preprint arXiv:2503.23537v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.23537v1 },
  abstract={ In the field of sensor-based Human Activity Recognition (HAR), deep neural networks provide advanced technical support. Many studies have proven that recognition accuracy can be improved by increasing the depth or width of the network. However, for wearable devices, the balance between network performance and resource consumption is crucial. With minimum resource consumption as the basic principle, we propose a universal attention feature purification mechanism, called MSAP, which is suitable for multi-scale networks. The mechanism effectively solves the feature redundancy caused by the superposition of multi-scale features by means of inter-scale attention screening and connection method. In addition, we have designed a network correction module that integrates seamlessly between layers of individual network modules to mitigate inherent problems in deep networks. We also built an embedded deployment system that is in line with the current level of wearable technology to test the practical feasibility of the HAR model, and further prove the efficiency of the method. Extensive experiments on four public datasets show that the proposed method model effectively reduces redundant features in filtered data and provides excellent performance with little resource consumption. }
}

@article{250322239v1,
  title={ Relationship between household attributes and contact patterns in urban   and rural South Africa },
  author={ Kausutua Tjikundi and Lorenzo Dall'Amico and Jackie Kleynhans and Stefano Tempia and Cheryl Cohen and Daniela Paolotti and Ciro Cattuto },
  journal={ arXiv preprint arXiv:2503.22239v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.22239v1 },
  abstract={ Households play a crucial role in the propagation of infectious diseases due to the frequent and prolonged interactions that typically occur between their members. Recent studies advocated for the importance of including socioeconomic variables in epidemic models to account for the heterogeneity induced by human behavior. While sub-Saharan Africa suffers the highest burden of infectious disease diffusion, limited efforts have been carried out to investigate the mixing patterns in the countries and their relation with social indicators. This work analyzes household contact matrices measured with wearable proximity sensors in a rural and an urban village in South Africa. Leveraging a rich data collection describing additional individual and household attributes, we investigate how the household contact matrix varies according to the household type (whether it is composed only of a familiar nucleus or by a larger group), the gender of its head (the primary decision-maker), the rural or urban context and the season in which it was measured. We show the household type and the gender of its head induce differences in the interaction patterns between household members, particularly regarding child caregiving. We argue the observed differences directly influence the basic reproductive number of an epidemic and should hence be accounted for the design of effective epidemic mitigation strategies. }
}

@article{250321843v1,
  title={ CMD-HAR: Cross-Modal Disentanglement for Wearable Human Activity   Recognition },
  author={ Hanyu Liu and Siyao Li and Ying Yu and Yixuan Jiang and Hang Xiao and Jingxi Long and Haotian Tang },
  journal={ arXiv preprint arXiv:2503.21843v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.21843v1 },
  abstract={ Human Activity Recognition (HAR) is a fundamental technology for numerous human - centered intelligent applications. Although deep learning methods have been utilized to accelerate feature extraction, issues such as multimodal data mixing, activity heterogeneity, and complex model deployment remain largely unresolved. The aim of this paper is to address issues such as multimodal data mixing, activity heterogeneity, and complex model deployment in sensor-based human activity recognition. We propose a spatiotemporal attention modal decomposition alignment fusion strategy to tackle the problem of the mixed distribution of sensor data. Key discriminative features of activities are captured through cross-modal spatio-temporal disentangled representation, and gradient modulation is combined to alleviate data heterogeneity. In addition, a wearable deployment simulation system is constructed. We conducted experiments on a large number of public datasets, demonstrating the effectiveness of the model. }
}

@article{250312334v2,
  title={ When neural implant meets multimodal LLM: A dual-loop system for   neuromodulation and naturalistic neuralbehavioral research },
  author={ Edward Hong Wang and Cynthia Xin Wen },
  journal={ arXiv preprint arXiv:2503.12334v2 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.12334v2 },
  abstract={ We propose a novel dual-loop system that synergistically combines responsive neurostimulation (RNS) implants with artificial intelligence-driven wearable devices for treating post-traumatic stress disorder (PTSD) and enabling naturalistic brain research. In PTSD Therapy Mode, an implanted closed-loop neural device monitors amygdala activity and provides on-demand stimulation upon detecting pathological theta oscillations, while an ensemble of wearables (smart glasses, smartwatches, smartphones) uses multimodal large language model (LLM) analysis of sensory data to detect environmental or physiological PTSD triggers and deliver timely audiovisual interventions. Logged events from both the neural and wearable loops are analyzed to personalize trigger detection and progressively transition patients to non-invasive interventions. In Neuroscience Research Mode, the same platform is adapted for real-world brain activity capture. Wearable-LLM systems recognize naturalistic events (social interactions, emotional situations, compulsive behaviors, decision making) and signal implanted RNS devices (via wireless triggers) to record synchronized intracranial data during these moments. This approach builds on recent advances in mobile intracranial EEG recording and closed-loop neuromodulation in humans (BRAIN Initiative, 2023) (Mobbs et al., 2021). We discuss how our interdisciplinary system could revolutionize PTSD therapy and cognitive neuroscience by enabling 24/7 monitoring, context-aware intervention, and rich data collection outside traditional labs. The vision is a future where AI-enhanced devices continuously collaborate with the human brain, offering therapeutic support and deep insights into neural function, with the resulting real-world context rich neural data, in turn, accelerating the development of more biologically-grounded and human-centric AI. }
}

@article{250317978v1,
  title={ PIM: Physics-Informed Multi-task Pre-training for Improving Inertial   Sensor-Based Human Activity Recognition },
  author={ Dominique Nshimyimana and Vitor Fortes Rey and Sungho Suh and Bo Zhou and Paul Lukowicz },
  journal={ arXiv preprint arXiv:2503.17978v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.17978v1 },
  abstract={ Human activity recognition (HAR) with deep learning models relies on large amounts of labeled data, often challenging to obtain due to associated cost, time, and labor. Self-supervised learning (SSL) has emerged as an effective approach to leverage unlabeled data through pretext tasks, such as masked reconstruction and multitask learning with signal processing-based data augmentations, to pre-train encoder models. However, such methods are often derived from computer vision approaches that disregard physical mechanisms and constraints that govern wearable sensor data and the phenomena they reflect. In this paper, we propose a physics-informed multi-task pre-training (PIM) framework for IMU-based HAR. PIM generates pre-text tasks based on the understanding of basic physical aspects of human motion: including movement speed, angles of movement, and symmetry between sensor placements. Given a sensor signal, we calculate corresponding features using physics-based equations and use them as pretext tasks for SSL. This enables the model to capture fundamental physical characteristics of human activities, which is especially relevant for multi-sensor systems. Experimental evaluations on four HAR benchmark datasets demonstrate that the proposed method outperforms existing state-of-the-art methods, including data augmentation and masked reconstruction, in terms of accuracy and F1 score. We have observed gains of almost 10\\textbackslash{}\% in macro f1 score and accuracy with only 2 to 8 labeled examples per class and up to 3\% when there is no reduction in the amount of training data. }
}

@article{250316904v1,
  title={ Deep Learning for Human Locomotion Analysis in Lower-Limb Exoskeletons:   A Comparative Study },
  author={ Omar Coser and Christian Tamantini and Matteo Tortora and Leonardo Furia and Rosa Sicilia and Loredana Zollo and Paolo Soda },
  journal={ arXiv preprint arXiv:2503.16904v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.16904v1 },
  abstract={ Wearable robotics for lower-limb assistance have become a pivotal area of research, aiming to enhance mobility for individuals with physical impairments or augment the performance of able-bodied users. Accurate and adaptive control systems are essential to ensure seamless interaction between the wearer and the robotic device, particularly when navigating diverse and dynamic terrains. Despite the recent advances in neural networks for time series analysis, no attempts have been directed towards the classification of ground conditions, categorized into five classes and subsequently determining the ramp's slope and stair's height. In this respect, this paper presents an experimental comparison between eight deep neural network backbones to predict high-level locomotion parameters across diverse terrains.   All the models are trained on the publicly available CAMARGO 2021 dataset. IMU-only data equally or outperformed IMU+EMG inputs, promoting a cost-effective and efficient design. Indeeds, using three IMU sensors, the LSTM achieved high terrain classification accuracy (0.94 +- 0.04) and precise ramp slope (1.95 +- 0.58\{\\textbackslash{}deg\}) and the CNN-LSTM a stair height (15.65 +- 7.40 mm) estimations. As a further contribution, SHAP analysis justified sensor reduction without performance loss, ensuring a lightweight setup. The system operates with \~{}2 ms inference time, supporting real-time applications. The code is code available at https://github.com/cosbidev/Human-Locomotion-Identification. }
}

@article{241020081v3,
  title={ emg2qwerty: A Large Dataset with Baselines for Touch Typing using   Surface Electromyography },
  author={ Viswanath Sivakumar and Jeffrey Seely and Alan Du and Sean R Bittner and Adam Berenzweig and Anuoluwapo Bolarinwa and Alexandre Gramfort and Michael I Mandel },
  journal={ arXiv preprint arXiv:2410.20081v3 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.20081v3 },
  abstract={ Surface electromyography (sEMG) non-invasively measures signals generated by muscle activity with sufficient sensitivity to detect individual spinal neurons and richness to identify dozens of gestures and their nuances. Wearable wrist-based sEMG sensors have the potential to offer low friction, subtle, information rich, always available human-computer inputs. To this end, we introduce emg2qwerty, a large-scale dataset of non-invasive electromyographic signals recorded at the wrists while touch typing on a QWERTY keyboard, together with ground-truth annotations and reproducible baselines. With 1,135 sessions spanning 108 users and 346 hours of recording, this is the largest such public dataset to date. These data demonstrate non-trivial, but well defined hierarchical relationships both in terms of the generative process, from neurons to muscles and muscle combinations, as well as in terms of domain shift across users and user sessions. Applying standard modeling techniques from the closely related field of Automatic Speech Recognition (ASR), we show strong baseline performance on predicting key-presses using sEMG signals alone. We believe the richness of this task and dataset will facilitate progress in several problems of interest to both the machine learning and neuroscientific communities. Dataset and code can be accessed at https://github.com/facebookresearch/emg2qwerty. }
}

@article{250311433v1,
  title={ Adaptive Torque Control of Exoskeletons under Spasticity Conditions via   Reinforcement Learning },
  author={ Andrés Chavarrías and David Rodriguez-Cianca and Pablo Lanillos },
  journal={ arXiv preprint arXiv:2503.11433v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.11433v1 },
  abstract={ Spasticity is a common movement disorder symptom in individuals with cerebral palsy, hereditary spastic paraplegia, spinal cord injury and stroke, being one of the most disabling features in the progression of these diseases. Despite the potential benefit of using wearable robots to treat spasticity, their use is not currently recommended to subjects with a level of spasticity above \$\{1\^{}+\}\$ on the Modified Ashworth Scale. The varying dynamics of this velocity-dependent tonic stretch reflex make it difficult to deploy safe personalized controllers. Here, we describe a novel adaptive torque controller via deep reinforcement learning (RL) for a knee exoskeleton under joint spasticity conditions, which accounts for task performance and interaction forces reduction. To train the RL agent, we developed a digital twin, including a musculoskeletal-exoskeleton system with joint misalignment and a differentiable spastic reflexes model for the muscles activation. Results for a simulated knee extension movement showed that the agent learns to control the exoskeleton for individuals with different levels of spasticity. The proposed controller was able to reduce maximum torques applied to the human joint under spastic conditions by an average of 10.6\\textbackslash{}\% and decreases the root mean square until the settling time by 8.9\\textbackslash{}\% compared to a conventional compliant controller. }
}

@article{250309143v1,
  title={ Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video   Understanding },
  author={ Haoyu Zhang and Qiaohui Chu and Meng Liu and Yunxiao Wang and Bin Wen and Fan Yang and Tingting Gao and Di Zhang and Yaowei Wang and Liqiang Nie },
  journal={ arXiv preprint arXiv:2503.09143v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.09143v1 },
  abstract={ AI personal assistants, deployed through robots or wearables, require embodied understanding to collaborate effectively with humans. Current Multimodal Large Language Models (MLLMs) primarily focus on third-person (exocentric) vision, overlooking the unique aspects of first-person (egocentric) videos. Additionally, high acquisition costs limit data size, impairing MLLM performance. To address these challenges, we propose learning the mapping between exocentric and egocentric domains, leveraging the extensive exocentric knowledge within existing MLLMs to enhance egocentric video understanding. To this end, we introduce Ego-ExoClip, a pre-training dataset comprising 1.1M synchronized ego-exo clip-text pairs derived from Ego-Exo4D. Our approach features a progressive training pipeline with three stages: Teacher Self-Preparation, Teacher-Student Guidance, and Student Self-Practice. Additionally, we propose an instruction-tuning data EgoIT from multiple sources to strengthen the model's instruction-following capabilities, along with the EgoBench benchmark comprising eight different tasks for thorough evaluation. Extensive experiments across diverse egocentric tasks reveal that existing MLLMs perform inadequately in egocentric video understanding, while our model significantly outperforms these leading models. }
}

@article{250307825v1,
  title={ Helios 2.0: A Robust, Ultra-Low Power Gesture Recognition System   Optimised for Event-Sensor based Wearables },
  author={ Prarthana Bhattacharyya and Joshua Mitton and Ryan Page and Owen Morgan and Oliver Powell and Benjamin Menzies and Gabriel Homewood and Kemi Jacobs and Paolo Baesso and Taru Muhonen and Richard Vigars and Louis Berridge },
  journal={ arXiv preprint arXiv:2503.07825v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.07825v1 },
  abstract={ We present an advance in wearable technology: a mobile-optimized, real-time, ultra-low-power event camera system that enables natural hand gesture control for smart glasses, dramatically improving user experience. While hand gesture recognition in computer vision has advanced significantly, critical challenges remain in creating systems that are intuitive, adaptable across diverse users and environments, and energy-efficient enough for practical wearable applications. Our approach tackles these challenges through carefully selected microgestures: lateral thumb swipes across the index finger (in both directions) and a double pinch between thumb and index fingertips. These human-centered interactions leverage natural hand movements, ensuring intuitive usability without requiring users to learn complex command sequences. To overcome variability in users and environments, we developed a novel simulation methodology that enables comprehensive domain sampling without extensive real-world data collection. Our power-optimised architecture maintains exceptional performance, achieving F1 scores above 80\\textbackslash{}\% on benchmark datasets featuring diverse users and environments. The resulting models operate at just 6-8 mW when exploiting the Qualcomm Snapdragon Hexagon DSP, with our 2-channel implementation exceeding 70\\textbackslash{}\% F1 accuracy and our 6-channel model surpassing 80\\textbackslash{}\% F1 accuracy across all gesture classes in user studies. These results were achieved using only synthetic training data. This improves on the state-of-the-art for F1 accuracy by 20\\textbackslash{}\% with a power reduction 25x when using DSP. This advancement brings deploying ultra-low-power vision systems in wearable devices closer and opens new possibilities for seamless human-computer interaction. }
}

@article{250306311v1,
  title={ Hybrid CNN-Dilated Self-attention Model Using Inertial and Body-Area   Electrostatic Sensing for Gym Workout Recognition, Counting, and User   Authentification },
  author={ Sizhen Bian and Vitor Fortes Rey and Siyu Yuan and Paul Lukowicz },
  journal={ arXiv preprint arXiv:2503.06311v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.06311v1 },
  abstract={ While human body capacitance (\$HBC\$) has been explored as a novel wearable motion sensing modality, its competence has never been quantitatively demonstrated compared to that of the dominant inertial measurement unit (\$IMU\$) in practical scenarios. This work is thus motivated to evaluate the contribution of \$HBC\$ in wearable motion sensing. A real-life case study, gym workout tracking, is described to assess the effectiveness of \$HBC\$ as a complement to \$IMU\$ in activity recognition. Fifty gym sessions from ten volunteers were collected, bringing a fifty-hour annotated \$IMU\$ and \$HBC\$ dataset. With a hybrid CNN-Dilated neural network model empowered with the self-attention mechanism, \$HBC\$ slightly improves accuracy to the \$IMU\$ for workout recognition and has substantial advantages over \$IMU\$ for repetition counting. This work helps to enhance the understanding of \$HBC\$, a novel wearable motion-sensing modality based on the body-area electrostatic field. All materials presented in this work are open-sourced to promote further study \\textbackslash{}footnote\{https://github.com/zhaxidele/Toolkit-for-HBC-sensing\}. }
}

@article{250306089v1,
  title={ Fish2Mesh Transformer: 3D Human Mesh Recovery from Egocentric Vision },
  author={ David C. Jeong and Aditya Puranik and James Vong and Vrushabh Abhijit Deogirikar and Ryan Fell and Julianna Dietrich and Maria Kyrarini and Christopher Kitts },
  journal={ arXiv preprint arXiv:2503.06089v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.06089v1 },
  abstract={ Egocentric human body estimation allows for the inference of user body pose and shape from a wearable camera's first-person perspective. Although research has used pose estimation techniques to overcome self-occlusions and image distortions caused by head-mounted fisheye images, similar advances in 3D human mesh recovery (HMR) techniques have been limited. We introduce Fish2Mesh, a fisheye-aware transformer-based model designed for 3D egocentric human mesh recovery. We propose an egocentric position embedding block to generate an ego-specific position table for the Swin Transformer to reduce fisheye image distortion. Our model utilizes multi-task heads for SMPL parametric regression and camera translations, estimating 3D and 2D joints as auxiliary loss to support model training. To address the scarcity of egocentric camera data, we create a training dataset by employing the pre-trained 4D-Human model and third-person cameras for weak supervision. Our experiments demonstrate that Fish2Mesh outperforms previous state-of-the-art 3D HMR models. }
}

@article{250305629v1,
  title={ Exploring FMCW Radars and Feature Maps for Activity Recognition: A   Benchmark Study },
  author={ Ali Samimi Fard and Mohammadreza Mashhadigholamali and Samaneh Zolfaghari and Hajar Abedi and Mainak Chakraborty and Luigi Borzì and Masoud Daneshtalab and George Shaker },
  journal={ arXiv preprint arXiv:2503.05629v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.05629v1 },
  abstract={ Human Activity Recognition has gained significant attention due to its diverse applications, including ambient assisted living and remote sensing. Wearable sensor-based solutions often suffer from user discomfort and reliability issues, while video-based methods raise privacy concerns and perform poorly in low-light conditions or long ranges. This study introduces a Frequency-Modulated Continuous Wave radar-based framework for human activity recognition, leveraging a 60 GHz radar and multi-dimensional feature maps. Unlike conventional approaches that process feature maps as images, this study feeds multi-dimensional feature maps -- Range-Doppler, Range-Azimuth, and Range-Elevation -- as data vectors directly into the machine learning (SVM, MLP) and deep learning (CNN, LSTM, ConvLSTM) models, preserving the spatial and temporal structures of the data. These features were extracted from a novel dataset with seven activity classes and validated using two different validation approaches. The ConvLSTM model outperformed conventional machine learning and deep learning models, achieving an accuracy of 90.51\% and an F1-score of 87.31\% on cross-scene validation and an accuracy of 89.56\% and an F1-score of 87.15\% on leave-one-person-out cross-validation. The results highlight the approach's potential for scalable, non-intrusive, and privacy-preserving activity monitoring in real-world scenarios. }
}

@article{250304183v1,
  title={ CrowdHMTware: A Cross-level Co-adaptation Middleware for Context-aware   Mobile DL Deployment },
  author={ Sicong Liu and Bin Guo and Shiyan Luo and Yuzhan Wang and Hao Luo and Cheng Fang and Yuan Xu and Ke Ma and Yao Li and Zhiwen Yu },
  journal={ arXiv preprint arXiv:2503.04183v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.04183v1 },
  abstract={ There are many deep learning (DL) powered mobile and wearable applications today continuously and unobtrusively sensing the ambient surroundings to enhance all aspects of human lives.To enable robust and private mobile sensing, DL models are often deployed locally on resource-constrained mobile devices using techniques such as model compression or offloading.However, existing methods, either front-end algorithm level (i.e. DL model compression/partitioning) or back-end scheduling level (i.e. operator/resource scheduling), cannot be locally online because they require offline retraining to ensure accuracy or rely on manually pre-defined strategies, struggle with dynamic adaptability.The primary challenge lies in feeding back runtime performance from the back-end level to the front-end level optimization decision. Moreover, the adaptive mobile DL model porting middleware with cross-level co-adaptation is less explored, particularly in mobile environments with diversity and dynamics. In response, we introduce CrowdHMTware, a dynamic context-adaptive DL model deployment middleware for heterogeneous mobile devices. It establishes an automated adaptation loop between cross-level functional components, i.e. elastic inference, scalable offloading, and model-adaptive engine, enhancing scalability and adaptability. Experiments with four typical tasks across 15 platforms and a real-world case study demonstrate that CrowdHMTware can effectively scale DL model, offloading, and engine actions across diverse platforms and tasks. It hides run-time system issues from developers, reducing the required developer expertise. }
}

@article{250101350v2,
  title={ Tracking behavioural differences across chronotypes: A case study in   Finland using Oura rings },
  author={ Chandreyee Roy and Kunal Bhattacharya and Kimmo Kaski },
  journal={ arXiv preprint arXiv:2501.01350v2 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2501.01350v2 },
  abstract={ Non-invasive mobile wearables like fitness trackers, smartwatches and rings allow for an easier and relatively less expensive approach to study everyday human behaviour when compared to traditional longitudinal methods. Here we have utilised smart rings manufactured by Oura to obtain granular data from nineteen healthy participants over the time span of one year (October 2023 - September 2024) along with monthly surveys for nine months to track their subjective stress during the study. We have investigated longitudinal sleep and activity patterns of three chronotype groups of participating individuals: morning type (MT), neither type (NT) and evening type (ET). We find that while ET individuals do not seem to lead as healthy life as the MT or NT individuals in terms of overall sleep and activity, they seem to have significantly improved their habits during the duration of the study. The activity in all chronotype groups varies across the year with ET showing an increasing trend. Furthermore, we also show that the Daylight Saving Time changes affect the MT and ET chronotypes, oppositely. Finally, using a mixed-effects regression model, we show that an individual's perceived stress is significantly associated with their time spent in bed during the night time sleep, monthly survey response time, and chronotype, while accounting for individual variability. }
}

@article{250216175v1,
  title={ Mojito: LLM-Aided Motion Instructor with Jitter-Reduced Inertial Tokens },
  author={ Ziwei Shan and Yaoyu He and Chengfeng Zhao and Jiashen Du and Jingyan Zhang and Qixuan Zhang and Jingyi Yu and Lan Xu },
  journal={ arXiv preprint arXiv:2502.16175v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.16175v1 },
  abstract={ Human bodily movements convey critical insights into action intentions and cognitive processes, yet existing multimodal systems primarily focused on understanding human motion via language, vision, and audio, which struggle to capture the dynamic forces and torques inherent in 3D motion. Inertial measurement units (IMUs) present a promising alternative, offering lightweight, wearable, and privacy-conscious motion sensing. However, processing of streaming IMU data faces challenges such as wireless transmission instability, sensor noise, and drift, limiting their utility for long-term real-time motion capture (MoCap), and more importantly, online motion analysis. To address these challenges, we introduce Mojito, an intelligent motion agent that integrates inertial sensing with large language models (LLMs) for interactive motion capture and behavioral analysis. }
}

@article{250215058v1,
  title={ FIP: Endowing Robust Motion Capture on Daily Garment by Fusing Flex and   Inertial Sensors },
  author={ Jiawei Fang and Ruonan Zheng and Yuanyao and Xiaoxia Gao and Chengxu Zuo and Shihui Guo and Yiyue Luo },
  journal={ arXiv preprint arXiv:2502.15058v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.15058v1 },
  abstract={ What if our clothes could capture our body motion accurately? This paper introduces Flexible Inertial Poser (FIP), a novel motion-capturing system using daily garments with two elbow-attached flex sensors and four Inertial Measurement Units (IMUs). To address the inevitable sensor displacements in loose wearables which degrade joint tracking accuracy significantly, we identify the distinct characteristics of the flex and inertial sensor displacements and develop a Displacement Latent Diffusion Model and a Physics-informed Calibrator to compensate for sensor displacements based on such observations, resulting in a substantial improvement in motion capture accuracy. We also introduce a Pose Fusion Predictor to enhance multimodal sensor fusion. Extensive experiments demonstrate that our method achieves robust performance across varying body shapes and motions, significantly outperforming SOTA IMU approaches with a 19.5\% improvement in angular error, a 26.4\% improvement in elbow angular error, and a 30.1\% improvement in positional error. FIP opens up opportunities for ubiquitous human-computer interactions and diverse interactive applications such as Metaverse, rehabilitation, and fitness analysis. }
}

@article{250214430v1,
  title={ Cardiac Evidence Backtracking for Eating Behavior Monitoring using   Collocative Electrocardiogram Imagining },
  author={ Xu-Lu Zhang and Zhen-Qun Yang and Dong-Mei Jiang and Ga Liao and Qing Li and Ramesh Jain and Xiao-Yong Wei },
  journal={ arXiv preprint arXiv:2502.14430v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.14430v1 },
  abstract={ Eating monitoring has remained an open challenge in medical research for years due to the lack of non-invasive sensors for continuous monitoring and the reliable methods for automatic behavior detection. In this paper, we present a pilot study using the wearable 24-hour ECG for sensing and tailoring the sophisticated deep learning for ad-hoc and interpretable detection. This is accomplished using a collocative learning framework in which 1) we construct collocative tensors as pseudo-images from 1D ECG signals to improve the feasibility of 2D image-based deep models; 2) we formulate the cardiac logic of analyzing the ECG data in a comparative way as periodic attention regulators so as to guide the deep inference to collect evidence in a human comprehensible manner; and 3) we improve the interpretability of the framework by enabling the backtracking of evidence with a set of methods designed for Class Activation Mapping (CAM) decoding and decision tree/forest generation. The effectiveness of the proposed framework has been validated on the largest ECG dataset of eating behavior with superior performance over conventional models, and its capacity of cardiac evidence mining has also been verified through the consistency of the evidence it backtracked and that of the previous medical studies. }
}

@article{250217477v1,
  title={ Frequency-Aware Masked Autoencoders for Human Activity Recognition using   Accelerometers },
  author={ Niels R. Lorenzen and Poul J. Jennum and Emmanuel Mignot and Andreas Brink-Kjaer },
  journal={ arXiv preprint arXiv:2502.17477v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.17477v1 },
  abstract={ Wearable accelerometers are widely used for continuous monitoring of physical activity. Supervised machine learning and deep learning algorithms have long been used to extract meaningful activity information from raw accelerometry data, but progress has been hampered by the limited amount of publicly available labeled data. Exploiting large unlabeled datasets using self-supervised pretraining is a relatively new and underexplored approach in the field of human activity recognition (HAR). We used a time-series transformer masked autoencoder (MAE) approach to self-supervised pretraining and propose a novel spectrogram-based loss function named the log-scale mean magnitude (LMM) loss. We compared MAE models pretrained with LMM to one trained with the mean squared error (MSE) loss. We leveraged the large unlabeled UK Biobank accelerometry dataset (n = 109k) for pretraining and evaluated downstream HAR performance using linear classifier in a smaller labelled dataset. We found that pretraining with the LMM loss improved performance compared to a model pretrained with the MSE loss, with balanced accuracies of 0.848 and 0.709, respectively. Further analysis revealed that better convergence of the LMM loss, but not the MSE loss significantly correlated with improved downstream performance (r=-0.61, p=0.04) for balanced accuracy). Finally, we compared our MAE models to the state-of-the-art for HAR, also pretrained on the UK Biobank accelerometry data. Our LMM-pretrained models performed better when finetuned using a linear classifier and performed comparably when finetuned using an LSTM classifier, while MSE-pretrained models consistently underperformed. Our findings demonstrate that the LMM loss is a robust and effective method for pretraining MAE models on accelerometer data for HAR. Future work should explore optimizing loss function combinations and extending our approach to other tasks. }
}

@article{250215767v1,
  title={ Breast Lump Detection and Localization with a Tactile Glove Using Deep   Learning },
  author={ Togzhan Syrymova and Amir Yelenov and Karina Burunchina and Nazgul Abulkhanova and Huseyin Atakan Varol and Juan Antonio Corrales Ramon and Zhanat Kappassov },
  journal={ arXiv preprint arXiv:2502.15767v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.15767v1 },
  abstract={ Breast cancer is the leading cause of mortality among women. Inspection of breasts by palpation is the key to early detection. We aim to create a wearable tactile glove that could localize the lump in breasts using deep learning (DL). In this work, we present our flexible fabric-based and soft wearable tactile glove for detecting the lumps within custom-made silicone breast prototypes (SBPs). SBPs are made of soft silicone that imitates the human skin and the inner part of the breast. Ball-shaped silicone tumors of 1.5-, 1.75- and 2.0-cm diameters are embedded inside to create another set with lumps. Our approach is based on the InceptionTime DL architecture with transfer learning between experienced and non-experienced users. We collected a dataset from 10 naive participants and one oncologist-mammologist palpating SBPs. We demonstrated that the DL model can classify lump presence, size and location with an accuracy of 82.22\%, 67.08\% and 62.63\%, respectively. In addition, we showed that the model adapted to unseen experienced users with an accuracy of 95.01\%, 88.54\% and 82.98\% for lump presence, size and location classification, respectively. This technology can assist inexperienced users or healthcare providers, thus facilitating more frequent routine checks. }
}

@article{250212173v1,
  title={ nanoML for Human Activity Recognition },
  author={ Alan T. L. Bacellar and Mugdha P. Jadhao and Shashank Nag and Priscila M. V. Lima and Felipe M. G. Franca and Lizy K. John },
  journal={ arXiv preprint arXiv:2502.12173v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.12173v1 },
  abstract={ Human Activity Recognition (HAR) is critical for applications in healthcare, fitness, and IoT, but deploying accurate models on resource-constrained devices remains challenging due to high energy and memory demands. This paper demonstrates the application of Differentiable Weightless Neural Networks (DWNs) to HAR, achieving competitive accuracies of 96.34\% and 96.67\% while consuming only 56nJ and 104nJ per sample, with an inference time of just 5ns per sample. The DWNs were implemented and evaluated on an FPGA, showcasing their practical feasibility for energy-efficient hardware deployment. DWNs achieve up to 926,000x energy savings and 260x memory reduction compared to state-of-the-art deep learning methods. These results position DWNs as a nano-machine learning nanoML model for HAR, setting a new benchmark in energy efficiency and compactness for edge and wearable devices, paving the way for ultra-efficient edge AI. }
}

@article{250207598v1,
  title={ Towards spatial computing: recent advances in multimodal natural   interaction for XR headsets },
  author={ Zhimin Wang and Maohang Rao and Shanghua Ye and Weitao Song and Feng Lu },
  journal={ arXiv preprint arXiv:2502.07598v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.07598v1 },
  abstract={ With the widespread adoption of Extended Reality (XR) headsets, spatial computing technologies are gaining increasing attention. Spatial computing enables interaction with virtual elements through natural input methods such as eye tracking, hand gestures, and voice commands, thus placing natural human-computer interaction at its core. While previous surveys have reviewed conventional XR interaction techniques, recent advancements in natural interaction, particularly driven by artificial intelligence (AI) and large language models (LLMs), have introduced new paradigms and technologies. In this paper, we review research on multimodal natural interaction for wearable XR, focusing on papers published between 2022 and 2024 in six top venues: ACM CHI, UIST, IMWUT (Ubicomp), IEEE VR, ISMAR, and TVCG. We classify and analyze these studies based on application scenarios, operation types, and interaction modalities. This analysis provides a structured framework for understanding how researchers are designing advanced natural interaction techniques in XR. Based on these findings, we discuss the challenges in natural interaction techniques and suggest potential directions for future research. This review provides valuable insights for researchers aiming to design natural and efficient interaction systems for XR, ultimately contributing to the advancement of spatial computing. }
}

@article{250205963v1,
  title={ Redefining Robot Generalization Through Interactive Intelligence },
  author={ Sharmita Dey },
  journal={ arXiv preprint arXiv:2502.05963v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.05963v1 },
  abstract={ Recent advances in large-scale machine learning have produced high-capacity foundation models capable of adapting to a broad array of downstream tasks. While such models hold great promise for robotics, the prevailing paradigm still portrays robots as single, autonomous decision-makers, performing tasks like manipulation and navigation, with limited human involvement. However, a large class of real-world robotic systems, including wearable robotics (e.g., prostheses, orthoses, exoskeletons), teleoperation, and neural interfaces, are semiautonomous, and require ongoing interactive coordination with human partners, challenging single-agent assumptions. In this position paper, we argue that robot foundation models must evolve to an interactive multi-agent perspective in order to handle the complexities of real-time human-robot co-adaptation. We propose a generalizable, neuroscience-inspired architecture encompassing four modules: (1) a multimodal sensing module informed by sensorimotor integration principles, (2) an ad-hoc teamwork model reminiscent of joint-action frameworks in cognitive science, (3) a predictive world belief model grounded in internal model theories of motor control, and (4) a memory/feedback mechanism that echoes concepts of Hebbian and reinforcement-based plasticity. Although illustrated through the lens of cyborg systems, where wearable devices and human physiology are inseparably intertwined, the proposed framework is broadly applicable to robots operating in semi-autonomous or interactive contexts. By moving beyond single-agent designs, our position emphasizes how foundation models in robotics can achieve a more robust, personalized, and anticipatory level of performance. }
}

@article{241107644v2,
  title={ Human Arm Pose Estimation with a Shoulder-worn Force-Myography Device   for Human-Robot Interaction },
  author={ Rotem Atari and Eran Bamani and Avishai Sintov },
  journal={ arXiv preprint arXiv:2411.07644v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2411.07644v2 },
  abstract={ Accurate human pose estimation is essential for effective Human-Robot Interaction (HRI). By observing a user's arm movements, robots can respond appropriately, whether it's providing assistance or avoiding collisions. While visual perception offers potential for human pose estimation, it can be hindered by factors like poor lighting or occlusions. Additionally, wearable inertial sensors, though useful, require frequent calibration as they do not provide absolute position information. Force-myography (FMG) is an alternative approach where muscle perturbations are externally measured. It has been used to observe finger movements, but its application to full arm state estimation is unexplored. In this letter, we investigate the use of a wearable FMG device that can observe the state of the human arm for real-time applications of HRI. We propose a Transformer-based model to map FMG measurements from the shoulder of the user to the physical pose of the arm. The model is also shown to be transferable to other users with limited decline in accuracy. Through real-world experiments with a robotic arm, we demonstrate collision avoidance without relying on visual perception. }
}

@article{240603857v3,
  title={ MuJo: Multimodal Joint Feature Space Learning for Human Activity   Recognition },
  author={ Stefan Gerd Fritsch and Cennet Oguz and Vitor Fortes Rey and Lala Ray and Maximilian Kiefer-Emmanouilidis and Paul Lukowicz },
  journal={ arXiv preprint arXiv:2406.03857v3 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2406.03857v3 },
  abstract={ Human activity recognition (HAR) is a long-standing problem in artificial intelligence with applications in a broad range of areas, including healthcare, sports and fitness, security, and more. The performance of HAR in real-world settings is strongly dependent on the type and quality of the input signal that can be acquired. Given an unobstructed, high-quality camera view of a scene, computer vision systems, in particular in conjunction with foundation models, can today fairly reliably distinguish complex activities. On the other hand, recognition using modalities such as wearable sensors (which are often more broadly available, e.g., in mobile phones and smartwatches) is a more difficult problem, as the signals often contain less information and labeled training data is more difficult to acquire. To alleviate the need for labeled data, we introduce our comprehensive Fitness Multimodal Activity Dataset (FiMAD) in this work, which can be used with the proposed pre-training method MuJo (Multimodal Joint Feature Space Learning) to enhance HAR performance across various modalities. FiMAD was created using YouTube fitness videos and contains parallel video, language, pose, and simulated IMU sensor data. MuJo utilizes this dataset to learn a joint feature space for these modalities. We show that classifiers pre-trained on FiMAD can increase the performance on real HAR datasets such as MM-Fit, MyoGym, MotionSense, and MHEALTH. For instance, on MM-Fit, we achieve a Macro F1-Score of up to 0.855 when fine-tuning on only 2\% of the training data and 0.942 when utilizing the complete training set for classification tasks. We compare our approach with other self-supervised ones and show that, unlike them, ours consistently improves compared to the baseline network performance while also providing better data efficiency. }
}

@article{250203364v1,
  title={ Scaling laws in wearable human activity recognition },
  author={ Tom Hoddes and Alex Bijamov and Saket Joshi and Daniel Roggen and Ali Etemad and Robert Harle and David Racz },
  journal={ arXiv preprint arXiv:2502.03364v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.03364v1 },
  abstract={ Many deep architectures and self-supervised pre-training techniques have been proposed for human activity recognition (HAR) from wearable multimodal sensors. Scaling laws have the potential to help move towards more principled design by linking model capacity with pre-training data volume. Yet, scaling laws have not been established for HAR to the same extent as in language and vision. By conducting an exhaustive grid search on both amount of pre-training data and Transformer architectures, we establish the first known scaling laws for HAR. We show that pre-training loss scales with a power law relationship to amount of data and parameter count and that increasing the number of users in a dataset results in a steeper improvement in performance than increasing data per user, indicating that diversity of pre-training data is important, which contrasts to some previously reported findings in self-supervised HAR. We show that these scaling laws translate to downstream performance improvements on three HAR benchmark datasets of postures, modes of locomotion and activities of daily living: UCI HAR and WISDM Phone and WISDM Watch. Finally, we suggest some previously published works should be revisited in light of these scaling laws with more adequate model capacities. }
}

@article{250116177v1,
  title={ BAG: Body-Aligned 3D Wearable Asset Generation },
  author={ Zhongjin Luo and Yang Li and Mingrui Zhang and Senbo Wang and Han Yan and Xibin Song and Taizhang Shang and Wei Mao and Hongdong Li and Xiaoguang Han and Pan Ji },
  journal={ arXiv preprint arXiv:2501.16177v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2501.16177v1 },
  abstract={ While recent advancements have shown remarkable progress in general 3D shape generation models, the challenge of leveraging these approaches to automatically generate wearable 3D assets remains unexplored. To this end, we present BAG, a Body-aligned Asset Generation method to output 3D wearable asset that can be automatically dressed on given 3D human bodies. This is achived by controlling the 3D generation process using human body shape and pose information. Specifically, we first build a general single-image to consistent multiview image diffusion model, and train it on the large Objaverse dataset to achieve diversity and generalizability. Then we train a Controlnet to guide the multiview generator to produce body-aligned multiview images. The control signal utilizes the multiview 2D projections of the target human body, where pixel values represent the XYZ coordinates of the body surface in a canonical space. The body-conditioned multiview diffusion generates body-aligned multiview images, which are then fed into a native 3D diffusion model to produce the 3D shape of the asset. Finally, by recovering the similarity transformation using multiview silhouette supervision and addressing asset-body penetration with physics simulators, the 3D asset can be accurately fitted onto the target human body. Experimental results demonstrate significant advantages over existing methods in terms of image prompt-following capability, shape diversity, and shape quality. Our project page is available at https://bag-3d.github.io/. }
}

@article{250115330v1,
  title={ Assessing the Impact of Sampling Irregularity in Time Series Data: Human   Activity Recognition As A Case Study },
  author={ Mengxi Liu and Daniel Geißler and Sizhen Bian and Bo Zhou and Paul Lukowicz },
  journal={ arXiv preprint arXiv:2501.15330v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2501.15330v1 },
  abstract={ Human activity recognition (HAR) ideally relies on data from wearable or environment-instrumented sensors sampled at regular intervals, enabling standard neural network models optimized for consistent time-series data as input. However, real-world sensor data often exhibits irregular sampling due to, for example, hardware constraints, power-saving measures, or communication delays, posing challenges for deployed static HAR models. This study assesses the impact of sampling irregularities on HAR by simulating irregular data through two methods: introducing slight inconsistencies in sampling intervals (timestamp variations) to mimic sensor jitter, and randomly removing data points (random dropout) to simulate missing values due to packet loss or sensor failure. We evaluate both discrete-time neural networks and continuous-time neural networks, which are designed to handle continuous-time data, on three public datasets. We demonstrate that timestamp variations do not significantly affect the performance of discrete-time neural networks, and the continuous-time neural network is also ineffective in addressing the challenges posed by irregular sampling, possibly due to limitations in modeling complex temporal patterns with missing data. Our findings underscore the necessity for new models or approaches that can robustly handle sampling irregularity in time-series data, like the reading in human activity recognition, paving the way for future research in this domain. }
}

@article{250114423v1,
  title={ Human Activity Recognition with a 6.5 GHz Reconfigurable Intelligent   Surface for Wi-Fi 6E },
  author={ Nuno Paulino and Mariana Oliveira and Francisco Ribeiro and Luís Outeiro and Pedro A. Lopes and Francisco Vilarinho and Sofia Inácio and Luís M. Pessoa },
  journal={ arXiv preprint arXiv:2501.14423v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2501.14423v1 },
  abstract={ Human Activity Recognition (HAR) is the identification and classification of static and dynamic human activities, which find applicability in domains like healthcare, entertainment, security, and cyber-physical systems. Traditional HAR approaches rely on wearable sensors, vision-based systems, or ambient sensing, each with inherent limitations such as privacy concerns or restricted sensing conditions. Recently, Radio Frequency (RF)-based HAR has emerged, relying on the interaction of RF signals with people to infer activities. Reconfigurable Intelligent Surfaces (RISs) offers significant potential in this domain by enabling dynamic control over the wireless environment, thus enhancing the information extracted from RF signals. We present an Hand Gesture Recognition (HGR) approach that employs our own 6.5 GHz RIS design to manipulate the RF medium in an area of interest. We validate the capability of our RIS to control the medium by characterizing its steering response, and further we gather and publish a dataset for HGR classification for three different hand gestures. By employing two Convolutional Neural Networks (CNNs) models trained on data gathered under random and optimized RIS configuration sequences, we achieved classification accuracies exceeding 90\%. }
}

@article{250107567v2,
  title={ Knit Happens: Designing the Mechanics of Machine Knitting },
  author={ Cosima du Pasquier and Sehui Jeong and Pan Liu and Susan Williams and Allison M. Okamura and Skylar Tibbits and Tian Chen },
  journal={ arXiv preprint arXiv:2501.07567v2 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2501.07567v2 },
  abstract={ Knit fabrics are mechanically durable and tough while sufficiently flexible to conform to curved substrates such as the human body. Recent advancements in industrial knitting enable unprecedented control over the pattern design and functionality of next generation knit fabrics. However, the ability to leverage this granular control to predict and tune the mechanical behavior of these fabrics remains limited due to their complex hierarchical and entangled microstructure. This study establishes a comprehensive experimental and numerical framework to characterize and model the mechanical properties of industrial knitted fabrics. By integrating precise experiments, finite element modeling, and a strain energy-based homogenization approach, we provide an accurate demonstration of how stitch length, pattern, and yarn material govern the anisotropic mechanical response of knitted fabrics. In doing so, we establish clear quantitative links between these parameters and key material properties, including stiffness and anisotropy, that emerge directly from our energy-based model. Recognizing that industrial knitting systems can produce fabrics with spatially varying parameters, we extend our framework to predict heterogeneous knits. We show that material transitions have minimal impact on the fabric's overall mechanical response, so heterogeneous fabrics can be modeled with our framework as patchworks of homogeneous samples. We design the first industrially knit sleeve explicitly optimized for both fit and function, using heterogeneous patterns that conform to target arm musculature and ensure uniform stress distribution. This work bridges the gap between computational modeling and scalable manufacturing, unlocking new possibilities for wearable devices, assistive textiles, and functional applications. }
}

@article{240110232v2,
  title={ ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative   Modeling of Human-Object Interactions },
  author={ Jeonghwan Kim and Jisoo Kim and Jeonghyeon Na and Hanbyul Joo },
  journal={ arXiv preprint arXiv:2401.10232v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2401.10232v2 },
  abstract={ To enable machines to understand the way humans interact with the physical world in daily life, 3D interaction signals should be captured in natural settings, allowing people to engage with multiple objects in a range of sequential and casual manipulations. To achieve this goal, we introduce our ParaHome system designed to capture dynamic 3D movements of humans and objects within a common home environment. Our system features a multi-view setup with 70 synchronized RGB cameras, along with wearable motion capture devices including an IMU-based body suit and hand motion capture gloves. By leveraging the ParaHome system, we collect a new human-object interaction dataset, including 486 minutes of sequences across 207 captures with 38 participants, offering advancements with three key aspects: (1) capturing body motion and dexterous hand manipulation motion alongside multiple objects within a contextual home environment; (2) encompassing sequential and concurrent manipulations paired with text descriptions; and (3) including articulated objects with multiple parts represented by 3D parameterized models. We present detailed design justifications for our system, and perform key generative modeling experiments to demonstrate the potential of our dataset. }
}

@article{250107224v1,
  title={ Touched by ChatGPT: Using an LLM to Drive Affective Tactile Interaction },
  author={ Qiaoqiao Ren and Tony Belpaeme },
  journal={ arXiv preprint arXiv:2501.07224v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2501.07224v1 },
  abstract={ Touch is a fundamental aspect of emotion-rich communication, playing a vital role in human interaction and offering significant potential in human-robot interaction. Previous research has demonstrated that a sparse representation of human touch can effectively convey social tactile signals. However, advances in human-robot tactile interaction remain limited, as many humanoid robots possess simplistic capabilities, such as only opening and closing their hands, restricting nuanced tactile expressions. In this study, we explore how a robot can use sparse representations of tactile vibrations to convey emotions to a person. To achieve this, we developed a wearable sleeve integrated with a 5x5 grid of vibration motors, enabling the robot to communicate diverse tactile emotions and gestures. Using chain prompts within a Large Language Model (LLM), we generated distinct 10-second vibration patterns corresponding to 10 emotions (e.g., happiness, sadness, fear) and 6 touch gestures (e.g., pat, rub, tap). Participants (N = 32) then rated each vibration stimulus based on perceived valence and arousal. People are accurate at recognising intended emotions, a result which aligns with earlier findings. These results highlight the LLM's ability to generate emotional haptic data and effectively convey emotions through tactile signals. By translating complex emotional and tactile expressions into vibratory patterns, this research demonstrates how LLMs can enhance physical interaction between humans and robots. }
}

@article{250106940v1,
  title={ Collaborative Human Activity Recognition with Passive Inter-Body   Electrostatic Field },
  author={ Sizhen Bian and Vitor Fortes Rey and Siyu Yuan and Paul Lukowicz },
  journal={ arXiv preprint arXiv:2501.06940v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2501.06940v1 },
  abstract={ The passive body-area electrostatic field has recently been aspiringly explored for wearable motion sensing, harnessing its two thrilling characteristics: full-body motion sensitivity and environmental sensitivity, which potentially empowers human activity recognition both independently and jointly from a single sensing front-end and theoretically brings significant competition against traditional inertial sensor that is incapable in environmental variations sensing. While most works focus on exploring the electrostatic field of a single body as the target, this work, for the first time, quantitatively evaluates the mutual effect of inter-body electrostatic fields and its contribution to collaborative activity recognition. A wearable electrostatic field sensing front-end and wrist-worn prototypes are built, and a sixteen-hour, manually annotated dataset is collected, involving an experiment of manipulating objects both independently and collaboratively. A regression model is finally used to recognize the collaborative activities among users. Despite the theoretical advantages of the body electrostatic field, the recognition of both single and collaborative activities shows unanticipated less-competitive recognition performance compared with the accelerometer. However, It is worth mentioning that this novel sensing modality improves the recognition F-score of user collaboration by 16\\textbackslash{}\% in the fusion result of the two wearable motion sensing modalities, demonstrating the potential of bringing body electrostatic field as a complementary power-efficient signal for collaborative activity tracking using wearables. }
}

@article{241114452v2,
  title={ Past, Present, and Future of Sensor-Based Human Activity Recognition   Using Wearables: A Surveying Tutorial on a Still Challenging Task },
  author={ Harish Haresamudram and Chi Ian Tang and Sungho Suh and Paul Lukowicz and Thomas Ploetz },
  journal={ arXiv preprint arXiv:2411.14452v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2411.14452v2 },
  abstract={ In the many years since the inception of wearable sensor-based Human Activity Recognition (HAR), a wide variety of methods have been introduced and evaluated for their ability to recognize activities. Substantial gains have been made since the days of hand-crafting heuristics as features, yet, progress has seemingly stalled on many popular benchmarks, with performance falling short of what may be considered 'sufficient'-- despite the increase in computational power and scale of sensor data, as well as rising complexity in techniques being employed. The HAR community approaches a new paradigm shift, this time incorporating world knowledge from foundational models. In this paper, we take stock of sensor-based HAR -- surveying it from its beginnings to the current state of the field, and charting its future. This is accompanied by a hands-on tutorial, through which we guide practitioners in developing HAR systems for real-world application scenarios. We provide a compendium for novices and experts alike, of methods that aim at finally solving the activity recognition problem. }
}

@article{250100723v1,
  title={ Metabolic energy expenditure for time-varying isometric forces },
  author={ Sriram Sekaripuram Muralidhar and Kristen Heitman and Samuel C. Walcott and Manoj Srinivasan },
  journal={ arXiv preprint arXiv:2501.00723v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2501.00723v1 },
  abstract={ Muscles consume metabolic energy (ATP) to produce force. A mathematical model for energy expenditure can be useful in estimating real-time costs of movements or to predict energy optimal movements. Metabolic cost models developed so far have predominantly aimed at dynamic movement tasks, where mechanical work dominates. Further, while it is known that both force magnitude and rate of change of force (force rate) affect metabolic cost, it is not known how these terms interact, or if the force rate dependence can be a consequence of the force dependence. Here, we performed extensive human subject experiments, involving each subject over 5 hours of metabolic trials, which systematically changed the mean forces and forces rates so as to characterize a holistic relation for metabolic cost based on both force and force rate -- or analogously, torque and torque rate. Our experiments involved humans producing symmetric or asymmetric sinusoidal forces with different means, amplitudes, frequencies, and rise and fall periods. We showed that the metabolic cost can be well-approximated by a sum of power law functions of torque and torque rate. We found that the metabolic cost scales non-linearly with joint torque (with exponent = 1.36) and non-linearly with torque rate (with exponent = 2.5). Surprisingly, the data suggested that the cost was roughly four times higher for decreasing the torque than increasing, mirroring the analogous ratio between the cost of positive and negative work. Using these metabolic cost relations, we show that if the metabolic cost scales with particular exponents with muscle force and force rates, the same exponents will be observed in multi-joint tasks with multiple muscles. Our new metabolic cost model involving both force and force rate will potentially allow better predictions of energy optimal movements and thus inform wearable robot design and analysis. }
}

@article{241220297v1,
  title={ FaGeL: Fabric LLMs Agent empowered Embodied Intelligence Evolution with   Autonomous Human-Machine Collaboration },
  author={ Jia Liu and Min Chen },
  journal={ arXiv preprint arXiv:2412.20297v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.20297v1 },
  abstract={ Recent advancements in Large Language Models (LLMs) have enhanced the reasoning capabilities of embodied agents, driving progress toward AGI-powered robotics. While LLMs have been applied to tasks like semantic reasoning and task generalization, their potential in open physical space exploration remains underexplored. This paper introduces FaGeL (Fabric aGent empowered by embodied intelligence with LLMs), an embodied agent integrating smart fabric technology for seamless, non-intrusive human-agent interaction. FaGeL autonomously generates tasks using multimodal data from wearable and ambient sensors, refining its behavior based on implicit human feedback in generated text, without explicit ratings or preferences. We also introduce a token-level saliency map to visualize LLM fine-tuning, enhancing the interpretability of token-level alignment. The system leverages dual feedback mechanisms to improve token-level alignment and addresses challenges in non-intrusive human-machine interaction and cognition evolution. Our contributions include FaGeL's development, the DualCUT algorithm for AI alignment, and experimental validation in cooperative tasks, demonstrating FaGeL's ability to adapt and evolve autonomously through implicit feedback. In the future, we plan to explore FaGeL's scalability in dynamic environments and its integration with other AI systems to develop AGI agents that adapt seamlessly to diverse human needs. }
}

@article{250315488v1,
  title={ Human-AI Collaboration for Wearable Technology Component Standardization },
  author={ Andrew M. Lydner },
  journal={ arXiv preprint arXiv:2503.15488v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2503.15488v1 },
  abstract={ Due to the multidisciplinary nature of wearable technology, the industry faces potential limitations in innovation. The wearable technology industry is still in its infancy and increased applicable use faces stagnation in the despite the plethora of technologies that have been largely wrist worn. This could be a result of the lack of multidisciplinary expert knowledge disseminating through the industry. Unlike other technologies which have standardizations and processes for how they are developed, wearable technologies exist in a realm of perpetual change as given the various materials and subcomponents that continue to be developed. It is essential that expert opinions form a collaborative foundation, and even more so that intelligent systems foster that collaboration. The caveat though, is likeliness of these artificial intelligence (AI) collaboration tools to be utilized by industry experts. Mental model development for AI tool usage could be applied to wearable technology innovation in this regard, thus the goal of this paper and focus of research. }
}

@article{241215980v1,
  title={ iRadar: Synthesizing Millimeter-Waves from Wearable Inertial Inputs for   Human Gesture Sensing },
  author={ Huanqi Yang and Mingda Han and Xinyue Li and Di Duan and Tianxing Li and Weitao Xu },
  journal={ arXiv preprint arXiv:2412.15980v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.15980v1 },
  abstract={ Millimeter-wave (mmWave) radar-based gesture recognition is gaining attention as a key technology to enable intuitive human-machine interaction. Nevertheless, the significant challenge lies in obtaining large-scale, high-quality mmWave gesture datasets. To tackle this problem, we present iRadar, a novel cross-modal gesture recognition framework that employs Inertial Measurement Unit (IMU) data to synthesize the radar signals generated by the corresponding gestures. The key idea is to exploit the IMU signals, which are commonly available in contemporary wearable devices, to synthesize the radar signals that would be produced if the same gesture was performed in front of a mmWave radar. However, several technical obstacles must be overcome due to the differences between mmWave and IMU signals, the noisy gesture sensing of mmWave radar, and the dynamics of human gestures. Firstly, we develop a method for processing IMU and mmWave data to extract critical gesture features. Secondly, we propose a diffusion-based IMU-to-radar translation model that accurately transforms IMU data into mmWave data. Lastly, we devise a novel transformer model to enhance gesture recognition performance. We thoroughly evaluate iRadar, involving 18 gestures and 30 subjects in three scenarios, using five wearable devices. Experimental results demonstrate that iRadar consistently achieves 99.82\% Top-3 accuracy across diverse scenarios. }
}

@article{241210710v2,
  title={ Virtual Trial Room with Computer Vision and Machine Learning },
  author={ Tulashi Prasad Joshi and Amrendra Kumar Yadav and Arjun Chhetri and Suraj Agrahari and Umesh Kanta Ghimire },
  journal={ arXiv preprint arXiv:2412.10710v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.10710v2 },
  abstract={ Online shopping has revolutionized the retail industry, providing customers with convenience and accessibility. However, customers often hesitate to purchase wearable products such as watches, jewelry, glasses, shoes, and clothes due to the lack of certainty regarding fit and suitability. This leads to significant return rates, causing problems for both customers and vendors. To address this issue, a platform called the Virtual Trial Room with Computer Vision and Machine Learning is designed which enables customers to easily check whether a product will fit and suit them or not. To achieve this, an AI-generated 3D model of the human head was created from a single 2D image using the DECA model. This 3D model was then superimposed with a custom-made 3D model of glass which is based on real-world measurements and fitted over the human head. To replicate the real-world look and feel, the model was retouched with textures, lightness, and smoothness. Furthermore, a full-stack application was developed utilizing various fornt-end and back-end technologies. This application enables users to view 3D-generated results on the website, providing an immersive and interactive experience. }
}

@article{241212209v1,
  title={ Challenges and Opportunities Associated with Technology Driven   Biomechanical Simulations },
  author={ Zartasha Mustansar and Haider Ali and Lee Margetts and Saad Ahmad Khan and Salma Sherbaz and Rehan Zafar Paracha },
  journal={ arXiv preprint arXiv:2412.12209v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.12209v1 },
  abstract={ This paper presents the principal challenges and opportunities associated with computational biomechanics research. The underlying cognitive control involved in the process of human motion is inherently complex, dynamic, multidimensional, and highly non-linear. The dynamics produced by the internal and external forces and the body's ability to react to them is biomechanics. Complex and non-rigid bodies, needs a lot of computing power and systems to execute however, in the absence of adequate resources, one may rely on new technology, machine learning tools and model order reduction approaches. It is also believed that machine learning approaches can enable us to embrace this complexity, if we could use three arms of ML i.e. predictive modeling, classification, and dimensionality reduction. Biomechanics, since it deals with motion and mobility come with a huge set of data over time. Using computational (Computer Solvers), Numerical approaches (MOR) and technological advances (Wearable sensors), can let us develop computationally inexpensive frameworks for biomechanics focused studies dealing with a huge amount of data. A lot of misunderstanding arises because of extensive data, standardization of the tools to process this, database for the material property definitions, validation and verification of biomechanical models and analytical tools to model various phenomena using computational and modelling techniques. Study of biomechanics through computational simulations can improve the prevention and treatment of diseases, predict the injury to reduce the risk and hence can strengthen pivotal sectors like sports and lifestyle. This is why we choose to present all those challenges and problems associated with biomechanical simulation with complex geometries fail so as to help improve, analysis, performance and design for better lifestyle. }
}

@article{241208507v1,
  title={ Strategies and Challenges of Efficient White-Box Training for Human   Activity Recognition },
  author={ Daniel Geissler and Bo Zhou and Paul Lukowicz },
  journal={ arXiv preprint arXiv:2412.08507v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.08507v1 },
  abstract={ Human Activity Recognition using time-series data from wearable sensors poses unique challenges due to complex temporal dependencies, sensor noise, placement variability, and diverse human behaviors. These factors, combined with the nontransparent nature of black-box Machine Learning models impede interpretability and hinder human comprehension of model behavior. This paper addresses these challenges by exploring strategies to enhance interpretability through white-box approaches, which provide actionable insights into latent space dynamics and model behavior during training. By leveraging human intuition and expertise, the proposed framework improves explainability, fosters trust, and promotes transparent Human Activity Recognition systems. A key contribution is the proposal of a Human-in-the-Loop framework that enables dynamic user interaction with models, facilitating iterative refinements to enhance performance and efficiency. Additionally, we investigate the usefulness of Large Language Model as an assistance to provide users with guidance for interpreting visualizations, diagnosing issues, and optimizing workflows. Together, these contributions present a scalable and efficient framework for developing interpretable and accessible Human Activity Recognition systems. }
}

@article{241207956v1,
  title={ Reciprocal Learning of Intent Inferral with Augmented Visual Feedback   for Stroke },
  author={ Jingxi Xu and Ava Chen and Lauren Winterbottom and Joaquin Palacios and Preethika Chivukula and Dawn M. Nilsen and Joel Stein and Matei Ciocarlie },
  journal={ arXiv preprint arXiv:2412.07956v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.07956v1 },
  abstract={ Intent inferral, the process by which a robotic device predicts a user's intent from biosignals, offers an effective and intuitive way to control wearable robots. Classical intent inferral methods treat biosignal inputs as unidirectional ground truths for training machine learning models, where the internal state of the model is not directly observable by the user. In this work, we propose reciprocal learning, a bidirectional paradigm that facilitates human adaptation to an intent inferral classifier. Our paradigm consists of iterative, interwoven stages that alternate between updating machine learning models and guiding human adaptation with the use of augmented visual feedback. We demonstrate this paradigm in the context of controlling a robotic hand orthosis for stroke, where the device predicts open, close, and relax intents from electromyographic (EMG) signals and provides appropriate assistance. We use LED progress-bar displays to communicate to the user the predicted probabilities for open and close intents by the classifier. Our experiments with stroke subjects show reciprocal learning improving performance in a subset of subjects (two out of five) without negatively impacting performance on the others. We hypothesize that, during reciprocal learning, subjects can learn to reproduce more distinguishable muscle activation patterns and generate more separable biosignals. }
}

@article{241206389v1,
  title={ Exploring the Impact of Synthetic Data on Human Gesture Recognition   Tasks Using GANs },
  author={ George Kontogiannis and Pantelis Tzamalis and Sotiris Nikoletseas },
  journal={ arXiv preprint arXiv:2412.06389v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.06389v1 },
  abstract={ In the evolving domain of Human Activity Recognition (HAR) using Internet of Things (IoT) devices, there is an emerging interest in employing Deep Generative Models (DGMs) to address data scarcity, enhance data quality, and improve classification metrics scores. Among these types of models, Generative Adversarial Networks (GANs) have arisen as a powerful tool for generating synthetic data that mimic real-world scenarios with high fidelity. However, Human Gesture Recognition (HGR), a subset of HAR, particularly in healthcare applications, using time series data such as allergic gestures, remains highly unexplored.   In this paper, we examine and evaluate the performance of two GANs in the generation of synthetic gesture motion data that compose a part of an open-source benchmark dataset. The data is related to the disease identification domain and healthcare, specifically to allergic rhinitis. We also focus on these AI models' performance in terms of fidelity, diversity, and privacy. Furthermore, we examine the scenario if the synthetic data can substitute real data, in training scenarios and how well models trained on synthetic data can be generalized for the allergic rhinitis gestures. In our work, these gestures are related to 6-axes accelerometer and gyroscope data, serving as multi-variate time series instances, and retrieved from smart wearable devices. To the best of our knowledge, this study is the first to explore the feasibility of synthesizing motion gestures for allergic rhinitis from wearable IoT device data using Generative Adversarial Networks (GANs) and testing their impact on the generalization of gesture recognition systems. It is worth noting that, even if our method has been applied to a specific category of gestures, it is designed to be generalized and can be deployed also to other motion data in the HGR domain. }
}

@article{241205277v1,
  title={ Text to Blind Motion },
  author={ Hee Jae Kim and Kathakoli Sengupta and Masaki Kuribayashi and Hernisa Kacorri and Eshed Ohn-Bar },
  journal={ arXiv preprint arXiv:2412.05277v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.05277v1 },
  abstract={ People who are blind perceive the world differently than those who are sighted, which can result in distinct motion characteristics. For instance, when crossing at an intersection, blind individuals may have different patterns of movement, such as veering more from a straight path or using touch-based exploration around curbs and obstacles. These behaviors may appear less predictable to motion models embedded in technologies such as autonomous vehicles. Yet, the ability of 3D motion models to capture such behavior has not been previously studied, as existing datasets for 3D human motion currently lack diversity and are biased toward people who are sighted. In this work, we introduce BlindWays, the first multimodal motion benchmark for pedestrians who are blind. We collect 3D motion data using wearable sensors with 11 blind participants navigating eight different routes in a real-world urban setting. Additionally, we provide rich textual descriptions that capture the distinctive movement characteristics of blind pedestrians and their interactions with both the navigation aid (e.g., a white cane or a guide dog) and the environment. We benchmark state-of-the-art 3D human prediction models, finding poor performance with off-the-shelf and pre-training-based methods for our novel task. To contribute toward safer and more reliable systems that can seamlessly reason over diverse human movements in their environments, our text-and-motion benchmark is available at https://blindways.github.io. }
}

@article{241202725v1,
  title={ emg2pose: A Large and Diverse Benchmark for Surface Electromyographic   Hand Pose Estimation },
  author={ Sasha Salter and Richard Warren and Collin Schlager and Adrian Spurr and Shangchen Han and Rohin Bhasin and Yujun Cai and Peter Walkington and Anuoluwapo Bolarinwa and Robert Wang and Nathan Danielson and Josh Merel and Eftychios Pnevmatikakis and Jesse Marshall },
  journal={ arXiv preprint arXiv:2412.02725v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.02725v1 },
  abstract={ Hands are the primary means through which humans interact with the world. Reliable and always-available hand pose inference could yield new and intuitive control schemes for human-computer interactions, particularly in virtual and augmented reality. Computer vision is effective but requires one or multiple cameras and can struggle with occlusions, limited field of view, and poor lighting. Wearable wrist-based surface electromyography (sEMG) presents a promising alternative as an always-available modality sensing muscle activities that drive hand motion. However, sEMG signals are strongly dependent on user anatomy and sensor placement, and existing sEMG models have required hundreds of users and device placements to effectively generalize. To facilitate progress on sEMG pose inference, we introduce the emg2pose benchmark, the largest publicly available dataset of high-quality hand pose labels and wrist sEMG recordings. emg2pose contains 2kHz, 16 channel sEMG and pose labels from a 26-camera motion capture rig for 193 users, 370 hours, and 29 stages with diverse gestures - a scale comparable to vision-based hand pose datasets. We provide competitive baselines and challenging tasks evaluating real-world generalization scenarios: held-out users, sensor placements, and stages. emg2pose provides the machine learning community a platform for exploring complex generalization problems, holding potential to significantly enhance the development of sEMG-based human-computer interactions. }
}

@article{240213949v2,
  title={ Generating Realistic Arm Movements in Reinforcement Learning: A   Quantitative Comparison of Reward Terms and Task Requirements },
  author={ Jhon P. F. Charaja and Isabell Wochner and Pierre Schumacher and Winfried Ilg and Martin Giese and Christophe Maufroy and Andreas Bulling and Syn Schmitt and Georg Martius and Daniel F. B. Haeufle },
  journal={ arXiv preprint arXiv:2402.13949v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2402.13949v2 },
  abstract={ The mimicking of human-like arm movement characteristics involves the consideration of three factors during control policy synthesis: (a) chosen task requirements, (b) inclusion of noise during movement execution and (c) chosen optimality principles. Previous studies showed that when considering these factors (a-c) individually, it is possible to synthesize arm movements that either kinematically match the experimental data or reproduce the stereotypical triphasic muscle activation pattern. However, to date no quantitative comparison has been made on how realistic the arm movement generated by each factor is; as well as whether a partial or total combination of all factors results in arm movements with human-like kinematic characteristics and a triphasic muscle pattern. To investigate this, we used reinforcement learning to learn a control policy for a musculoskeletal arm model, aiming to discern which combination of factors (a-c) results in realistic arm movements according to four frequently reported stereotypical characteristics. Our findings indicate that incorporating velocity and acceleration requirements into the reaching task, employing reward terms that encourage minimization of mechanical work, hand jerk, and control effort, along with the inclusion of noise during movement, leads to the emergence of realistic human arm movements in reinforcement learning. We expect that the gained insights will help in the future to better predict desired arm movements and corrective forces in wearable assistive devices. }
}

@article{241112778v1,
  title={ Lucia: A Temporal Computing Platform for Contextual Intelligence },
  author={ Weizhe Lin and Junxiao Shen },
  journal={ arXiv preprint arXiv:2411.12778v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2411.12778v1 },
  abstract={ The rapid evolution of artificial intelligence, especially through multi-modal large language models, has redefined user interactions, enabling responses that are contextually rich and human-like. As AI becomes an integral part of daily life, a new frontier has emerged: developing systems that not only understand spatial and sensory data but also interpret temporal contexts to build long-term, personalized memories. This report introduces Lucia, an open-source Temporal Computing Platform designed to enhance human cognition by capturing and utilizing continuous contextual memory. Lucia introduces a lightweight, wearable device that excels in both comfort and real-time data accessibility, distinguishing itself from existing devices that typically prioritize either wearability or perceptual capabilities alone. By recording and interpreting daily activities over time, Lucia enables users to access a robust temporal memory, enhancing cognitive processes such as decision-making and memory recall. }
}

@article{241024166v1,
  title={ Approaches to human activity recognition via passive radar },
  author={ Christian Bresciani and Federico Cerutti and Marco Cominelli },
  journal={ arXiv preprint arXiv:2410.24166v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.24166v1 },
  abstract={ The thesis explores novel methods for Human Activity Recognition (HAR) using passive radar with a focus on non-intrusive Wi-Fi Channel State Information (CSI) data. Traditional HAR approaches often use invasive sensors like cameras or wearables, raising privacy issues. This study leverages the non-intrusive nature of CSI, using Spiking Neural Networks (SNN) to interpret signal variations caused by human movements. These networks, integrated with symbolic reasoning frameworks such as DeepProbLog, enhance the adaptability and interpretability of HAR systems. SNNs offer reduced power consumption, ideal for privacy-sensitive applications. Experimental results demonstrate SNN-based neurosymbolic models achieve high accuracy making them a promising alternative for HAR across various domains. }
}

@article{241020423v1,
  title={ A Deconfounding Framework for Human Behavior Prediction: Enhancing   Robotic Systems in Dynamic Environments },
  author={ Wentao Gao and Cheng Zhou },
  journal={ arXiv preprint arXiv:2410.20423v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.20423v1 },
  abstract={ Accurate prediction of human behavior is crucial for effective human-robot interaction (HRI) systems, especially in dynamic environments where real-time decisions are essential. This paper addresses the challenge of forecasting future human behavior using multivariate time series data from wearable sensors, which capture various aspects of human movement. The presence of hidden confounding factors in this data often leads to biased predictions, limiting the reliability of traditional models. To overcome this, we propose a robust predictive model that integrates deconfounding techniques with advanced time series prediction methods, enhancing the model's ability to isolate true causal relationships and improve prediction accuracy. Evaluation on real-world datasets demonstrates that our approach significantly outperforms traditional methods, providing a more reliable foundation for responsive and adaptive HRI systems. }
}

@article{241020034v1,
  title={ Sensor2Text: Enabling Natural Language Interactions for Daily Activity   Tracking Using Wearable Sensors },
  author={ Wenqiang Chen and Jiaxuan Cheng and Leyao Wang and Wei Zhao and Wojciech Matusik },
  journal={ arXiv preprint arXiv:2410.20034v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.20034v1 },
  abstract={ Visual Question-Answering, a technology that generates textual responses from an image and natural language question, has progressed significantly. Notably, it can aid in tracking and inquiring about daily activities, crucial in healthcare monitoring, especially for elderly patients or those with memory disabilities. However, video poses privacy concerns and has a limited field of view. This paper presents Sensor2Text, a model proficient in tracking daily activities and engaging in conversations using wearable sensors. The approach outlined here tackles several challenges, including low information density in wearable sensor data, insufficiency of single wearable sensors in human activities recognition, and model's limited capacity for Question-Answering and interactive conversations. To resolve these obstacles, transfer learning and student-teacher networks are utilized to leverage knowledge from visual-language models. Additionally, an encoder-decoder neural network model is devised to jointly process language and sensor data for conversational purposes. Furthermore, Large Language Models are also utilized to enable interactive capabilities. The model showcases the ability to identify human activities and engage in Q\\textbackslash{}\&A dialogues using various wearable sensor modalities. It performs comparably to or better than existing visual-language models in both captioning and conversational tasks. To our knowledge, this represents the first model capable of conversing about wearable sensor data, offering an innovative approach to daily activity tracking that addresses privacy and field-of-view limitations associated with current vision-based solutions. }
}

@article{241017489v1,
  title={ Unsupervised Domain Adaptation for Action Recognition via   Self-Ensembling and Conditional Embedding Alignment },
  author={ Indrajeet Ghosh and Garvit Chugh and Abu Zaher Md Faridee and Nirmalya Roy },
  journal={ arXiv preprint arXiv:2410.17489v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.17489v1 },
  abstract={ Recent advancements in deep learning-based wearable human action recognition (wHAR) have improved the capture and classification of complex motions, but adoption remains limited due to the lack of expert annotations and domain discrepancies from user variations. Limited annotations hinder the model's ability to generalize to out-of-distribution samples. While data augmentation can improve generalizability, unsupervised augmentation techniques must be applied carefully to avoid introducing noise. Unsupervised domain adaptation (UDA) addresses domain discrepancies by aligning conditional distributions with labeled target samples, but vanilla pseudo-labeling can lead to error propagation. To address these challenges, we propose \$\\textbackslash{}mu\$DAR, a novel joint optimization architecture comprised of three functions: (i) consistency regularizer between augmented samples to improve model classification generalizability, (ii) temporal ensemble for robust pseudo-label generation and (iii) conditional distribution alignment to improve domain generalizability. The temporal ensemble works by aggregating predictions from past epochs to smooth out noisy pseudo-label predictions, which are then used in the conditional distribution alignment module to minimize kernel-based class-wise conditional maximum mean discrepancy (\$k\$CMMD) between the source and target feature space to learn a domain invariant embedding. The consistency-regularized augmentations ensure that multiple augmentations of the same sample share the same labels; this results in (a) strong generalization with limited source domain samples and (b) consistent pseudo-label generation in target samples. The novel integration of these three modules in \$\\textbackslash{}mu\$DAR results in a range of \$\\textbackslash{}approx\$ 4-12\% average macro-F1 score improvement over six state-of-the-art UDA methods in four benchmark wHAR datasets }
}

@article{241019818v1,
  title={ UniMTS: Unified Pre-training for Motion Time Series },
  author={ Xiyuan Zhang and Diyan Teng and Ranak Roy Chowdhury and Shuheng Li and Dezhi Hong and Rajesh K. Gupta and Jingbo Shang },
  journal={ arXiv preprint arXiv:2410.19818v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.19818v1 },
  abstract={ Motion time series collected from mobile and wearable devices such as smartphones and smartwatches offer significant insights into human behavioral patterns, with wide applications in healthcare, automation, IoT, and AR/XR due to their low-power, always-on nature. However, given security and privacy concerns, building large-scale motion time series datasets remains difficult, preventing the development of pre-trained models for human activity analysis. Typically, existing models are trained and tested on the same dataset, leading to poor generalizability across variations in device location, device mounting orientation and human activity type. In this paper, we introduce UniMTS, the first unified pre-training procedure for motion time series that generalizes across diverse device latent factors and activities. Specifically, we employ a contrastive learning framework that aligns motion time series with text descriptions enriched by large language models. This helps the model learn the semantics of time series to generalize across activities. Given the absence of large-scale motion time series data, we derive and synthesize time series from existing motion skeleton data with all-joint coverage. Spatio-temporal graph networks are utilized to capture the relationships across joints for generalization across different device locations. We further design rotation-invariant augmentation to make the model agnostic to changes in device mounting orientations. Our model shows exceptional generalizability across 18 motion time series classification benchmark datasets, outperforming the best baselines by 340\% in the zero-shot setting, 16.3\% in the few-shot setting, and 9.2\% in the full-shot setting. }
}

@article{241012273v1,
  title={ Stress Assessment with Convolutional Neural Network Using PPG Signals },
  author={ Yasin Hasanpoor and Bahram Tarvirdizadeh and Khalil Alipour and Mohammad Ghamari },
  journal={ arXiv preprint arXiv:2410.12273v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.12273v1 },
  abstract={ Stress is one of the main issues of nowadays lifestyle. If it becomes chronic it can have adverse effects on the human body. Thus, the early detection of stress is crucial to prevent its hurting effects on the human body and have a healthier life. Stress can be assessed using physiological signals. To this end, Photoplethysmography (PPG) is one of the most favorable physiological signals for stress assessment. This research is focused on developing a novel technique to assess stressful events using raw PPG signals recorded by Empatica E4 sensor. To achieve this goal, an adaptive convolutional neural network (CNN) combined with Multilayer Perceptron (MLP) has been utilized to realize the detection of stressful events. This research will use a dataset that is publicly available and named wearable stress and effect detection (WESAD). This dataset will be used to simulate the proposed model and to examine the advantages of the proposed developed model. The proposed model in this research will be able to distinguish between normal events and stressful events. This model will be able to detect stressful events with an accuracy of 96.7\%. }
}

@article{230405088v4,
  title={ WEAR: An Outdoor Sports Dataset for Wearable and Egocentric Activity   Recognition },
  author={ Marius Bock and Hilde Kuehne and Kristof Van Laerhoven and Michael Moeller },
  journal={ arXiv preprint arXiv:2304.05088v4 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2304.05088v4 },
  abstract={ Research has shown the complementarity of camera- and inertial-based data for modeling human activities, yet datasets with both egocentric video and inertial-based sensor data remain scarce. In this paper, we introduce WEAR, an outdoor sports dataset for both vision- and inertial-based human activity recognition (HAR). Data from 22 participants performing a total of 18 different workout activities was collected with synchronized inertial (acceleration) and camera (egocentric video) data recorded at 11 different outside locations. WEAR provides a challenging prediction scenario in changing outdoor environments using a sensor placement, in line with recent trends in real-world applications. Benchmark results show that through our sensor placement, each modality interestingly offers complementary strengths and weaknesses in their prediction performance. Further, in light of the recent success of single-stage Temporal Action Localization (TAL) models, we demonstrate their versatility of not only being trained using visual data, but also using raw inertial data and being capable to fuse both modalities by means of simple concatenation. The dataset and code to reproduce experiments is publicly available via: mariusbock.github.io/wear/. }
}

@article{241011341v1,
  title={ Using Zone Inflation and Volume Transfer to Design a Fabric-based   Pneumatic Exosuit with both Efficiency and Wearability },
  author={ Chendong Liu and Dapeng Yang and Jiachen Chen and Yiming Dai and Li Jiang and Shengquan Xie and Hong Liu },
  journal={ arXiv preprint arXiv:2410.11341v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.11341v1 },
  abstract={ Fabric-based pneumatic exosuits have a broad application prospect due to their good human-machine interaction performance, but their structural design paradigm has not yet been finalized and requires in-depth research. This paper proposes the concepts of zone inflation and volume transfer for the design of a fabric-based pneumatic exosuit with both efficiency and wearability. The meaning of zone inflation is to divide the inflation area of pneumatic exosuit into inflation-deflation zone and inflation-holding zone which can reduce the consumption of compressed air and improve efficiency. Volume transfer, a strategic distribution method of inflatable regions inside the garment, can effectively enhance the wearability of the exosuit. Using inexpensive thermoplastic polyurethane film and clothing fabric, the exosuit is made by heat pressing and sewing. The exosuit has a response time of 0.5s, a stress area of 1500mm2, and a profile of only 32mm, which can be hidden inside common clothing. A mathematical model is developed to predict the output torque of the exosuit with an error of 3.6\%. Mechanical experiments show that the exosuit outputs a torque of 9.1Nm at a pressure of 100kPa. Surface electromyography experiments show that the exosuit can provide users with a boost from sitting to standing, with an average reduction in electromyography signals of 14.95\%. The exosuit designed using these methods synthesizes efficiency and wearability and is expected to be an ideal paradigm for fabric-based pneumatic exosuits. }
}

@article{231115831v2,
  title={ Temporal Action Localization for Inertial-based Human Activity   Recognition },
  author={ Marius Bock and Michael Moeller and Kristof Van Laerhoven },
  journal={ arXiv preprint arXiv:2311.15831v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2311.15831v2 },
  abstract={ As of today, state-of-the-art activity recognition from wearable sensors relies on algorithms being trained to classify fixed windows of data. In contrast, video-based Human Activity Recognition, known as Temporal Action Localization (TAL), has followed a segment-based prediction approach, localizing activity segments in a timeline of arbitrary length. This paper is the first to systematically demonstrate the applicability of state-of-the-art TAL models for both offline and near-online Human Activity Recognition (HAR) using raw inertial data as well as pre-extracted latent features as input. Offline prediction results show that TAL models are able to outperform popular inertial models on a multitude of HAR benchmark datasets, with improvements reaching as much as 26\% in F1-score. We show that by analyzing timelines as a whole, TAL models can produce more coherent segments and achieve higher NULL-class accuracy across all datasets. We demonstrate that TAL is less suited for the immediate classification of small-sized windows of data, yet offers an interesting perspective on inertial-based HAR -- alleviating the need for fixed-size windows and enabling algorithms to recognize activities of arbitrary length. With design choices and training concepts yet to be explored, we argue that TAL architectures could be of significant value to the inertial-based HAR community. The code and data download to reproduce experiments is publicly available via github.com/mariusbock/tal\_for\_har. }
}

@article{241006972v1,
  title={ Diamond of Thought: A Design Thinking-Based Framework for LLMs in   Wearable Design },
  author={ Qiyang Miao and Jiang Xu and Zhihao Song and Chengrui Wang and Yu Cui },
  journal={ arXiv preprint arXiv:2410.06972v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.06972v1 },
  abstract={ Wearable design is an interdisciplinary field that balances technological innovation, human factors, and human-computer interactions. Despite contributions from various disciplines, many projects lack stable interdisciplinary teams, which often leads to design failures. Large language models (LLMs) integrate diverse information and generate innovative solutions, making them a valuable tool for enhancing design processes. Thus, we have explored the use of LLMs in wearable design by combining design-thinking principles with LLM capabilities. We have developed the ''Diamond of Thought'' framework and analysed 1,603 prototypes and 1,129 products from a body-centric perspective to create a comprehensive database. We employed retrieval-augmented generation to input database details into the LLMs, ensuring applicability to wearable design challenges and integration of embodied cognition into the process. Our LLM-based methodology for wearables has been experimentally validated, demonstrating the potential of LLMs for the advancement of design practices. This study offers new tools and methods for future wearable designs. }
}

@article{241006001v1,
  title={ TapType: Ten-finger text entry on everyday surfaces via Bayesian   inference },
  author={ Paul Streli and Jiaxi Jiang and Andreas Fender and Manuel Meier and Hugo Romat and Christian Holz },
  journal={ arXiv preprint arXiv:2410.06001v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.06001v1 },
  abstract={ Despite the advent of touchscreens, typing on physical keyboards remains most efficient for entering text, because users can leverage all fingers across a full-size keyboard for convenient typing. As users increasingly type on the go, text input on mobile and wearable devices has had to compromise on full-size typing. In this paper, we present TapType, a mobile text entry system for full-size typing on passive surfaces--without an actual keyboard. From the inertial sensors inside a band on either wrist, TapType decodes and relates surface taps to a traditional QWERTY keyboard layout. The key novelty of our method is to predict the most likely character sequences by fusing the finger probabilities from our Bayesian neural network classifier with the characters' prior probabilities from an n-gram language model. In our online evaluation, participants on average typed 19 words per minute with a character error rate of 0.6\% after 30 minutes of training. Expert typists thereby consistently achieved more than 25 WPM at a similar error rate. We demonstrate applications of TapType in mobile use around smartphones and tablets, as a complement to interaction in situated Mixed Reality outside visual control, and as an eyes-free mobile text input method using an audio feedback-only interface. }
}

@article{240800855v3,
  title={ HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and   Style Generation in Fashion Design },
  author={ Jianan Jiang and Di Wu and Hanhui Deng and Yidan Long and Wenyi Tang and Xiang Li and Can Liu and Zhanpeng Jin and Wenlei Zhang and Tangquan Qi },
  journal={ arXiv preprint arXiv:2408.00855v3 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.00855v3 },
  abstract={ The process of fashion design usually involves sketching, refining, and coloring, with designers drawing inspiration from various images to fuel their creative endeavors. However, conventional image search methods often yield irrelevant results, impeding the design process. Moreover, creating and coloring sketches can be time-consuming and demanding, acting as a bottleneck in the design workflow. In this work, we introduce HAIGEN (Human-AI Collaboration for GENeration), an efficient fashion design system for Human-AI collaboration developed to aid designers. Specifically, HAIGEN consists of four modules. T2IM, located in the cloud, generates reference inspiration images directly from text prompts. With three other modules situated locally, the I2SM batch generates the image material library into a certain designer-style sketch material library. The SRM recommends similar sketches in the generated library to designers for further refinement, and the STM colors the refined sketch according to the styles of inspiration images. Through our system, any designer can perform local personalized fine-tuning and leverage the powerful generation capabilities of large models in the cloud, streamlining the entire design development process. Given that our approach integrates both cloud and local model deployment schemes, it effectively safeguards design privacy by avoiding the need to upload personalized data from local designers. We validated the effectiveness of each module through extensive qualitative and quantitative experiments. User surveys also confirmed that HAIGEN offers significant advantages in design efficiency, positioning it as a new generation of aid-tool for designers. }
}

@article{240918755v1,
  title={ Transparency evaluation for the Kinematic Design of the Harnesses   through Human-Exoskeleton Interaction Modeling },
  author={ Riccardo Bezzini and Carlo Alberto Avizzano and Francesco Porcini and Alessandro Filippeschi },
  journal={ arXiv preprint arXiv:2409.18755v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2409.18755v1 },
  abstract={ Lower Limb Exoskeletons (LLEs) are wearable robots that provide mechanical power to the user. Human-exoskeleton (HE) connections must preserve the user's natural behavior during the interaction, avoiding undesired forces. Therefore, numerous works focus on their minimization. Given the inherent complications of repeatedly prototyping and experimentally testing a device, modeling the exoskeleton and its physical interaction with the user emerges as a valuable approach for assessing the design effects. This paper proposes a novel method to compare different exoskeleton configurations with a flexible simulation tool. This approach contemplates simulating the dynamics of the device, including its interaction with the wearer, to evaluate multiple connection mechanism designs along with the kinematics and actuation of the LLE. This evaluation is based on the minimization of the interaction wrenches through an optimization process that includes the impedance parameters at the interfaces as optimization variables and the similarity of the LLE's joint variables trajectories with the motion of the wearer's articulations. Exploratory tests are conducted using the Wearable Walker LLE in different configurations and measuring the interaction forces. Experimental data are then compared to the optimization outcomes, proving that the proposed method provides contact wrench estimations consistent with the collected measurements and previous outcomes from the literature. Copyright 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. }
}

@article{240918127v1,
  title={ EgoLM: Multi-Modal Language Model of Egocentric Motions },
  author={ Fangzhou Hong and Vladimir Guzov and Hyo Jin Kim and Yuting Ye and Richard Newcombe and Ziwei Liu and Lingni Ma },
  journal={ arXiv preprint arXiv:2409.18127v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2409.18127v1 },
  abstract={ As the prevalence of wearable devices, learning egocentric motions becomes essential to develop contextual AI. In this work, we present EgoLM, a versatile framework that tracks and understands egocentric motions from multi-modal inputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich contexts for the disambiguation of egomotion tracking and understanding, which are ill-posed under single modality conditions. To facilitate the versatile and multi-modal framework, our key insight is to model the joint distribution of egocentric motions and natural languages using large language models (LLM). Multi-modal sensor inputs are encoded and projected to the joint latent space of language models, and used to prompt motion generation or text generation for egomotion tracking or understanding, respectively. Extensive experiments on large-scale multi-modal human motion dataset validate the effectiveness of EgoLM as a generalist model for universal egocentric learning. }
}

@article{240916415v1,
  title={ Improving Intersession Reproducibility for Forearm Ultrasound based Hand   Gesture Classification through an Incremental Learning Approach },
  author={ Keshav Bimbraw and Jack Rothenberg and Haichong K. Zhang },
  journal={ arXiv preprint arXiv:2409.16415v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2409.16415v1 },
  abstract={ Ultrasound images of the forearm can be used to classify hand gestures towards developing human machine interfaces. In our previous work, we have demonstrated gesture classification using ultrasound on a single subject without removing the probe before evaluation. This has limitations in usage as once the probe is removed and replaced, the accuracy declines since the classifier performance is sensitive to the probe location on the arm. In this paper, we propose training a model on multiple data collection sessions to create a generalized model, utilizing incremental learning through fine tuning. Ultrasound data was acquired for 5 hand gestures within a session (without removing and putting the probe back on) and across sessions. A convolutional neural network (CNN) with 5 cascaded convolution layers was used for this study. A pre-trained CNN was fine tuned with the convolution blocks acting as a feature extractor, and the parameters of the remaining layers updated in an incremental fashion. Fine tuning was done using different session splits within a session and between multiple sessions. We found that incremental fine tuning can help enhance classification accuracy with more fine tuning sessions. After 2 fine tuning sessions for each experiment, we found an approximate 10\% increase in classification accuracy. This work demonstrates that incremental learning through fine tuning on ultrasound based hand gesture classification can be used improves accuracy while saving storage, processing power, and time. It can be expanded to generalize between multiple subjects and towards developing personalized wearable devices. }
}

@article{240916081v1,
  title={ Online Multi-level Contrastive Representation Distillation for   Cross-Subject fNIRS Emotion Recognition },
  author={ Zhili Lai and Chunmei Qing and Junpeng Tan and Wanxiang Luo and Xiangmin Xu },
  journal={ arXiv preprint arXiv:2409.16081v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2409.16081v1 },
  abstract={ Utilizing functional near-infrared spectroscopy (fNIRS) signals for emotion recognition is a significant advancement in understanding human emotions. However, due to the lack of artificial intelligence data and algorithms in this field, current research faces the following challenges: 1) The portable wearable devices have higher requirements for lightweight models; 2) The objective differences of physiology and psychology among different subjects aggravate the difficulty of emotion recognition. To address these challenges, we propose a novel cross-subject fNIRS emotion recognition method, called the Online Multi-level Contrastive Representation Distillation framework (OMCRD). Specifically, OMCRD is a framework designed for mutual learning among multiple lightweight student networks. It utilizes multi-level fNIRS feature extractor for each sub-network and conducts multi-view sentimental mining using physiological signals. The proposed Inter-Subject Interaction Contrastive Representation (IS-ICR) facilitates knowledge transfer for interactions between student models, enhancing cross-subject emotion recognition performance. The optimal student network can be selected and deployed on a wearable device. Some experimental results demonstrate that OMCRD achieves state-of-the-art results in emotional perception and affective imagery tasks. }
}

@article{240308214v3,
  title={ P2LHAP:Wearable sensor-based human activity recognition, segmentation   and forecast through Patch-to-Label Seq2Seq Transformer },
  author={ Shuangjian Li and Tao Zhu and Mingxing Nie and Huansheng Ning and Zhenyu Liu and Liming Chen },
  journal={ arXiv preprint arXiv:2403.08214v3 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2403.08214v3 },
  abstract={ Traditional deep learning methods struggle to simultaneously segment, recognize, and forecast human activities from sensor data. This limits their usefulness in many fields such as healthcare and assisted living, where real-time understanding of ongoing and upcoming activities is crucial. This paper introduces P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles all three tasks in a efficient single-task model. P2LHAP divides sensor data streams into a sequence of ''patches'', served as input tokens, and outputs a sequence of patch-level activity labels including the predicted future activities. A unique smoothing technique based on surrounding patch labels, is proposed to identify activity boundaries accurately. Additionally, P2LHAP learns patch-level representation by sensor signal channel-independent Transformer encoders and decoders. All channels share embedding and Transformer weights across all sequences. Evaluated on three public datasets, P2LHAP significantly outperforms the state-of-the-art in all three tasks, demonstrating its effectiveness and potential for real-world applications. }
}

@article{241003546v1,
  title={ Multidimensional Human Activity Recognition With Large Language Model: A   Conceptual Framework },
  author={ Syed Mhamudul Hasan },
  journal={ arXiv preprint arXiv:2410.03546v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.03546v1 },
  abstract={ In high-stake environments like emergency response or elder care, the integration of large language model (LLM), revolutionize risk assessment, resource allocation, and emergency responses in Human Activity Recognition (HAR) systems by leveraging data from various wearable sensors. We propose a conceptual framework that utilizes various wearable devices, each considered as a single dimension, to support a multidimensional learning approach within HAR systems. By integrating and processing data from these diverse sources, LLMs can process and translate complex sensor inputs into actionable insights. This integration mitigates the inherent uncertainties and complexities associated with them, and thus enhancing the responsiveness and effectiveness of emergency services. This paper sets the stage for exploring the transformative potential of LLMs within HAR systems in empowering emergency workers to navigate the unpredictable and risky environments they encounter in their critical roles. }
}

@article{240313132v3,
  title={ Wearable Roller Rings to Augment In-Hand Manipulation through Active   Surfaces },
  author={ Hayden Webb and Podshara Chanrungmaneekul and Shenli Yuan and Kaiyu Hang },
  journal={ arXiv preprint arXiv:2403.13132v3 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2403.13132v3 },
  abstract={ In-hand manipulation is a crucial ability for reorienting and repositioning objects within grasps. The main challenges in this are not only the complexity of the computational models, but also the risks of grasp instability caused by active finger motions, such as rolling, sliding, breaking, and remaking contacts. This paper presents the development of the Roller Ring (RR), a modular robotic attachment with active surfaces that is wearable by both robot and human hands to manipulate without lifting a finger. By installing the angled RRs on hands, such that their spatial motions are not colinear, we derive a general differential motion model for manipulating objects. Our motion model shows that complete in-hand manipulation skill sets can be provided by as few as only 2 RRs through non-holonomic object motions, while more RRs can enable enhanced manipulation dexterity with fewer motion constraints. Through extensive experiments, we test the RRs on both a robot hand and a human hand to evaluate their manipulation capabilities. We show that the RRs can be employed to manipulate arbitrary object shapes to provide dexterous in-hand manipulation. }
}

@article{240908859v1,
  title={ Optimized Design of A Haptic Unit for Vibrotactile Amplitude Modulation },
  author={ Jingchen Huang and Yun Fang and Weichao Guo and Xinjun Sheng },
  journal={ arXiv preprint arXiv:2409.08859v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2409.08859v1 },
  abstract={ Communicating information to users is a crucial aspect of human-machine interaction. Vibrotactile feedback encodes information into spatiotemporal vibrations, enabling users to perceive tactile sensations. It offers advantages such as lightweight, wearability, and high stability, with broad applications in sensory substitution, virtual reality, education, and healthcare. However, existing haptic unit designs lack amplitude modulation capabilities, which limits their applications. This paper proposed an optimized design of the haptic unit from the perspective of vibration amplitude modulation. A modified elastic model was developed to describe the propagation and attenuation mechanisms of vibration in the skin. Based on the model, two types of hierarchical architectural design were proposed. The design incorporated various materials arranged in multiple layers to amplify or attenuate the vibration amplitude as it traveled through the structure. An experimental platform was built to evaluate the performance of the optimized design. }
}

@article{240906341v1,
  title={ A Wearable Multi-Modal Edge-Computing System for Real-Time Kitchen   Activity Recognition },
  author={ Mengxi Liu and Sungho Suh and Juan Felipe Vargas and Bo Zhou and Agnes Grünerbl and Paul Lukowicz },
  journal={ arXiv preprint arXiv:2409.06341v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2409.06341v1 },
  abstract={ In the human activity recognition research area, prior studies predominantly concentrate on leveraging advanced algorithms on public datasets to enhance recognition performance, little attention has been paid to executing real-time kitchen activity recognition on energy-efficient, cost-effective edge devices. Besides, the prevalent approach of segregating data collection and context extraction across different devices escalates power usage, latency, and user privacy risks, impeding widespread adoption. This work presents a multi-modal wearable edge computing system for human activity recognition in real-time. Integrating six different sensors, ranging from inertial measurement units (IMUs) to thermal cameras, and two different microcontrollers, this system achieves end-to-end activity recognition, from data capture to context extraction, locally. Evaluation in an unmodified realistic kitchen validates its efficacy in recognizing fifteen activities, including a null class. Employing a compact machine learning model (184.5 kbytes) yields an average accuracy of 87.83 \\textbackslash{}\%, with model inference completed in 25.26 ms on the microcontroller. Comparative analysis with alternative microcontrollers showcases power consumption and inference speed performance, demonstrating the proposed system's viability. }
}

@article{240902064v2,
  title={ Personalized Federated Learning via Active Sampling },
  author={ Alexander Jung and Yasmin SarcheshmehPour and Amirhossein Mohammadi },
  journal={ arXiv preprint arXiv:2409.02064v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2409.02064v2 },
  abstract={ Consider a collection of data generators which could represent, e.g., humans equipped with a smart-phone or wearables. We want to train a personalized (or tailored) model for each data generator even if they provide only small local datasets. The available local datasets might fail to provide sufficient statistical power to train high-dimensional models (such as deep neural networks) effectively. One possible solution is to identify similar data generators and pool their local datasets to obtain a sufficiently large training set. This paper proposes a novel method for sequentially identifying similar (or relevant) data generators. Our method is similar in spirit to active sampling methods but does not require exchange of raw data. Indeed, our method evaluates the relevance of a data generator by evaluating the effect of a gradient step using its local dataset. This evaluation can be performed in a privacy-friendly fashion without sharing raw data. We extend this method to non-parametric models by a suitable generalization of the gradient step to update a hypothesis using the local dataset provided by a data generator. }
}

@article{240814578v2,
  title={ Multi-faceted Sensory Substitution for Curb Alerting: A Pilot   Investigation in Persons with Blindness and Low Vision },
  author={ Ligao Ruan and Giles Hamilton-Fletcher and Mahya Beheshti and Todd E Hudson and Maurizio Porfiri and JR Rizzo },
  journal={ arXiv preprint arXiv:2408.14578v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.14578v2 },
  abstract={ Curbs -- the edge of a raised sidewalk at the point where it meets a street -- crucial in urban environments where they help delineate safe pedestrian zones, from dangerous vehicular lanes. However, curbs themselves are significant navigation hazards, particularly for people who are blind or have low vision (pBLV). The challenges faced by pBLV in detecting and properly orientating themselves for these abrupt elevation changes can lead to falls and serious injuries. Despite recent advancements in assistive technologies, the detection and early warning of curbs remains a largely unsolved challenge. This paper aims to tackle this gap by introducing a novel, multi-faceted sensory substitution approach hosted on a smart wearable; the platform leverages an RGB camera and an embedded system to capture and segment curbs in real time and provide early warning and orientation information. The system utilizes YOLO (You Only Look Once) v8 segmentation model, trained on our custom curb dataset for the camera input. The output of the system consists of adaptive auditory beeps, abstract sonification, and speech, conveying information about the relative distance and orientation of curbs. Through human-subjects experimentation, we demonstrate the effectiveness of the system as compared to the white cane. Results show that our system can provide advanced warning through a larger safety window than the cane, while offering nearly identical curb orientation information. }
}

@article{240814146v1,
  title={ TSAK: Two-Stage Semantic-Aware Knowledge Distillation for Efficient   Wearable Modality and Model Optimization in Manufacturing Lines },
  author={ Hymalai Bello and Daniel Geißler and Sungho Suh and Bo Zhou and Paul Lukowicz },
  journal={ arXiv preprint arXiv:2408.14146v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.14146v1 },
  abstract={ Smaller machine learning models, with less complex architectures and sensor inputs, can benefit wearable sensor-based human activity recognition (HAR) systems in many ways, from complexity and cost to battery life. In the specific case of smart factories, optimizing human-robot collaboration hinges on the implementation of cutting-edge, human-centric AI systems. To this end, workers' activity recognition enables accurate quantification of performance metrics, improving efficiency holistically. We present a two-stage semantic-aware knowledge distillation (KD) approach, TSAK, for efficient, privacy-aware, and wearable HAR in manufacturing lines, which reduces the input sensor modalities as well as the machine learning model size, while reaching similar recognition performance as a larger multi-modal and multi-positional teacher model. The first stage incorporates a teacher classifier model encoding attention, causal, and combined representations. The second stage encompasses a semantic classifier merging the three representations from the first stage. To evaluate TSAK, we recorded a multi-modal dataset at a smart factory testbed with wearable and privacy-aware sensors (IMU and capacitive) located on both workers' hands. In addition, we evaluated our approach on OpenPack, the only available open dataset mimicking the wearable sensor placements on both hands in the manufacturing HAR scenario. We compared several KD strategies with different representations to regulate the training process of a smaller student model. Compared to the larger teacher model, the student model takes fewer sensor channels from a single hand, has 79\% fewer parameters, runs 8.88 times faster, and requires 96.6\% less computing power (FLOPS). }
}

@article{240813041v1,
  title={ A Comparison of Deep Learning and Established Methods for Calf Behaviour   Monitoring },
  author={ Oshana Dissanayake and Lucile Riaboff and Sarah E. McPherson and Emer Kennedy and Pádraig Cunningham },
  journal={ arXiv preprint arXiv:2408.13041v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.13041v1 },
  abstract={ In recent years, there has been considerable progress in research on human activity recognition using data from wearable sensors. This technology also has potential in the context of animal welfare in livestock science. In this paper, we report on research on animal activity recognition in support of welfare monitoring. The data comes from collar-mounted accelerometer sensors worn by Holstein and Jersey calves, the objective being to detect changes in behaviour indicating sickness or stress. A key requirement in detecting changes in behaviour is to be able to classify activities into classes, such as drinking, running or walking. In Machine Learning terms, this is a time-series classification task, and in recent years, the Rocket family of methods have emerged as the state-of-the-art in this area. We have over 27 hours of labelled time-series data from 30 calves for our analysis. Using this data as a baseline, we present Rocket's performance on a 6-class classification task. Then, we compare this against the performance of 11 Deep Learning (DL) methods that have been proposed as promising methods for time-series classification. Given the success of DL in related areas, it is reasonable to expect that these methods will perform well here as well. Surprisingly, despite taking care to ensure that the DL methods are configured correctly, none of them match Rocket's performance. A possible explanation for the impressive success of Rocket is that it has the data encoding benefits of DL models in a much simpler classification framework. }
}

@article{240809527v2,
  title={ ALS-HAR: Harnessing Wearable Ambient Light Sensors to Enhance IMU-based   Human Activity Recogntion },
  author={ Lala Shakti Swarup Ray and Daniel Geißler and Mengxi Liu and Bo Zhou and Sungho Suh and Paul Lukowicz },
  journal={ arXiv preprint arXiv:2408.09527v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.09527v2 },
  abstract={ Despite the widespread integration of ambient light sensors (ALS) in smart devices commonly used for screen brightness adaptation, their application in human activity recognition (HAR), primarily through body-worn ALS, is largely unexplored. In this work, we developed ALS-HAR, a robust wearable light-based motion activity classifier. Although ALS-HAR achieves comparable accuracy to other modalities, its natural sensitivity to external disturbances, such as changes in ambient light, weather conditions, or indoor lighting, makes it challenging for daily use. To address such drawbacks, we introduce strategies to enhance environment-invariant IMU-based activity classifications through augmented multi-modal and contrastive classifications by transferring the knowledge extracted from the ALS. Our experiments on a real-world activity dataset for three different scenarios demonstrate that while ALS-HAR's accuracy strongly relies on external lighting conditions, cross-modal information can still improve other HAR systems, such as IMU-based classifiers.Even in scenarios where ALS performs insufficiently, the additional knowledge enables improved accuracy and macro F1 score by up to 4.2 \% and 6.4 \%, respectively, for IMU-based classifiers and even surpasses multi-modal sensor fusion models in two of our three experiment scenarios. Our research highlights the untapped potential of ALS integration in advancing sensor-based HAR technology, paving the way for practical and efficient wearable ALS-based activity recognition systems with potential applications in healthcare, sports monitoring, and smart indoor environments. }
}

@article{240812023v1,
  title={ Limitations in Employing Natural Language Supervision for Sensor-Based   Human Activity Recognition -- And Ways to Overcome Them },
  author={ Harish Haresamudram and Apoorva Beedu and Mashfiqui Rabbi and Sankalita Saha and Irfan Essa and Thomas Ploetz },
  journal={ arXiv preprint arXiv:2408.12023v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.12023v1 },
  abstract={ Cross-modal contrastive pre-training between natural language and other modalities, e.g., vision and audio, has demonstrated astonishing performance and effectiveness across a diverse variety of tasks and domains. In this paper, we investigate whether such natural language supervision can be used for wearable sensor based Human Activity Recognition (HAR), and discover that-surprisingly-it performs substantially worse than standard end-to-end training and self-supervision. We identify the primary causes for this as: sensor heterogeneity and the lack of rich, diverse text descriptions of activities. To mitigate their impact, we also develop strategies and assess their effectiveness through an extensive experimental evaluation. These strategies lead to significant increases in activity recognition, bringing performance closer to supervised and self-supervised training, while also enabling the recognition of unseen activities and cross modal retrieval of videos. Overall, our work paves the way for better sensor-language learning, ultimately leading to the development of foundational models for HAR using wearables. }
}

@article{240811346v1,
  title={ Non-verbal Hands-free Control for Smart Glasses using Teeth Clicks },
  author={ Payal Mohapatra and Ali Aroudi and Anurag Kumar and Morteza Khaleghimeybodi },
  journal={ arXiv preprint arXiv:2408.11346v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.11346v1 },
  abstract={ Smart glasses are emerging as a popular wearable computing platform potentially revolutionizing the next generation of human-computer interaction. The widespread adoption of smart glasses has created a pressing need for discreet and hands-free control methods. Traditional input techniques, such as voice commands or tactile gestures, can be intrusive and non-discreet. Additionally, voice-based control may not function well in noisy acoustic conditions. We propose a novel, discreet, non-verbal, and non-tactile approach to controlling smart glasses through subtle vibrations on the skin induced by teeth clicking. We demonstrate that these vibrations can be sensed by accelerometers embedded in the glasses with a low-footprint predictive model. Our proposed method, called STEALTHsense, utilizes a temporal broadcasting-based neural network architecture with just 88K trainable parameters and 7.14M Multiply and Accumulate (MMAC) per inference unit. We benchmark our proposed STEALTHsense against state-of-the-art deep learning approaches and traditional low-footprint machine learning approaches. We conducted a study across 21 participants to collect representative samples for two distinct teeth-clicking patterns and many non-patterns for robust training of STEALTHsense, achieving an average cross-person accuracy of 0.93. Field testing confirmed its effectiveness, even in noisy conditions, underscoring STEALTHsense's potential for real-world applications, offering a promising solution for smart glasses interaction. }
}

@article{240808734v1,
  title={ User-centered evaluation of the Wearable Walker lower limb exoskeleton,   preliminary assessment based on the Experience protocol },
  author={ Cristian Camardella and Vittorio Lippi and Francesco Porcini and Giulia Bassani and Lucia Lencioni and Christoph Mauer and Christian Haverkamp and Carlo Alberto Avizzano and Antonio Frisoli and Alessandro Filippeschi },
  journal={ arXiv preprint arXiv:2408.08734v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.08734v1 },
  abstract={ Using lower-limbs exoskeletons provides potential advantages in terms of productivity and safety associated with reduced stress. However, complex issues in human-robot interaction are still open, such as the physiological effects of exoskeletons and the impact on the user's subjective experience. In this work, an innovative exoskeleton, the Wearable Walker, is assessed using the EXPERIENCE benchmarking protocol from the EUROBENCH project. The Wearable Walker is a lower-limb exoskeleton that enhances human abilities, such as carrying loads. The device uses a unique control approach called Blend Control that provides smooth assistance torques. It operates two models simultaneously, one in the case in which the left foot is grounded and another for the grounded right foot. These models generate assistive torques combined to provide continuous and smooth overall assistance, preventing any abrupt changes in torque due to model switching. The EXPERIENCE protocol consists of walking on flat ground while gathering physiological signals such as heart rate, its variability, respiration rate, and galvanic skin response and completing a questionnaire. The test was performed with five healthy subjects. The scope of the present study is twofold: to evaluate the specific exoskeleton and its current control system to gain insight into possible improvements and to present a case study for a formal and replicable benchmarking of wearable robots. }
}

@article{240803958v2,
  title={ Optimizing Emotion Recognition with Wearable Sensor Data: Unveiling   Patterns in Body Movements and Heart Rate through Random Forest   Hyperparameter Tuning },
  author={ Zikri Kholifah Nur and Rifki Wijaya and Gia Septiana Wulandari },
  journal={ arXiv preprint arXiv:2408.03958v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.03958v2 },
  abstract={ This research delves into the utilization of smartwatch sensor data and heart rate monitoring to discern individual emotions based on body movement and heart rate. Emotions play a pivotal role in human life, influencing mental well-being, quality of life, and even physical and physiological responses. The data were sourced from prior research by Juan C. Quiroz, PhD. The study enlisted 50 participants who donned smartwatches and heart rate monitors while completing a 250-meter walk. Emotions were induced through both audio-visual and audio stimuli, with participants' emotional states evaluated using the PANAS questionnaire. The study scrutinized three scenarios: viewing a movie before walking, listening to music before walking, and listening to music while walking. Personal baselines were established using DummyClassifier with the 'most\_frequent' strategy from the sklearn library, and various models, including Logistic Regression and Random Forest, were employed to gauge the impacts of these activities. Notably, a novel approach was undertaken by incorporating hyperparameter tuning to the Random Forest model using RandomizedSearchCV. The outcomes showcased substantial enhancements with hyperparameter tuning in the Random Forest model, yielding mean accuracies of 86.63\% for happy vs. sad and 76.33\% for happy vs. neutral vs. sad. }
}

@article{240805169v1,
  title={ Weak-Annotation of HAR Datasets using Vision Foundation Models },
  author={ Marius Bock and Kristof Van Laerhoven and Michael Moeller },
  journal={ arXiv preprint arXiv:2408.05169v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.05169v1 },
  abstract={ As wearable-based data annotation remains, to date, a tedious, time-consuming task requiring researchers to dedicate substantial time, benchmark datasets within the field of Human Activity Recognition in lack richness and size compared to datasets available within related fields. Recently, vision foundation models such as CLIP have gained significant attention, helping the vision community advance in finding robust, generalizable feature representations. With the majority of researchers within the wearable community relying on vision modalities to overcome the limited expressiveness of wearable data and accurately label their to-be-released benchmark datasets offline, we propose a novel, clustering-based annotation pipeline to significantly reduce the amount of data that needs to be annotated by a human annotator. We show that using our approach, the annotation of centroid clips suffices to achieve average labelling accuracies close to 90\% across three publicly available HAR benchmark datasets. Using the weakly annotated datasets, we further demonstrate that we can match the accuracy scores of fully-supervised deep learning classifiers across all three benchmark datasets. Code as well as supplementary figures and results are publicly downloadable via github.com/mariusbock/weak\_har. }
}

@article{240804243v1,
  title={ MU-MAE: Multimodal Masked Autoencoders-Based One-Shot Learning },
  author={ Rex Liu and Xin Liu },
  journal={ arXiv preprint arXiv:2408.04243v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.04243v1 },
  abstract={ With the exponential growth of multimedia data, leveraging multimodal sensors presents a promising approach for improving accuracy in human activity recognition. Nevertheless, accurately identifying these activities using both video data and wearable sensor data presents challenges due to the labor-intensive data annotation, and reliance on external pretrained models or additional data. To address these challenges, we introduce Multimodal Masked Autoencoders-Based One-Shot Learning (Mu-MAE). Mu-MAE integrates a multimodal masked autoencoder with a synchronized masking strategy tailored for wearable sensors. This masking strategy compels the networks to capture more meaningful spatiotemporal features, which enables effective self-supervised pretraining without the need for external data. Furthermore, Mu-MAE leverages the representation extracted from multimodal masked autoencoders as prior information input to a cross-attention multimodal fusion layer. This fusion layer emphasizes spatiotemporal features requiring attention across different modalities while highlighting differences from other classes, aiding in the classification of various classes in metric-based one-shot learning. Comprehensive evaluations on MMAct one-shot classification show that Mu-MAE outperforms all the evaluated approaches, achieving up to an 80.17\% accuracy for five-way one-shot multimodal classification, without the use of additional data. }
}

@article{240319026v3,
  title={ EgoNav: Egocentric Scene-aware Human Trajectory Prediction },
  author={ Weizhuo Wang and C. Karen Liu and Monroe Kennedy III },
  journal={ arXiv preprint arXiv:2403.19026v3 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2403.19026v3 },
  abstract={ Wearable collaborative robots stand to assist human wearers who need fall prevention assistance or wear exoskeletons. Such a robot needs to be able to constantly adapt to the surrounding scene based on egocentric vision, and predict the ego motion of the wearer. In this work, we leveraged body-mounted cameras and sensors to anticipate the trajectory of human wearers through complex surroundings. To facilitate research in ego-motion prediction, we have collected a comprehensive walking scene navigation dataset centered on the user's perspective. We then present a method to predict human motion conditioning on the surrounding static scene. Our method leverages a diffusion model to produce a distribution of potential future trajectories, taking into account the user's observation of the environment. To that end, we introduce a compact representation to encode the user's visual memory of the surroundings, as well as an efficient sample-generating technique to speed up real-time inference of a diffusion model. We ablate our model and compare it to baselines, and results show that our model outperforms existing methods on key metrics of collision avoidance and trajectory mode coverage. }
}

@article{240802547v1,
  title={ The Role of Functional Muscle Networks in Improving Hand Gesture   Perception for Human-Machine Interfaces },
  author={ Costanza Armanini and Tuka Alhanai and Farah E. Shamout and S. Farokh Atashzar },
  journal={ arXiv preprint arXiv:2408.02547v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.02547v1 },
  abstract={ Developing accurate hand gesture perception models is critical for various robotic applications, enabling effective communication between humans and machines and directly impacting neurorobotics and interactive robots. Recently, surface electromyography (sEMG) has been explored for its rich informational context and accessibility when combined with advanced machine learning approaches and wearable systems. The literature presents numerous approaches to boost performance while ensuring robustness for neurorobots using sEMG, often resulting in models requiring high processing power, large datasets, and less scalable solutions. This paper addresses this challenge by proposing the decoding of muscle synchronization rather than individual muscle activation. We study coherence-based functional muscle networks as the core of our perception model, proposing that functional synchronization between muscles and the graph-based network of muscle connectivity encode contextual information about intended hand gestures. This can be decoded using shallow machine learning approaches without the need for deep temporal networks. Our technique could impact myoelectric control of neurorobots by reducing computational burdens and enhancing efficiency. The approach is benchmarked on the Ninapro database, which contains 12 EMG signals from 40 subjects performing 17 hand gestures. It achieves an accuracy of 85.1\%, demonstrating improved performance compared to existing methods while requiring much less computational power. The results support the hypothesis that a coherence-based functional muscle network encodes critical information related to gesture execution, significantly enhancing hand gesture perception with potential applications for neurorobotic systems and interactive machines. }
}

@article{240719859v1,
  title={ ProRuka: A highly efficient HMI algorithm for controlling a novel   prosthetic hand with 6-DOF using sonomyography },
  author={ Vaheh Nazari and Yong-Ping Zheng },
  journal={ arXiv preprint arXiv:2407.19859v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2407.19859v1 },
  abstract={ Sonomyography (SMG) is a novel human-machine interface that controls upper-limb prostheses by monitoring forearm muscle activity using ultrasonic imaging. SMG has been investigated for controlling upper-limb prostheses during the last two decades. The results show that this method, in combination with artificial intelligence, can classify different hand gestures with an accuracy of more than 90\%, making it a great alternative control system compared to electromyography (EMG). However, up to now there are few reports of a system integrating SMG together with a prosthesis for testing on amputee subjects to demonstrate its capability in relation to daily activities. In this study, we developed ProRuka, a novel low-cost 6-degree-of-freedom prosthetic hand integrated with the control provided by a SMG system with a wearable ultrasound imaging probe. The classification of hand gestures using different machine learning classification/regression algorithms including KNN, nearest neighbor regression, random forest, decision tree classifier, decision tree regression, support vector regression and support vector machine in combination with a transfer learning model (VGG16) was first evaluated off-line to determine its reliability and precision. Additionally, the developed controlling system were evaluated on two amputees, in real-time experiments using a variety of hand function test kits. The results from an off-line study including ten healthy participants indicated that nine different hand motions can be classified with a success rate of 100\%. In addition, the hand function test in real time (using 4 different hand gestures) confirmed that the designed prosthesis with the SMG controlling system can assist amputees to perform a variety of hand movements needed in daily activities. }
}

@article{240807282v1,
  title={ Consistency Based Weakly Self-Supervised Learning for Human Activity   Recognition with Wearables },
  author={ Taoran Sheng and Manfred Huber },
  journal={ arXiv preprint arXiv:2408.07282v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.07282v1 },
  abstract={ While the widely available embedded sensors in smartphones and other wearable devices make it easier to obtain data of human activities, recognizing different types of human activities from sensor-based data remains a difficult research topic in ubiquitous computing. One reason for this is that most of the collected data is unlabeled. However, many current human activity recognition (HAR) systems are based on supervised methods, which heavily rely on the labels of the data. We describe a weakly self-supervised approach in this paper that consists of two stages: (1) In stage one, the model learns from the nature of human activities by projecting the data into an embedding space where similar activities are grouped together; (2) In stage two, the model is fine-tuned using similarity information in a few-shot learning fashion using the similarity information of the data. This allows downstream classification or clustering tasks to benefit from the embeddings. Experiments on three benchmark datasets demonstrate the framework's effectiveness and show that our approach can help the clustering algorithm achieve comparable performance in identifying and categorizing the underlying human activities as pure supervised techniques applied directly to a corresponding fully labeled data set. }
}

@article{240714289v1,
  title={ Neuromuscular Modeling for Locomotion with Wearable Assistive Robots --   A primer },
  author={ Mohamed Irfan Refai and Huawei Wang and Antonio Gogeascoechea and Rafael Ornelas Kobayashi and Lucas A. Gaudio and Federica Damonte and Guillaume Durandau and Herman van der Kooij and Utku S. Yavuz and Massimo Sartori },
  journal={ arXiv preprint arXiv:2407.14289v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2407.14289v1 },
  abstract={ Wearable assistive robots (WR) for the lower extremity are extensively documented in literature. Various interfaces have been designed to control these devices during gait and balance activities. However, achieving seamless and intuitive control requires accurate modeling of the human neuromusculoskeletal (NMSK) system. Such modeling enables WR to anticipate user intentions and determine the necessary joint assistance. Despite the existence of controllers interfacing with the NMSK system, robust and generalizable techniques across different tasks remain scarce. Designing these novel controllers necessitates the combined expertise of neurophysiologists, who understand the physiology of movement initiation and generation, and biomechatronic engineers, who design and control devices that assist movement. This paper aims to bridge the gaps between these fields by presenting a primer on key concepts and the current state of the science in each area. We present three main sections: the neuromechanics of locomotion, neuromechanical models of movement, and existing neuromechanical controllers used in WR. Through these sections, we provide a comprehensive overview of seminal studies in the field, facilitating collaboration between neurophysiologists and biomechatronic engineers for future advances in wearable robotics for locomotion. }
}

@article{240404833v2,
  title={ ShoeModel: Learning to Wear on the User-specified Shoes via Diffusion   Model },
  author={ Binghui Chen and Wenyu Li and Yifeng Geng and Xuansong Xie and Wangmeng Zuo },
  journal={ arXiv preprint arXiv:2404.04833v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2404.04833v2 },
  abstract={ With the development of the large-scale diffusion model, Artificial Intelligence Generated Content (AIGC) techniques are popular recently. However, how to truly make it serve our daily lives remains an open question. To this end, in this paper, we focus on employing AIGC techniques in one filed of E-commerce marketing, i.e., generating hyper-realistic advertising images for displaying user-specified shoes by human. Specifically, we propose a shoe-wearing system, called Shoe-Model, to generate plausible images of human legs interacting with the given shoes. It consists of three modules: (1) shoe wearable-area detection module (WD), (2) leg-pose synthesis module (LpS) and the final (3) shoe-wearing image generation module (SW). Them three are performed in ordered stages. Compared to baselines, our ShoeModel is shown to generalize better to different type of shoes and has ability of keeping the ID-consistency of the given shoes, as well as automatically producing reasonable interactions with human. Extensive experiments show the effectiveness of our proposed shoe-wearing system. Figure 1 shows the input and output examples of our ShoeModel. }
}

@article{230508752v3,
  title={ A Matter of Annotation: An Empirical Study on In Situ and Self-Recall   Activity Annotations from Wearable Sensors },
  author={ Alexander Hoelzemann and Kristof Van Laerhoven },
  journal={ arXiv preprint arXiv:2305.08752v3 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2305.08752v3 },
  abstract={ Research into the detection of human activities from wearable sensors is a highly active field, benefiting numerous applications, from ambulatory monitoring of healthcare patients via fitness coaching to streamlining manual work processes.   We present an empirical study that evaluates and contrasts four commonly employed annotation methods in user studies focused on in-the-wild data collection. For both the user-driven, in situ annotations, where participants annotate their activities during the actual recording process, and the recall methods, where participants retrospectively annotate their data at the end of each day, the participants had the flexibility to select their own set of activity classes and corresponding labels.   Our study illustrates that different labeling methodologies directly impact the annotations' quality, as well as the capabilities of a deep learning classifier trained with the data. We noticed that in situ methods produce less but more precise labels than recall methods. Furthermore, we combined an activity diary with a visualization tool that enables the participant to inspect and label their activity data. Due to the introduction of such a tool were able to decrease missing annotations and increase the annotation consistency, and therefore the F1-Score of the deep learning model by up to 8\% (ranging between 82.1 and 90.4\% F1-Score). Furthermore, we discuss the advantages and disadvantages of the methods compared in our study, the biases they could introduce, and the consequences of their usage on human activity recognition studies as well as possible solutions. }
}

@article{231002011v3,
  title={ Decoding Human Activities: Analyzing Wearable Accelerometer and   Gyroscope Data for Activity Recognition },
  author={ Utsab Saha and Sawradip Saha and Tahmid Kabir and Shaikh Anowarul Fattah and Mohammad Saquib },
  journal={ arXiv preprint arXiv:2310.02011v3 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2310.02011v3 },
  abstract={ A person's movement or relative positioning can be effectively captured by different types of sensors and corresponding sensor output can be utilized in various manipulative techniques for the classification of different human activities. This letter proposes an effective scheme for human activity recognition, which introduces two unique approaches within a multi-structural architecture, named FusionActNet. The first approach aims to capture the static and dynamic behavior of a particular action by using two dedicated residual networks and the second approach facilitates the final decision-making process by introducing a guidance module. A two-stage training process is designed where at the first stage, residual networks are pre-trained separately by using static (where the human body is immobile) and dynamic (involving movement of the human body) data. In the next stage, the guidance module along with the pre-trained static or dynamic models are used to train the given sensor data. Here the guidance module learns to emphasize the most relevant prediction vector obtained from the static or dynamic models, which helps to effectively classify different human activities. The proposed scheme is evaluated using two benchmark datasets and compared with state-of-the-art methods. The results clearly demonstrate that our method outperforms existing approaches in terms of accuracy, precision, recall, and F1 score, achieving 97.35\% and 95.35\% accuracy on the UCI HAR and Motion-Sense datasets, respectively which highlights both the effectiveness and stability of the proposed scheme. }
}

@article{240700233v1,
  title={ Methodology to Deploy CNN-Based Computer Vision Models on Immersive   Wearable Devices },
  author={ Kaveh Malek and Fernando Moreu },
  journal={ arXiv preprint arXiv:2407.00233v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2407.00233v1 },
  abstract={ Convolutional Neural Network (CNN) models often lack the ability to incorporate human input, which can be addressed by Augmented Reality (AR) headsets. However, current AR headsets face limitations in processing power, which has prevented researchers from performing real-time, complex image recognition tasks using CNNs in AR headsets. This paper presents a method to deploy CNN models on AR headsets by training them on computers and transferring the optimized weight matrices to the headset. The approach transforms the image data and CNN layers into a one-dimensional format suitable for the AR platform. We demonstrate this method by training the LeNet-5 CNN model on the MNIST dataset using PyTorch and deploying it on a HoloLens AR headset. The results show that the model maintains an accuracy of approximately 98\%, similar to its performance on a computer. This integration of CNN and AR enables real-time image processing on AR headsets, allowing for the incorporation of human input into AI models. }
}

@article{240616815v1,
  title={ ClotheDreamer: Text-Guided Garment Generation with 3D Gaussians },
  author={ Yufei Liu and Junshu Tang and Chu Zheng and Shijie Zhang and Jinkun Hao and Junwei Zhu and Dongjin Huang },
  journal={ arXiv preprint arXiv:2406.16815v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2406.16815v1 },
  abstract={ High-fidelity 3D garment synthesis from text is desirable yet challenging for digital avatar creation. Recent diffusion-based approaches via Score Distillation Sampling (SDS) have enabled new possibilities but either intricately couple with human body or struggle to reuse. We introduce ClotheDreamer, a 3D Gaussian-based method for generating wearable, production-ready 3D garment assets from text prompts. We propose a novel representation Disentangled Clothe Gaussian Splatting (DCGS) to enable separate optimization. DCGS represents clothed avatar as one Gaussian model but freezes body Gaussian splats. To enhance quality and completeness, we incorporate bidirectional SDS to supervise clothed avatar and garment RGBD renderings respectively with pose conditions and propose a new pruning strategy for loose clothing. Our approach can also support custom clothing templates as input. Benefiting from our design, the synthetic 3D garment can be easily applied to virtual try-on and support physically accurate animation. Extensive experiments showcase our method's superior and competitive performance. Our project page is at https://ggxxii.github.io/clothedreamer. }
}

@article{240613807v2,
  title={ AlanaVLM: A Multimodal Embodied AI Foundation Model for Egocentric Video   Understanding },
  author={ Alessandro Suglia and Claudio Greco and Katie Baker and Jose L. Part and Ioannis Papaioannou and Arash Eshghi and Ioannis Konstas and Oliver Lemon },
  journal={ arXiv preprint arXiv:2406.13807v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2406.13807v2 },
  abstract={ AI personal assistants deployed via robots or wearables require embodied understanding to collaborate with humans effectively. However, current Vision-Language Models (VLMs) primarily focus on third-person view videos, neglecting the richness of egocentric perceptual experience. To address this gap, we propose three key contributions. First, we introduce the Egocentric Video Understanding Dataset (EVUD) for training VLMs on video captioning and question answering tasks specific to egocentric videos. Second, we present AlanaVLM, a 7B parameter VLM trained using parameter-efficient methods on EVUD. Finally, we evaluate AlanaVLM's capabilities on OpenEQA, a challenging benchmark for embodied video question answering. Our model achieves state-of-the-art performance, outperforming open-source models including strong Socratic models using GPT-4 as a planner by 3.6\%. Additionally, we outperform Claude 3 and Gemini Pro Vision 1.0 and showcase competitive results compared to Gemini Pro 1.5 and GPT-4V, even surpassing the latter in spatial reasoning. This research paves the way for building efficient VLMs that can be deployed in robots or wearables, leveraging embodied video understanding to collaborate seamlessly with humans in everyday tasks, contributing to the next generation of Embodied AI. }
}

@article{220602909v3,
  title={ Self-supervised Learning for Human Activity Recognition Using 700,000   Person-days of Wearable Data },
  author={ Hang Yuan and Shing Chan and Andrew P. Creagh and Catherine Tong and Aidan Acquah and David A. Clifton and Aiden Doherty },
  journal={ arXiv preprint arXiv:2206.02909v3 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2206.02909v3 },
  abstract={ Advances in deep learning for human activity recognition have been relatively limited due to the lack of large labelled datasets. In this study, we leverage self-supervised learning techniques on the UK-Biobank activity tracker dataset--the largest of its kind to date--containing more than 700,000 person-days of unlabelled wearable sensor data. Our resulting activity recognition model consistently outperformed strong baselines across seven benchmark datasets, with an F1 relative improvement of 2.5\%-100\% (median 18.4\%), the largest improvements occurring in the smaller datasets. In contrast to previous studies, our results generalise across external datasets, devices, and environments. Our open-source model will help researchers and developers to build customisable and generalisable activity classifiers with high performance. }
}

@article{240613141v1,
  title={ Implant-to-Wearable Communication through the Human Body: Exploring the   Effects of Encapsulated Capacitive and Galvanic Transmitters },
  author={ Anyu Jiang and Cassandra Acebal and Brook Heyd and Trustin White and Gurleen Kainth and Arunashish Datta and Shreyas Sen and Adam Khalifa and Baibhab Chatterjee },
  journal={ arXiv preprint arXiv:2406.13141v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2406.13141v1 },
  abstract={ Data transfer using human-body communication (HBC) represents an actively explored alternative solution to address the challenges related to energy-efficiency, tissue absorption, and security of conventional wireless. Although the use of HBC for wearable-to-wearable communication has been well-explored, different configurations for the transmitter (Tx) and receiver (Rx) for implant-to-wearable HBC needs further studies. This paper substantiates the hypothesis that a fully implanted galvanic Tx is more efficient than a capacitive Tx for interaction with a wearable Rx. Given the practical limitations of implanting an ideal capacitive device, we choose a galvanic device with one electrode encapsulated to model the capacitive scenario. We analyze the lumped circuit model for in-body to out-of-body communication, and perform Circuit-based as well as Finite Element Method (FEM) simulations to explore how the encapsulation thickness affects the received signal levels. We demonstrate in-vivo experimental results on live Sprague Dawley rats to validate the hypothesis, and show that compared to the galvanic Tx, the channel loss will be \$\\textbackslash{}approx\$ 20 dB higher with each additional mm thickness of capacitive encapsulation, eventually going below the noise floor for ideal capacitive Tx. }
}

@article{240609442v1,
  title={ An insertable glucose sensor using a compact and cost-effective   phosphorescence lifetime imager and machine learning },
  author={ Artem Goncharov and Zoltan Gorocs and Ridhi Pradhan and Brian Ko and Ajmal Ajmal and Andres Rodriguez and David Baum and Marcell Veszpremi and Xilin Yang and Maxime Pindrys and Tianle Zheng and Oliver Wang and Jessica C. Ramella-Roman and Michael J. McShane and Aydogan Ozcan },
  journal={ arXiv preprint arXiv:2406.09442v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2406.09442v1 },
  abstract={ Optical continuous glucose monitoring (CGM) systems are emerging for personalized glucose management owing to their lower cost and prolonged durability compared to conventional electrochemical CGMs. Here, we report a computational CGM system, which integrates a biocompatible phosphorescence-based insertable biosensor and a custom-designed phosphorescence lifetime imager (PLI). This compact and cost-effective PLI is designed to capture phosphorescence lifetime images of an insertable sensor through the skin, where the lifetime of the emitted phosphorescence signal is modulated by the local concentration of glucose. Because this phosphorescence signal has a very long lifetime compared to tissue autofluorescence or excitation leakage processes, it completely bypasses these noise sources by measuring the sensor emission over several tens of microseconds after the excitation light is turned off. The lifetime images acquired through the skin are processed by neural network-based models for misalignment-tolerant inference of glucose levels, accurately revealing normal, low (hypoglycemia) and high (hyperglycemia) concentration ranges. Using a 1-mm thick skin phantom mimicking the optical properties of human skin, we performed in vitro testing of the PLI using glucose-spiked samples, yielding 88.8\% inference accuracy, also showing resilience to random and unknown misalignments within a lateral distance of \~{}4.7 mm with respect to the position of the insertable sensor underneath the skin phantom. Furthermore, the PLI accurately identified larger lateral misalignments beyond 5 mm, prompting user intervention for re-alignment. The misalignment-resilient glucose concentration inference capability of this compact and cost-effective phosphorescence lifetime imager makes it an appealing wearable diagnostics tool for real-time tracking of glucose and other biomarkers. }
}

@article{240605900v1,
  title={ Large Language Models Memorize Sensor Datasets! Implications on Human   Activity Recognition Research },
  author={ Harish Haresamudram and Hrudhai Rajasekhar and Nikhil Murlidhar Shanbhogue and Thomas Ploetz },
  journal={ arXiv preprint arXiv:2406.05900v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2406.05900v1 },
  abstract={ The astonishing success of Large Language Models (LLMs) in Natural Language Processing (NLP) has spurred their use in many application domains beyond text analysis, including wearable sensor-based Human Activity Recognition (HAR). In such scenarios, often sensor data are directly fed into an LLM along with text instructions for the model to perform activity classification. Seemingly remarkable results have been reported for such LLM-based HAR systems when they are evaluated on standard benchmarks from the field. Yet, we argue, care has to be taken when evaluating LLM-based HAR systems in such a traditional way. Most contemporary LLMs are trained on virtually the entire (accessible) internet -- potentially including standard HAR datasets. With that, it is not unlikely that LLMs actually had access to the test data used in such benchmark experiments.The resulting contamination of training data would render these experimental evaluations meaningless. In this paper we investigate whether LLMs indeed have had access to standard HAR datasets during training. We apply memorization tests to LLMs, which involves instructing the models to extend given snippets of data. When comparing the LLM-generated output to the original data we found a non-negligible amount of matches which suggests that the LLM under investigation seems to indeed have seen wearable sensor data from the benchmark datasets during training. For the Daphnet dataset in particular, GPT-4 is able to reproduce blocks of sensor readings. We report on our investigations and discuss potential implications on HAR research, especially with regards to reporting results on experimental evaluation }
}

@article{240601194v2,
  title={ AFF-ttention! Affordances and Attention models for Short-Term Object   Interaction Anticipation },
  author={ Lorenzo Mur-Labadia and Ruben Martinez-Cantin and Josechu Guerrero and Giovanni Maria Farinella and Antonino Furnari },
  journal={ arXiv preprint arXiv:2406.01194v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2406.01194v2 },
  abstract={ Short-Term object-interaction Anticipation consists of detecting the location of the next-active objects, the noun and verb categories of the interaction, and the time to contact from the observation of egocentric video. This ability is fundamental for wearable assistants or human robot interaction to understand the user goals, but there is still room for improvement to perform STA in a precise and reliable way. In this work, we improve the performance of STA predictions with two contributions: 1. We propose STAformer, a novel attention-based architecture integrating frame guided temporal pooling, dual image-video attention, and multiscale feature fusion to support STA predictions from an image-input video pair. 2. We introduce two novel modules to ground STA predictions on human behavior by modeling affordances.First, we integrate an environment affordance model which acts as a persistent memory of interactions that can take place in a given physical scene. Second, we predict interaction hotspots from the observation of hands and object trajectories, increasing confidence in STA predictions localized around the hotspot. Our results show significant relative Overall Top-5 mAP improvements of up to +45\% on Ego4D and +42\% on a novel set of curated EPIC-Kitchens STA labels. We will release the code, annotations, and pre extracted affordances on Ego4D and EPIC- Kitchens to encourage future research in this area. }
}

@article{240519349v1,
  title={ Beyond Isolated Frames: Enhancing Sensor-Based Human Activity   Recognition through Intra- and Inter-Frame Attention },
  author={ Shuai Shao and Yu Guan and Victor Sanchez },
  journal={ arXiv preprint arXiv:2405.19349v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2405.19349v1 },
  abstract={ Human Activity Recognition (HAR) has become increasingly popular with ubiquitous computing, driven by the popularity of wearable sensors in fields like healthcare and sports. While Convolutional Neural Networks (ConvNets) have significantly contributed to HAR, they often adopt a frame-by-frame analysis, concentrating on individual frames and potentially overlooking the broader temporal dynamics inherent in human activities. To address this, we propose the intra- and inter-frame attention model. This model captures both the nuances within individual frames and the broader contextual relationships across multiple frames, offering a comprehensive perspective on sequential data. We further enrich the temporal understanding by proposing a novel time-sequential batch learning strategy. This learning strategy preserves the chronological sequence of time-series data within each batch, ensuring the continuity and integrity of temporal patterns in sensor-based HAR. }
}

@article{240519348v1,
  title={ NERULA: A Dual-Pathway Self-Supervised Learning Framework for   Electrocardiogram Signal Analysis },
  author={ Gouthamaan Manimaran and Sadasivan Puthusserypady and Helena Domínguez and Adrian Atienza and Jakob E. Bardram },
  journal={ arXiv preprint arXiv:2405.19348v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2405.19348v1 },
  abstract={ Electrocardiogram (ECG) signals are critical for diagnosing heart conditions and capturing detailed cardiac patterns. As wearable single-lead ECG devices become more common, efficient analysis methods are essential. We present NERULA (Non-contrastive ECG and Reconstruction Unsupervised Learning Algorithm), a self-supervised framework designed for single-lead ECG signals. NERULA's dual-pathway architecture combines ECG reconstruction and non-contrastive learning to extract detailed cardiac features. Our 50\% masking strategy, using both masked and inverse-masked signals, enhances model robustness against real-world incomplete or corrupted data. The non-contrastive pathway aligns representations of masked and inverse-masked signals, while the reconstruction pathway comprehends and reconstructs missing features. We show that combining generative and discriminative paths into the training spectrum leads to better results by outperforming state-of-the-art self-supervised learning benchmarks in various tasks, demonstrating superior performance in ECG analysis, including arrhythmia classification, gender classification, age regression, and human activity recognition. NERULA's dual-pathway design offers a robust, efficient solution for comprehensive ECG signal interpretation. }
}

@article{240512420v1,
  title={ GarmentDreamer: 3DGS Guided Garment Synthesis with Diverse Geometry and   Texture Details },
  author={ Boqian Li and Xuan Li and Ying Jiang and Tianyi Xie and Feng Gao and Huamin Wang and Yin Yang and Chenfanfu Jiang },
  journal={ arXiv preprint arXiv:2405.12420v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2405.12420v1 },
  abstract={ Traditional 3D garment creation is labor-intensive, involving sketching, modeling, UV mapping, and texturing, which are time-consuming and costly. Recent advances in diffusion-based generative models have enabled new possibilities for 3D garment generation from text prompts, images, and videos. However, existing methods either suffer from inconsistencies among multi-view images or require additional processes to separate cloth from the underlying human model. In this paper, we propose GarmentDreamer, a novel method that leverages 3D Gaussian Splatting (GS) as guidance to generate wearable, simulation-ready 3D garment meshes from text prompts. In contrast to using multi-view images directly predicted by generative models as guidance, our 3DGS guidance ensures consistent optimization in both garment deformation and texture synthesis. Our method introduces a novel garment augmentation module, guided by normal and RGBA information, and employs implicit Neural Texture Fields (NeTF) combined with Score Distillation Sampling (SDS) to generate diverse geometric and texture details. We validate the effectiveness of our approach through comprehensive qualitative and quantitative experiments, showcasing the superior performance of GarmentDreamer over state-of-the-art alternatives. Our project page is available at: https://xuan-li.github.io/GarmentDreamerDemo/. }
}

@article{240500712v2,
  title={ SoK: Behind the Accuracy of Complex Human Activity Recognition Using   Deep Learning },
  author={ Duc-Anh Nguyen and Nhien-An Le-Khac },
  journal={ arXiv preprint arXiv:2405.00712v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2405.00712v2 },
  abstract={ Human Activity Recognition (HAR) is a well-studied field with research dating back to the 1980s. Over time, HAR technologies have evolved significantly from manual feature extraction, rule-based algorithms, and simple machine learning models to powerful deep learning models, from one sensor type to a diverse array of sensing modalities. The scope has also expanded from recognising a limited set of activities to encompassing a larger variety of both simple and complex activities. However, there still exist many challenges that hinder advancement in complex activity recognition using modern deep learning methods. In this paper, we comprehensively systematise factors leading to inaccuracy in complex HAR, such as data variety and model capacity. Among many sensor types, we give more attention to wearable and camera due to their prevalence. Through this Systematisation of Knowledge (SoK) paper, readers can gain a solid understanding of the development history and existing challenges of HAR, different categorisations of activities, obstacles in deep learning-based complex HAR that impact accuracy, and potential research directions. }
}

@article{240419541v1,
  title={ Ultra Inertial Poser: Scalable Motion Capture and Tracking from Sparse   Inertial Sensors and Ultra-Wideband Ranging },
  author={ Rayan Armani and Changlin Qian and Jiaxi Jiang and Christian Holz },
  journal={ arXiv preprint arXiv:2404.19541v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2404.19541v1 },
  abstract={ While camera-based capture systems remain the gold standard for recording human motion, learning-based tracking systems based on sparse wearable sensors are gaining popularity. Most commonly, they use inertial sensors, whose propensity for drift and jitter have so far limited tracking accuracy. In this paper, we propose Ultra Inertial Poser, a novel 3D full body pose estimation method that constrains drift and jitter in inertial tracking via inter-sensor distances. We estimate these distances across sparse sensor setups using a lightweight embedded tracker that augments inexpensive off-the-shelf 6D inertial measurement units with ultra-wideband radio-based ranging\$-\$dynamically and without the need for stationary reference anchors. Our method then fuses these inter-sensor distances with the 3D states estimated from each sensor Our graph-based machine learning model processes the 3D states and distances to estimate a person's 3D full body pose and translation. To train our model, we synthesize inertial measurements and distance estimates from the motion capture database AMASS. For evaluation, we contribute a novel motion dataset of 10 participants who performed 25 motion types, captured by 6 wearable IMU+UWB trackers and an optical motion capture system, totaling 200 minutes of synchronized sensor data (UIP-DB). Our extensive experiments show state-of-the-art performance for our method over PIP and TIP, reducing position error from \$13.62\$ to \$10.65cm\$ (\$22\\textbackslash{}\%\$ better) and lowering jitter from \$1.56\$ to \$0.055km/s\^{}3\$ (a reduction of \$97\\textbackslash{}\%\$). }
}

@article{240616886v1,
  title={ Sensor Data Augmentation from Skeleton Pose Sequences for Improving   Human Activity Recognition },
  author={ Parham Zolfaghari and Vitor Fortes Rey and Lala Ray and Hyun Kim and Sungho Suh and Paul Lukowicz },
  journal={ arXiv preprint arXiv:2406.16886v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2406.16886v1 },
  abstract={ The proliferation of deep learning has significantly advanced various fields, yet Human Activity Recognition (HAR) has not fully capitalized on these developments, primarily due to the scarcity of labeled datasets. Despite the integration of advanced Inertial Measurement Units (IMUs) in ubiquitous wearable devices like smartwatches and fitness trackers, which offer self-labeled activity data from users, the volume of labeled data remains insufficient compared to domains where deep learning has achieved remarkable success. Addressing this gap, in this paper, we propose a novel approach to improve wearable sensor-based HAR by introducing a pose-to-sensor network model that generates sensor data directly from 3D skeleton pose sequences. our method simultaneously trains the pose-to-sensor network and a human activity classifier, optimizing both data reconstruction and activity recognition. Our contributions include the integration of simultaneous training, direct pose-to-sensor generation, and a comprehensive evaluation on the MM-Fit dataset. Experimental results demonstrate the superiority of our framework with significant performance improvements over baseline methods. }
}

@article{231017861v2,
  title={ Soft Wrist Exosuit Actuated by Fabric Pneumatic Artificial Muscles },
  author={ Katalin Schäffer and Yasemin Ozkan-Aydin and Margaret M. Coad },
  journal={ arXiv preprint arXiv:2310.17861v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2310.17861v2 },
  abstract={ Recently, soft actuator-based exosuits have gained interest, due to their high strength-to-weight ratio, inherent safety, and low cost. We present a novel wrist exosuit actuated by fabric pneumatic artificial muscles that has lightweight wearable components (160 g) and can move the wrist in flexion/extension and ulnar/radial deviation. We derive a model representing the torque exerted by the exosuit and demonstrate the use of the model to choose an optimal design for an example user. We evaluate the accuracy of the model by measuring the exosuit torques throughout the full range of wrist flexion/extension. We show the importance of accounting for the displacement of the mounting points, as this helps to achieve the smallest mean absolute error (0.283 Nm) compared to other models. Furthermore, we present the measurement of the exosuit-actuated range of motion on a passive human wrist. Finally, we demonstrate the device controlling the passive human wrist to move to a desired orientation along a one and a two-degree-of-freedom trajectory. The evaluation results show that, compared to other pneumatically actuated wrist exosuits, the presented exosuit is lightweight and strong (with peak torque of 3.3 Nm) but has a limited range of motion. }
}

@article{240415331v1,
  title={ Comparing Self-Supervised Learning Techniques for Wearable Human   Activity Recognition },
  author={ Sannara Ek and Riccardo Presotto and Gabriele Civitarese and François Portet and Philippe Lalanda and Claudio Bettini },
  journal={ arXiv preprint arXiv:2404.15331v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2404.15331v1 },
  abstract={ Human Activity Recognition (HAR) based on the sensors of mobile/wearable devices aims to detect the physical activities performed by humans in their daily lives. Although supervised learning methods are the most effective in this task, their effectiveness is constrained to using a large amount of labeled data during training. While collecting raw unlabeled data can be relatively easy, annotating data is challenging due to costs, intrusiveness, and time constraints.   To address these challenges, this paper explores alternative approaches for accurate HAR using a limited amount of labeled data. In particular, we have adapted recent Self-Supervised Learning (SSL) algorithms to the HAR domain and compared their effectiveness. We investigate three state-of-the-art SSL techniques of different families: contrastive, generative, and predictive. Additionally, we evaluate the impact of the underlying neural network on the recognition rate by comparing state-of-the-art CNN and transformer architectures.   Our results show that a Masked Auto Encoder (MAE) approach significantly outperforms other SSL approaches, including SimCLR, commonly considered one of the best-performing SSL methods in the HAR domain.   The code and the pre-trained SSL models are publicly available for further research and development. }
}

@article{240403130v1,
  title={ Biodegradable Interactive Materials },
  author={ Zhihan Zhang and Mallory Parker and Kuotian Liao and Jerry Cao and Anandghan Waghmare and Joseph Breda and Chris Matsumura and Serena Eley and Eleftheria Roumeli and Shwetak Patel and Vikram Iyer },
  journal={ arXiv preprint arXiv:2404.03130v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2404.03130v1 },
  abstract={ The sense of touch is fundamental to how we interact with the physical and digital world. Conventional interactive surfaces and tactile interfaces use electronic sensors embedded into objects, however this approach poses serious challenges both for environmental sustainability and a future of truly ubiquitous interaction systems where information is encoded into everyday objects. In this work, we present Biodegradable Interactive Materials: backyard-compostable interactive interfaces that leverage information encoded in material properties. Inspired by natural systems, we propose an architecture that programmatically encodes multidimensional information into materials themselves and combines them with wearable devices that extend human senses to perceive the embedded data. We combine unrefined biological matter from plants and algae like chlorella with natural minerals like graphite and magnetite to produce materials with varying electrical, magnetic, and surface properties. We perform in-depth analysis using physics models, computational simulations, and real-world experiments to characterize their information density and develop decoding methods. Our passive, chip-less materials can robustly encode 12 bits of information, equivalent to 4096 unique classes. We further develop wearable device prototypes that can decode this information during touch interactions using off-the-shelf sensors. We demonstrate sample applications such as customized buttons, tactile maps, and interactive surfaces. We further demonstrate the natural degradation of these interactive materials in degrade outdoors within 21 days and perform a comparative environmental analysis of the benefits of this approach. }
}

@article{231106455v2,
  title={ Aria-NeRF: Multimodal Egocentric View Synthesis },
  author={ Jiankai Sun and Jianing Qiu and Chuanyang Zheng and John Tucker and Javier Yu and Mac Schwager },
  journal={ arXiv preprint arXiv:2311.06455v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2311.06455v2 },
  abstract={ We seek to accelerate research in developing rich, multimodal scene models trained from egocentric data, based on differentiable volumetric ray-tracing inspired by Neural Radiance Fields (NeRFs). The construction of a NeRF-like model from an egocentric image sequence plays a pivotal role in understanding human behavior and holds diverse applications within the realms of VR/AR. Such egocentric NeRF-like models may be used as realistic simulations, contributing significantly to the advancement of intelligent agents capable of executing tasks in the real-world. The future of egocentric view synthesis may lead to novel environment representations going beyond today's NeRFs by augmenting visual data with multimodal sensors such as IMU for egomotion tracking, audio sensors to capture surface texture and human language context, and eye-gaze trackers to infer human attention patterns in the scene. To support and facilitate the development and evaluation of egocentric multimodal scene modeling, we present a comprehensive multimodal egocentric video dataset. This dataset offers a comprehensive collection of sensory data, featuring RGB images, eye-tracking camera footage, audio recordings from a microphone, atmospheric pressure readings from a barometer, positional coordinates from GPS, connectivity details from Wi-Fi and Bluetooth, and information from dual-frequency IMU datasets (1kHz and 800Hz) paired with a magnetometer. The dataset was collected with the Meta Aria Glasses wearable device platform. The diverse data modalities and the real-world context captured within this dataset serve as a robust foundation for furthering our understanding of human behavior and enabling more immersive and intelligent experiences in the realms of VR, AR, and robotics. }
}

@article{240306998v2,
  title={ High-speed Low-consumption sEMG-based Transient-state micro-Gesture   Recognition },
  author={ Youfang Han and Wei Zhao and Xiangjin Chen and Xin Meng },
  journal={ arXiv preprint arXiv:2403.06998v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2403.06998v2 },
  abstract={ Gesture recognition on wearable devices is extensively applied in human-computer interaction. Electromyography (EMG) has been used in many gesture recognition systems for its rapid perception of muscle signals. However, analyzing EMG signals on devices, like smart wristbands, usually needs inference models to have high performances, such as low inference latency, low power consumption, and low memory occupation. Therefore, this paper proposes an improved spiking neural network (SNN) to achieve these goals. We propose an adaptive multi-delta coding as a spiking coding method to improve recognition accuracy. We propose two additive solvers for SNN, which can reduce inference energy consumption and amount of parameters significantly, and improve the robustness of temporal differences. In addition, we propose a linear action detection method TAD-LIF, which is suitable for SNNs. TAD-LIF is an improved LIF neuron that can detect transient-state gestures quickly and accurately. We collected two datasets from 20 subjects including 6 micro gestures. The collection devices are two designed lightweight consumer-level sEMG wristbands (3 and 8 electrode channels respectively). Compared to CNN, FCN, and normal SNN-based methods, the proposed SNN has higher recognition accuracy. The accuracy of the proposed SNN is 83.85\% and 93.52\% on the two datasets respectively. In addition, the inference latency of the proposed SNN is about 1\% of CNN, the power consumption is about 0.1\% of CNN, and the memory occupation is about 20\% of CNN. The proposed methods can be used for precise, high-speed, and low-power micro-gesture recognition tasks, and are suitable for consumer-level intelligent wearable devices, which is a general way to achieve ubiquitous computing. }
}

@article{240304387v1,
  title={ Comparison of Deep Learning Techniques on Human Activity Recognition   using Ankle Inertial Signals },
  author={ Farhad Nazari and Darius Nahavandi and Navid Mohajer and Abbas Khosravi },
  journal={ arXiv preprint arXiv:2403.04387v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2403.04387v1 },
  abstract={ Human Activity Recognition (HAR) is one of the fundamental building blocks of human assistive devices like orthoses and exoskeletons. There are different approaches to HAR depending on the application. Numerous studies have been focused on improving them by optimising input data or classification algorithms. However, most of these studies have been focused on applications like security and monitoring, smart devices, the internet of things, etc. On the other hand, HAR can help adjust and control wearable assistive devices, yet there has not been enough research facilitating its implementation. In this study, we propose several models to predict four activities from inertial sensors located in the ankle area of a lower-leg assistive device user. This choice is because they do not need to be attached to the user's skin and can be directly implemented inside the control unit of the device. The proposed models are based on Artificial Neural Networks and could achieve up to 92.8\% average classification accuracy }
}

@article{230214595v3,
  title={ MateRobot: Material Recognition in Wearable Robotics for People with   Visual Impairments },
  author={ Junwei Zheng and Jiaming Zhang and Kailun Yang and Kunyu Peng and Rainer Stiefelhagen },
  journal={ arXiv preprint arXiv:2302.14595v3 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2302.14595v3 },
  abstract={ People with Visual Impairments (PVI) typically recognize objects through haptic perception. Knowing objects and materials before touching is desired by the target users but under-explored in the field of human-centered robotics. To fill this gap, in this work, a wearable vision-based robotic system, MateRobot, is established for PVI to recognize materials and object categories beforehand. To address the computational constraints of mobile platforms, we propose a lightweight yet accurate model MateViT to perform pixel-wise semantic segmentation, simultaneously recognizing both objects and materials. Our methods achieve respective 40.2\% and 51.1\% of mIoU on COCOStuff-10K and DMS datasets, surpassing the previous method with +5.7\% and +7.0\% gains. Moreover, on the field test with participants, our wearable system reaches a score of 28 in the NASA-Task Load Index, indicating low cognitive demands and ease of use. Our MateRobot demonstrates the feasibility of recognizing material property through visual cues and offers a promising step towards improving the functionality of wearable robots for PVI. The source code has been made publicly available at https://junweizheng93.github.io/publications/MATERobot/MATERobot.html. }
}

@article{240302135v1,
  title={ Memoro: Using Large Language Models to Realize a Concise Interface for   Real-Time Memory Augmentation },
  author={ Wazeer Zulfikar and Samantha Chan and Pattie Maes },
  journal={ arXiv preprint arXiv:2403.02135v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2403.02135v1 },
  abstract={ People have to remember an ever-expanding volume of information. Wearables that use information capture and retrieval for memory augmentation can help but can be disruptive and cumbersome in real-world tasks, such as in social settings. To address this, we developed Memoro, a wearable audio-based memory assistant with a concise user interface. Memoro uses a large language model (LLM) to infer the user's memory needs in a conversational context, semantically search memories, and present minimal suggestions. The assistant has two interaction modes: Query Mode for voicing queries and Queryless Mode for on-demand predictive assistance, without explicit query. Our study of (N=20) participants engaged in a real-time conversation demonstrated that using Memoro reduced device interaction time and increased recall confidence while preserving conversational quality. We report quantitative results and discuss the preferences and experiences of users. This work contributes towards utilizing LLMs to design wearable memory augmentation systems that are minimally disruptive. }
}

@article{240301229v1,
  title={ REWIND Dataset: Privacy-preserving Speaking Status Segmentation from   Multimodal Body Movement Signals in the Wild },
  author={ Jose Vargas Quiros and Chirag Raman and Stephanie Tan and Ekin Gedik and Laura Cabrera-Quiros and Hayley Hung },
  journal={ arXiv preprint arXiv:2403.01229v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2403.01229v1 },
  abstract={ Recognizing speaking in humans is a central task towards understanding social interactions. Ideally, speaking would be detected from individual voice recordings, as done previously for meeting scenarios. However, individual voice recordings are hard to obtain in the wild, especially in crowded mingling scenarios due to cost, logistics, and privacy concerns. As an alternative, machine learning models trained on video and wearable sensor data make it possible to recognize speech by detecting its related gestures in an unobtrusive, privacy-preserving way. These models themselves should ideally be trained using labels obtained from the speech signal. However, existing mingling datasets do not contain high quality audio recordings. Instead, speaking status annotations have often been inferred by human annotators from video, without validation of this approach against audio-based ground truth. In this paper we revisit no-audio speaking status estimation by presenting the first publicly available multimodal dataset with high-quality individual speech recordings of 33 subjects in a professional networking event. We present three baselines for no-audio speaking status segmentation: a) from video, b) from body acceleration (chest-worn accelerometer), c) from body pose tracks. In all cases we predict a 20Hz binary speaking status signal extracted from the audio, a time resolution not available in previous datasets. In addition to providing the signals and ground truth necessary to evaluate a wide range of speaking status detection methods, the availability of audio in REWIND makes it suitable for cross-modality studies not feasible with previous mingling datasets. Finally, our flexible data consent setup creates new challenges for multimodal systems under missing modalities. }
}

@article{240307926v1,
  title={ Value Prediction for Spatiotemporal Gait Data Using Deep Learning },
  author={ Ryan Cavanagh and Jelena Trajkovic and Wenlu Zhang and I-Hung Khoo and Vennila Krishnan },
  journal={ arXiv preprint arXiv:2403.07926v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2403.07926v1 },
  abstract={ Human gait has been commonly used for the diagnosis and evaluation of medical conditions and for monitoring the progress during treatment and rehabilitation. The use of wearable sensors that capture pressure or motion has yielded techniques that analyze the gait data to aid recovery, identify activity performed, or identify individuals. Deep learning, usually employing classification, has been successfully utilized in a variety of applications such as computer vision, biomedical imaging analysis, and natural language processing. We expand the application of deep learning to value prediction of time-series of spatiotemporal gait data. Moreover, we explore several deep learning architectures (Recurrent Neural Networks (RNN) and RNN combined with Convolutional Neural Networks (CNN)) to make short- and long-distance predictions using two different experimental setups. Our results show that short-distance prediction has an RMSE as low as 0.060675, and long-distance prediction RMSE as low as 0.106365. Additionally, the results show that the proposed deep learning models are capable of predicting the entire trial when trained and validated using the trials from the same participant. The proposed, customized models, used with value prediction open possibilities for additional applications, such as fall prediction, in-home progress monitoring, aiding of exoskeleton movement, and authentication. }
}

@article{240219229v1,
  title={ CAPTURE-24: A large dataset of wrist-worn activity tracker data   collected in the wild for human activity recognition },
  author={ Shing Chan and Hang Yuan and Catherine Tong and Aidan Acquah and Abram Schonfeldt and Jonathan Gershuny and Aiden Doherty },
  journal={ arXiv preprint arXiv:2402.19229v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2402.19229v1 },
  abstract={ Existing activity tracker datasets for human activity recognition are typically obtained by having participants perform predefined activities in an enclosed environment under supervision. This results in small datasets with a limited number of activities and heterogeneity, lacking the mixed and nuanced movements normally found in free-living scenarios. As such, models trained on laboratory-style datasets may not generalise out of sample. To address this problem, we introduce a new dataset involving wrist-worn accelerometers, wearable cameras, and sleep diaries, enabling data collection for over 24 hours in a free-living setting. The result is CAPTURE-24, a large activity tracker dataset collected in the wild from 151 participants, amounting to 3883 hours of accelerometer data, of which 2562 hours are annotated. CAPTURE-24 is two to three orders of magnitude larger than existing publicly available datasets, which is critical to developing accurate human activity recognition models. }
}

@article{240217171v1,
  title={ LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free   Environment },
  author={ Yiming Ren and Xiao Han and Chengfeng Zhao and Jingya Wang and Lan Xu and Jingyi Yu and Yuexin Ma },
  journal={ arXiv preprint arXiv:2402.17171v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2402.17171v1 },
  abstract={ For human-centric large-scale scenes, fine-grained modeling for 3D human global pose and shape is significant for scene understanding and can benefit many real-world applications. In this paper, we present LiveHPS, a novel single-LiDAR-based approach for scene-level human pose and shape estimation without any limitation of light conditions and wearable devices. In particular, we design a distillation mechanism to mitigate the distribution-varying effect of LiDAR point clouds and exploit the temporal-spatial geometric and dynamic information existing in consecutive frames to solve the occlusion and noise disturbance. LiveHPS, with its efficient configuration and high-quality output, is well-suited for real-world applications. Moreover, we propose a huge human motion dataset, named FreeMotion, which is collected in various scenarios with diverse human poses, shapes and translations. It consists of multi-modal and multi-view acquisition data from calibrated and synchronized LiDARs, cameras, and IMUs. Extensive experiments on our new dataset and other public datasets demonstrate the SOTA performance and robustness of our approach. We will release our code and dataset soon. }
}

@article{240209761v1,
  title={ A Framework For Gait-Based User Demography Estimation Using Inertial   Sensors },
  author={ Chinmay Prakash Swami },
  journal={ arXiv preprint arXiv:2402.09761v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2402.09761v1 },
  abstract={ Human gait has been shown to provide crucial motion cues for various applications. Recognizing patterns in human gait has been widely adopted in various application areas such as security, virtual reality gaming, medical rehabilitation, and ailment identification. Furthermore, wearable inertial sensors have been widely used for not only recording gait but also to predict users' demography. Machine Learning techniques such as deep learning, combined with inertial sensor signals, have shown promising results in recognizing patterns in human gait and estimate users' demography. However, the black-box nature of such deep learning models hinders the researchers from uncovering the reasons behind the model's predictions. Therefore, we propose leveraging deep learning and Layer-Wise Relevance Propagation (LRP) to identify the important variables that play a vital role in identifying the users' demography such as age and gender. To assess the efficacy of this approach we train a deep neural network model on a large sensor-based gait dataset consisting of 745 subjects to identify users' age and gender. Using LRP we identify the variables relevant for characterizing the gait patterns. Thus, we enable interpretation of non-linear ML models which are experts in identifying the users' demography based on inertial signals. We believe this approach can not only provide clinicians information about the gait parameters relevant to age and gender but also can be expanded to analyze and diagnose gait disorders. }
}

@article{240205700v1,
  title={ RF Energy Absorption in Human Bodies Due to Wearable Antennas in the 2.4   GHz Frequency Band },
  author={ Marta Fernandez and Hugo G. Espinosa and David Guerra and Ivan Pena and David V. Thiel and Amaia Arrinda },
  journal={ arXiv preprint arXiv:2402.05700v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2402.05700v1 },
  abstract={ Human exposure to electromagnetic fields produced by two wearable antennas operating in the 2.4 GHz frequency band was assessed by computational tools. Both antennas were designed to be attached to the skin, but they were intended for different applications. The first antenna was designed for off-body applications, i.e. to communicate with a device placed outside the body, while the second antenna model was optimized to communicate with a device located inside the body. The power absorption in human tissues was determined at several locations of adult male and female body models. The maximum specific absorption rate (SAR) value obtained with the off-body antenna was found on the torso of the woman model and was equal to 0.037 W/kg at 2.45 GHz. SAR levels increased significantly for the antenna transmitting inside the body. In this case, SAR values ranged between 0.23 and 0.45 W/kg at the same body location. The power absorbed in different body tissues and total power absorbed in the body were also calculated; the maximum total power absorbed was equal to 5.2 mW for an antenna input power equal to 10 mW. }
}

@article{240202910v2,
  title={ DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage   Temporal Convolutional Network },
  author={ Meng Shang and Lenore Dedeyne and Jolan Dupont and Laura Vercauteren and Nadjia Amini and Laurence Lapauw and Evelien Gielen and Sabine Verschueren and Carolina Varon and Walter De Raedt and Bart Vanrumste },
  journal={ arXiv preprint arXiv:2402.02910v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2402.02910v2 },
  abstract={ The Otago Exercise Program (OEP) represents a crucial rehabilitation initiative tailored for older adults, aimed at enhancing balance and strength. Despite previous efforts utilizing wearable sensors for OEP recognition, existing studies have exhibited limitations in terms of accuracy and robustness. This study addresses these limitations by employing a single waist-mounted Inertial Measurement Unit (IMU) to recognize OEP exercises among community-dwelling older adults in their daily lives. A cohort of 36 older adults participated in laboratory settings, supplemented by an additional 7 older adults recruited for at-home assessments. The study proposes a Dual-Scale Multi-Stage Temporal Convolutional Network (DS-MS-TCN) designed for two-level sequence-to-sequence classification, incorporating them in one loss function. In the first stage, the model focuses on recognizing each repetition of the exercises (micro labels). Subsequent stages extend the recognition to encompass the complete range of exercises (macro labels). The DS-MS-TCN model surpasses existing state-of-the-art deep learning models, achieving f1-scores exceeding 80\% and Intersection over Union (IoU) f1-scores surpassing 60\% for all four exercises evaluated. Notably, the model outperforms the prior study utilizing the sliding window technique, eliminating the need for post-processing stages and window size tuning. To our knowledge, we are the first to present a novel perspective on enhancing Human Activity Recognition (HAR) systems through the recognition of each repetition of activities. }
}

@article{231018562v2,
  title={ Optimization-Free Test-Time Adaptation for Cross-Person Activity   Recognition },
  author={ Shuoyuan Wang and Jindong Wang and HuaJun Xi and Bob Zhang and Lei Zhang and Hongxin Wei },
  journal={ arXiv preprint arXiv:2310.18562v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2310.18562v2 },
  abstract={ Human Activity Recognition (HAR) models often suffer from performance degradation in real-world applications due to distribution shifts in activity patterns across individuals. Test-Time Adaptation (TTA) is an emerging learning paradigm that aims to utilize the test stream to adjust predictions in real-time inference, which has not been explored in HAR before. However, the high computational cost of optimization-based TTA algorithms makes it intractable to run on resource-constrained edge devices. In this paper, we propose an Optimization-Free Test-Time Adaptation (OFTTA) framework for sensor-based HAR. OFTTA adjusts the feature extractor and linear classifier simultaneously in an optimization-free manner. For the feature extractor, we propose Exponential DecayTest-time Normalization (EDTN) to replace the conventional batch normalization (CBN) layers. EDTN combines CBN and Test-time batch Normalization (TBN) to extract reliable features against domain shifts with TBN's influence decreasing exponentially in deeper layers. For the classifier, we adjust the prediction by computing the distance between the feature and the prototype, which is calculated by a maintained support set. In addition, the update of the support set is based on the pseudo label, which can benefit from reliable features extracted by EDTN. Extensive experiments on three public cross-person HAR datasets and two different TTA settings demonstrate that OFTTA outperforms the state-of-the-art TTA approaches in both classification performance and computational efficiency. Finally, we verify the superiority of our proposed OFTTA on edge devices, indicating possible deployment in real applications. Our code is available at https://github.com/Claydon-Wang/OFTTA. }
}

@article{231003512v2,
  title={ Otago Exercises Monitoring for Older Adults by a Single IMU and   Hierarchical Machine Learning Models },
  author={ Meng Shang and Lenore Dedeyne and Jolan Dupont and Laura Vercauteren and Nadjia Amini and Laurence Lapauw and Evelien Gielen and Sabine Verschueren and Carolina Varon and Walter De Raedt and Bart Vanrumste },
  journal={ arXiv preprint arXiv:2310.03512v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2310.03512v2 },
  abstract={ Otago Exercise Program (OEP) is a rehabilitation program for older adults to improve frailty, sarcopenia, and balance. Accurate monitoring of patient involvement in OEP is challenging, as self-reports (diaries) are often unreliable. With the development of wearable sensors, Human Activity Recognition (HAR) systems using wearable sensors have revolutionized healthcare. However, their usage for OEP still shows limited performance. The objective of this study is to build an unobtrusive and accurate system to monitor OEP for older adults. Data was collected from older adults wearing a single waist-mounted Inertial Measurement Unit (IMU). Two datasets were collected, one in a laboratory setting, and one at the homes of the patients. A hierarchical system is proposed with two stages: 1) using a deep learning model to recognize whether the patients are performing OEP or activities of daily life (ADLs) using a 10-minute sliding window; 2) based on stage 1, using a 6-second sliding window to recognize the OEP sub-classes performed. The results showed that in stage 1, OEP could be recognized with window-wise f1-scores over 0.95 and Intersection-over-Union (IoU) f1-scores over 0.85 for both datasets. In stage 2, for the home scenario, four activities could be recognized with f1-scores over 0.8: ankle plantarflexors, abdominal muscles, knee bends, and sit-to-stand. The results showed the potential of monitoring the compliance of OEP using a single IMU in daily life. Also, some OEP sub-classes are possible to be recognized for further analysis. }
}

@article{240117217v2,
  title={ GazeGPT: Augmenting Human Capabilities using Gaze-contingent Contextual   AI for Smart Eyewear },
  author={ Robert Konrad and Nitish Padmanaban and J. Gabriel Buckmaster and Kevin C. Boyle and Gordon Wetzstein },
  journal={ arXiv preprint arXiv:2401.17217v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2401.17217v2 },
  abstract={ Multimodal large language models (LMMs) excel in world knowledge and problem-solving abilities. Through the use of a world-facing camera and contextual AI, emerging smart accessories aim to provide a seamless interface between humans and LMMs. Yet, these wearable computing systems lack an understanding of the user's attention. We introduce GazeGPT as a new user interaction paradigm for contextual AI. GazeGPT uses eye tracking to help the LMM understand which object in the world-facing camera view a user is paying attention to. Using extensive user evaluations, we show that this gaze-contingent mechanism is a faster and more accurate pointing mechanism than alternatives; that it augments human capabilities by significantly improving their accuracy in a dog-breed classification task; and that it is consistently ranked as more natural than head- or body-driven selection mechanisms for contextual AI. Moreover, we prototype a variety of application scenarios that suggest GazeGPT could be of significant value to users as part of future AI-driven personal assistants. }
}

@article{240209434v1,
  title={ Disentangling Imperfect: A Wavelet-Infused Multilevel Heterogeneous   Network for Human Activity Recognition in Flawed Wearable Sensor Data },
  author={ Mengna Liu and Dong Xiang and Xu Cheng and Xiufeng Liu and Dalin Zhang and Shengyong Chen and Christian S. Jensen },
  journal={ arXiv preprint arXiv:2402.09434v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2402.09434v1 },
  abstract={ The popularity and diffusion of wearable devices provides new opportunities for sensor-based human activity recognition that leverages deep learning-based algorithms. Although impressive advances have been made, two major challenges remain. First, sensor data is often incomplete or noisy due to sensor placement and other issues as well as data transmission failure, calling for imputation of missing values, which also introduces noise. Second, human activity has multi-scale characteristics. Thus, different groups of people and even the same person may behave differently under different circumstances. To address these challenges, we propose a multilevel heterogeneous neural network, called MHNN, for sensor data analysis. We utilize multilevel discrete wavelet decomposition to extract multi-resolution features from sensor data. This enables distinguishing signals with different frequencies, thereby suppressing noise. As the components resulting from the decomposition are heterogeneous, we equip the proposed model with heterogeneous feature extractors that enable the learning of multi-scale features. Due to the complementarity of these features, we also include a cross aggregation module for enhancing their interactions. An experimental study using seven publicly available datasets offers evidence that MHNN can outperform other cutting-edge models and offers evidence of robustness to missing values and noise. An ablation study confirms the importance of each module. }
}

@article{230509802v3,
  title={ Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large   Language Models },
  author={ Evan King and Haoxiang Yu and Sangsu Lee and Christine Julien },
  journal={ arXiv preprint arXiv:2305.09802v3 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2305.09802v3 },
  abstract={ Smart home assistants function best when user commands are direct and well-specified (e.g., ''turn on the kitchen light''), or when a hard-coded routine specifies the response. In more natural communication, however, human speech is unconstrained, often describing goals (e.g., ''make it cozy in here'' or ''help me save energy'') rather than indicating specific target devices and actions to take on those devices. Current systems fail to understand these under-specified commands since they cannot reason about devices and settings as they relate to human situations. We introduce large language models (LLMs) to this problem space, exploring their use for controlling devices and creating automation routines in response to under-specified user commands in smart homes. We empirically study the baseline quality and failure modes of LLM-created action plans with a survey of age-diverse users. We find that LLMs can reason creatively to achieve challenging goals, but they experience patterns of failure that diminish their usefulness. We address these gaps with Sasha, a smarter smart home assistant. Sasha responds to loosely-constrained commands like ''make it cozy'' or ''help me sleep better'' by executing plans to achieve user goals, e.g., setting a mood with available devices, or devising automation routines. We implement and evaluate Sasha in a hands-on user study, showing the capabilities and limitations of LLM-driven smart homes when faced with unconstrained user-generated scenarios. }
}

@article{240112464v1,
  title={ Estimation of posture and joint angle of human body using foot pressure   distribution: Morphological computation with human foot },
  author={ Yo Kobayashi and Yasutaka Nakashima },
  journal={ arXiv preprint arXiv:2401.12464v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2401.12464v1 },
  abstract={ This paper proposes a novel contact and wearable sensing system for estimating the upper body posture and joint angles (ankle, knee, and hip) of the human body using foot pressure distribution information obtained from a sensor attached to the plantar region. In the proposed estimation method, sensors are installed only on the plantar region, which is the end of the human body and the point of contact with the environment. The posture and joint angles of other parts of the body are estimated using only this information. As a contact and wearable sensor, the proposed system differs from previous measurement systems in the sense that the sensor does not need to be placed near the target joint or body. The estimation was carried out using a multivariate linear regression model with the foot pressure distribution as the input and the joint angle or posture as the output. The results reveal that it is possible to estimate the posture and joint angles of the human body from foot pressure distribution information (R2\$\\textbackslash{}fallingdotseq\$0.9). The proposed estimation method was validated by morphological computation to confirm that it is enabled by foot morphology. The validation approach compared the estimation accuracy achieved when an object was interposed between the foot pressure distribution sensor and the plantar region and the morphological relationship of the plantar region to the environment varied. The results reveal that there is a significant difference in the estimation accuracy between cases with and without an intervening object, suggesting that the morphology of the plantar region contributes to the estimation. Furthermore, the proposed estimation method is considered as physical reservoir computing, wherein the human foot is used as a computational resource. }
}

@article{240415279v1,
  title={ Jointly Modeling Spatio-Temporal Features of Tactile Signals for Action   Classification },
  author={ Jimmy Lin and Junkai Li and Jiasi Gao and Weizhi Ma and Yang Liu },
  journal={ arXiv preprint arXiv:2404.15279v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2404.15279v1 },
  abstract={ Tactile signals collected by wearable electronics are essential in modeling and understanding human behavior. One of the main applications of tactile signals is action classification, especially in healthcare and robotics. However, existing tactile classification methods fail to capture the spatial and temporal features of tactile signals simultaneously, which results in sub-optimal performances. In this paper, we design Spatio-Temporal Aware tactility Transformer (STAT) to utilize continuous tactile signals for action classification. We propose spatial and temporal embeddings along with a new temporal pretraining task in our model, which aims to enhance the transformer in modeling the spatio-temporal features of tactile signals. Specially, the designed temporal pretraining task is to differentiate the time order of tubelet inputs to model the temporal properties explicitly. Experimental results on a public action classification dataset demonstrate that our model outperforms state-of-the-art methods in all metrics. }
}

@article{240105437v2,
  title={ Representation Learning for Wearable-Based Applications in the Case of   Missing Data },
  author={ Janosch Jungo and Yutong Xiang and Shkurta Gashi and Christian Holz },
  journal={ arXiv preprint arXiv:2401.05437v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2401.05437v2 },
  abstract={ Wearable devices continuously collect sensor data and use it to infer an individual's behavior, such as sleep, physical activity, and emotions. Despite the significant interest and advancements in this field, modeling multimodal sensor data in real-world environments is still challenging due to low data quality and limited data annotations. In this work, we investigate representation learning for imputing missing wearable data and compare it with state-of-the-art statistical approaches. We investigate the performance of the transformer model on 10 physiological and behavioral signals with different masking ratios. Our results show that transformers outperform baselines for missing data imputation of signals that change more frequently, but not for monotonic signals. We further investigate the impact of imputation strategies and masking rations on downstream classification tasks. Our study provides insights for the design and development of masking-based self-supervised learning tasks and advocates the adoption of hybrid-based imputation strategies to address the challenge of missing data in wearable devices. }
}

@article{240105477v1,
  title={ Standardizing Your Training Process for Human Activity Recognition   Models: A Comprehensive Review in the Tunable Factors },
  author={ Yiran Huang and Haibin Zhao and Yexu Zhou and Till Riedel and Michael Beigl },
  journal={ arXiv preprint arXiv:2401.05477v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2401.05477v1 },
  abstract={ In recent years, deep learning has emerged as a potent tool across a multitude of domains, leading to a surge in research pertaining to its application in the wearable human activity recognition (WHAR) domain. Despite the rapid development, concerns have been raised about the lack of standardization and consistency in the procedures used for experimental model training, which may affect the reproducibility and reliability of research results. In this paper, we provide an exhaustive review of contemporary deep learning research in the field of WHAR and collate information pertaining to the training procedure employed in various studies. Our findings suggest that a major trend is the lack of detail provided by model training protocols. Besides, to gain a clearer understanding of the impact of missing descriptions, we utilize a control variables approach to assess the impact of key tunable components (e.g., optimization techniques and early stopping criteria) on the inter-subject generalization capabilities of HAR models. With insights from the analyses, we define a novel integrated training procedure tailored to the WHAR model. Empirical results derived using five well-known \\textbackslash{}ac\{whar\} benchmark datasets and three classical HAR model architectures demonstrate the effectiveness of our proposed methodology: in particular, there is a significant improvement in macro F1 leave one subject out cross-validation performance. }
}

@article{240102255v1,
  title={ Balancing Continual Learning and Fine-tuning for Human Activity   Recognition },
  author={ Chi Ian Tang and Lorena Qendro and Dimitris Spathis and Fahim Kawsar and Akhil Mathur and Cecilia Mascolo },
  journal={ arXiv preprint arXiv:2401.02255v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2401.02255v1 },
  abstract={ Wearable-based Human Activity Recognition (HAR) is a key task in human-centric machine learning due to its fundamental understanding of human behaviours. Due to the dynamic nature of human behaviours, continual learning promises HAR systems that are tailored to users' needs. However, because of the difficulty in collecting labelled data with wearable sensors, existing approaches that focus on supervised continual learning have limited applicability, while unsupervised continual learning methods only handle representation learning while delaying classifier training to a later stage. This work explores the adoption and adaptation of CaSSLe, a continual self-supervised learning model, and Kaizen, a semi-supervised continual learning model that balances representation learning and down-stream classification, for the task of wearable-based HAR. These schemes re-purpose contrastive learning for knowledge retention and, Kaizen combines that with self-training in a unified scheme that can leverage unlabelled and labelled data for continual learning. In addition to comparing state-of-the-art self-supervised continual learning schemes, we further investigated the importance of different loss terms and explored the trade-off between knowledge retention and learning from new tasks. In particular, our extensive evaluation demonstrated that the use of a weighting factor that reflects the ratio between learned and new classes achieves the best overall trade-off in continual learning. }
}

@article{240105412v1,
  title={ Spatial-Related Sensors Matters: 3D Human Motion Reconstruction Assisted   with Textual Semantics },
  author={ Xueyuan Yang and Chao Yao and Xiaojuan Ban },
  journal={ arXiv preprint arXiv:2401.05412v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2401.05412v1 },
  abstract={ Leveraging wearable devices for motion reconstruction has emerged as an economical and viable technique. Certain methodologies employ sparse Inertial Measurement Units (IMUs) on the human body and harness data-driven strategies to model human poses. However, the reconstruction of motion based solely on sparse IMUs data is inherently fraught with ambiguity, a consequence of numerous identical IMU readings corresponding to different poses. In this paper, we explore the spatial importance of multiple sensors, supervised by text that describes specific actions. Specifically, uncertainty is introduced to derive weighted features for each IMU. We also design a Hierarchical Temporal Transformer (HTT) and apply contrastive learning to achieve precise temporal and feature alignment of sensor data with textual semantics. Experimental results demonstrate our proposed approach achieves significant improvements in multiple metrics compared to existing methods. Notably, with textual supervision, our method not only differentiates between ambiguous actions such as sitting and standing but also produces more precise and natural motion. }
}

@article{210913613v5,
  title={ Sonogenetics is a novel antiarrhythmic treatment },
  author={ Yang Li and Xingang Wang and Jianzhong Guo and Yong Wang and Vladimir Zykov and Eberhard Bodenschatz and Xiang Gao },
  journal={ arXiv preprint arXiv:2109.13613v5 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2109.13613v5 },
  abstract={ Arrhythmia of the heart is a dangerous and potentially fatal condition. The current widely used treatment is the implantable cardioverter defibrillator (ICD), but it is invasive and affects the patient's quality of life. The sonogenetic treatment technique proposed here focuses transthoracic ultrasound on the heart, noninvasively stimulates endogenous stretch-activated Piezo1 ion channels on the focal region's cardiomyocyte plasma membrane, and restores normal heart rhythm. In contrast to anchoring the implanted ICD lead at a fixed position in the myocardium, the size and position of the ultrasound focal region can be selected dynamically by adjusting the signal phases of every piezoelectric chip on the wearable ultrasonic phased array, and it allows novel and efficient defibrillations. Based on the developed interdisciplinary electromechanical model of sonogenetic treatment, our analysis shows that the proposed ultrasound intensity and frequency will be safe and painless for humans and well below the limits established by the U.S. Food and Drug Administration. }
}

@article{231204147v1,
  title={ An Improved Masking Strategy for Self-supervised Masked Reconstruction   in Human Activity Recognition },
  author={ Jinqiang Wang and Tao Zhu and Huansheng Ning },
  journal={ arXiv preprint arXiv:2312.04147v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2312.04147v1 },
  abstract={ Masked reconstruction serves as a fundamental pretext task for self-supervised learning, enabling the model to enhance its feature extraction capabilities by reconstructing the masked segments from extensive unlabeled data. In human activity recognition, this pretext task employed a masking strategy centered on the time dimension. However, this masking strategy fails to fully exploit the inherent characteristics of wearable sensor data and overlooks the inter-channel information coupling, thereby limiting its potential as a powerful pretext task. To address these limitations, we propose a novel masking strategy called Channel Masking. It involves masking the sensor data along the channel dimension, thereby compelling the encoder to extract channel-related features while performing the masked reconstruction task. Moreover, Channel Masking can be seamlessly integrated with masking strategies along the time dimension, thereby motivating the self-supervised model to undertake the masked reconstruction task in both the time and channel dimensions. Integrated masking strategies are named Time-Channel Masking and Span-Channel Masking. Finally, we optimize the reconstruction loss function to incorporate the reconstruction loss in both the time and channel dimensions. We evaluate proposed masking strategies on three public datasets, and experimental results show that the proposed strategies outperform prior strategies in both self-supervised and semi-supervised scenarios. }
}

@article{231104236v1,
  title={ Distributed Agent-Based Collaborative Learning in Cross-Individual   Wearable Sensor-Based Human Activity Recognition },
  author={ Ahmad Esmaeili and Zahra Ghorrati and Eric T. Matson },
  journal={ arXiv preprint arXiv:2311.04236v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2311.04236v1 },
  abstract={ The rapid growth of wearable sensor technologies holds substantial promise for the field of personalized and context-aware Human Activity Recognition. Given the inherently decentralized nature of data sources within this domain, the utilization of multi-agent systems with their inherent decentralization capabilities presents an opportunity to facilitate the development of scalable, adaptable, and privacy-conscious methodologies. This paper introduces a collaborative distributed learning approach rooted in multi-agent principles, wherein individual users of sensor-equipped devices function as agents within a distributed network, collectively contributing to the comprehensive process of learning and classifying human activities. In this proposed methodology, not only is the privacy of activity monitoring data upheld for each individual, eliminating the need for an external server to oversee the learning process, but the system also exhibits the potential to surmount the limitations of conventional centralized models and adapt to the unique attributes of each user. The proposed approach has been empirically tested on two publicly accessible human activity recognition datasets, specifically PAMAP2 and HARTH, across varying settings. The provided empirical results conclusively highlight the efficacy of inter-individual collaborative learning when contrasted with centralized configurations, both in terms of local and global generalization. }
}

@article{231010697v1,
  title={ Synthetic IMU Datasets and Protocols Can Simplify Fall Detection   Experiments and Optimize Sensor Configuration },
  author={ Jie Tang and Bin He and Junkai Xu and Tian Tan and Zhipeng Wang and Yanmin Zhou and Shuo Jiang },
  journal={ arXiv preprint arXiv:2310.10697v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2310.10697v1 },
  abstract={ Falls represent a significant cause of injury among the elderly population. Extensive research has been devoted to the utilization of wearable IMU sensors in conjunction with machine learning techniques for fall detection. To address the challenge of acquiring costly training data, this paper presents a novel method that generates a substantial volume of synthetic IMU data with minimal real fall experiments. First, unmarked 3D motion capture technology is employed to reconstruct human movements. Subsequently, utilizing the biomechanical simulation platform Opensim and forward kinematic methods, an ample amount of training data from various body segments can be custom generated. An LSTM model is trained, achieving testing accuracies of 91.99\% and 86.62\% on two distinct datasets of actual fall-related IMU data, demonstrated the comparable performance of models trained using genuine IMU data. Building upon the simulation framework, this paper further optimized the single IMU attachment position and multiple IMU combinations on fall detection. The proposed method simplifies fall detection data acquisition experiments, provides novel venue for generating low cost synthetic data in scenario where acquiring data for machine learning is challenging and paves the way for customizing machine learning configurations. }
}

@article{231010029v1,
  title={ A Human Motion Compensation Framework for a Supernumerary Robotic Arm },
  author={ Xin Zhang and Pietro Balatti and Mattia Leonori and Arash Ajoudani },
  journal={ arXiv preprint arXiv:2310.10029v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2310.10029v1 },
  abstract={ Supernumerary robotic arms (SRAs) can be used as the third arm to complement and augment the abilities of human users. The user carrying a SRA forms a connected kinodynamic chain, which can be viewed as a special class of floating-base robot systems. However, unlike general floating-base robot systems, human users are the bases of SRAs and they have their subjective behaviors/motions. This implies that human body motions can unintentionally affect the SRA's end-effector movements. To address this challenge, we propose a framework to compensate for the human whole-body motions that interfere with the SRA's end-effector trajectories. The SRA system in this study consists of a 6-degree-of-freedom lightweight arm and a wearable interface. The wearable interface allows users to adjust the installation position of the SRA to fit different body shapes. An inertial measurement unit (IMU)-based sensory interface can provide the body skeleton motion feedback of the human user in real time. By simplifying the floating-base kinematics model, we design an effective motion planner by reconstructing the Jacobian matrix of the SRA. Under the proposed framework, the performance of the reconstructed Jacobian method is assessed by comparing the results obtained with the classical nullspace-based method through two sets of experiments. }
}

@article{231009114v1,
  title={ Timestamp-supervised Wearable-based Activity Segmentation and   Recognition with Contrastive Learning and Order-Preserving Optimal Transport },
  author={ Songpengcheng Xia and Lei Chu and Ling Pei and Jiarui Yang and Wenxian Yu and Robert C. Qiu },
  journal={ arXiv preprint arXiv:2310.09114v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2310.09114v1 },
  abstract={ Human activity recognition (HAR) with wearables is one of the serviceable technologies in ubiquitous and mobile computing applications. The sliding-window scheme is widely adopted while suffering from the multi-class windows problem. As a result, there is a growing focus on joint segmentation and recognition with deep-learning methods, aiming at simultaneously dealing with HAR and time-series segmentation issues. However, obtaining the full activity annotations of wearable data sequences is resource-intensive or time-consuming, while unsupervised methods yield poor performance. To address these challenges, we propose a novel method for joint activity segmentation and recognition with timestamp supervision, in which only a single annotated sample is needed in each activity segment. However, the limited information of sparse annotations exacerbates the gap between recognition and segmentation tasks, leading to sub-optimal model performance. Therefore, the prototypes are estimated by class-activation maps to form a sample-to-prototype contrast module for well-structured embeddings. Moreover, with the optimal transport theory, our approach generates the sample-level pseudo-labels that take advantage of unlabeled data between timestamp annotations for further performance improvement. Comprehensive experiments on four public HAR datasets demonstrate that our model trained with timestamp supervision is superior to the state-of-the-art weakly-supervised methods and achieves comparable performance to the fully-supervised approaches. }
}

@article{231112829v1,
  title={ Intelligent Knee Sleeves: A Real-time Multimodal Dataset for 3D Lower   Body Motion Estimation Using Smart Textile },
  author={ Wenwen Zhang and Arvin Tashakori and Zenan Jiang and Amir Servati and Harishkumar Narayana and Saeid Soltanian and Rou Yi Yeap and Meng Han Ma and Lauren Toy and Peyman Servati },
  journal={ arXiv preprint arXiv:2311.12829v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2311.12829v1 },
  abstract={ The kinematics of human movements and locomotion are closely linked to the activation and contractions of muscles. To investigate this, we present a multimodal dataset with benchmarks collected using a novel pair of Intelligent Knee Sleeves (Texavie MarsWear Knee Sleeves) for human pose estimation. Our system utilizes synchronized datasets that comprise time-series data from the Knee Sleeves and the corresponding ground truth labels from the visualized motion capture camera system. We employ these to generate 3D human models solely based on the wearable data of individuals performing different activities. We demonstrate the effectiveness of this camera-free system and machine learning algorithms in the assessment of various movements and exercises, including extension to unseen exercises and individuals. The results show an average error of 7.21 degrees across all eight lower body joints when compared to the ground truth, indicating the effectiveness and reliability of the Knee Sleeve system for the prediction of different lower body joints beyond the knees. The results enable human pose estimation in a seamless manner without being limited by visual occlusion or the field of view of cameras. Our results show the potential of multimodal wearable sensing in a variety of applications from home fitness to sports, healthcare, and physical rehabilitation focusing on pose and movement estimation. }
}

@article{230315585v3,
  title={ Beyond Accuracy: A Critical Review of Fairness in Machine Learning for   Mobile and Wearable Computing },
  author={ Sofia Yfantidou and Marios Constantinides and Dimitris Spathis and Athena Vakali and Daniele Quercia and Fahim Kawsar },
  journal={ arXiv preprint arXiv:2303.15585v3 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2303.15585v3 },
  abstract={ The field of mobile and wearable computing is undergoing a revolutionary integration of machine learning. Devices can now diagnose diseases, predict heart irregularities, and unlock the full potential of human cognition. However, the underlying algorithms powering these predictions are not immune to biases with respect to sensitive attributes (e.g., gender, race), leading to discriminatory outcomes. The goal of this work is to explore the extent to which the mobile and wearable computing community has adopted ways of reporting information about datasets and models to surface and, eventually, counter biases. Our systematic review of papers published in the Proceedings of the ACM Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) journal from 2018-2022 indicates that, while there has been progress made on algorithmic fairness, there is still ample room for growth. Our findings show that only a small portion (5\%) of published papers adheres to modern fairness reporting, while the overwhelming majority thereof focuses on accuracy or error metrics. To generalize these results across venues of similar scope, we analyzed recent proceedings of ACM MobiCom, MobiSys, and SenSys, IEEE Pervasive, and IEEE Transactions on Mobile Computing Computing, and found no deviation from our primary result. In light of these findings, our work provides practical guidelines for the design and development of mobile and wearable technologies that not only strive for accuracy but also fairness. }
}

@article{230912602v1,
  title={ ViT-MDHGR: Cross-day Reliability and Agility in Dynamic Hand Gesture   Prediction via HD-sEMG Signal Decoding },
  author={ Qin Hu and Golara Ahmadi Azar and Alyson Fletcher and Sundeep Rangan and S. Farokh Atashzar },
  journal={ arXiv preprint arXiv:2309.12602v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2309.12602v1 },
  abstract={ Surface electromyography (sEMG) and high-density sEMG (HD-sEMG) biosignals have been extensively investigated for myoelectric control of prosthetic devices, neurorobotics, and more recently human-computer interfaces because of their capability for hand gesture recognition/prediction in a wearable and non-invasive manner. High intraday (same-day) performance has been reported. However, the interday performance (separating training and testing days) is substantially degraded due to the poor generalizability of conventional approaches over time, hindering the application of such techniques in real-life practices. There are limited recent studies on the feasibility of multi-day hand gesture recognition. The existing studies face a major challenge: the need for long sEMG epochs makes the corresponding neural interfaces impractical due to the induced delay in myoelectric control. This paper proposes a compact ViT-based network for multi-day dynamic hand gesture prediction. We tackle the main challenge as the proposed model only relies on very short HD-sEMG signal windows (i.e., 50 ms, accounting for only one-sixth of the convention for real-time myoelectric implementation), boosting agility and responsiveness. Our proposed model can predict 11 dynamic gestures for 20 subjects with an average accuracy of over 71\% on the testing day, 3-25 days after training. Moreover, when calibrated on just a small portion of data from the testing day, the proposed model can achieve over 92\% accuracy by retraining less than 10\% of the parameters for computational efficiency. }
}

@article{230701666v2,
  title={ Sensors and Systems for Monitoring Mental Fatigue: A systematic review },
  author={ Prabin Sharma and Joanna C. Justus and Megha Thapa and Govinda R. Poudel },
  journal={ arXiv preprint arXiv:2307.01666v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2307.01666v2 },
  abstract={ Mental fatigue is a leading cause of motor vehicle accidents, medical errors, loss of workplace productivity, and student disengagements in e-learning environment. Development of sensors and systems that can reliably track mental fatigue can prevent accidents, reduce errors, and help increase workplace productivity. This review provides a critical summary of theoretical models of mental fatigue, a description of key enabling sensor technologies, and a systematic review of recent studies using biosensor-based systems for tracking mental fatigue in humans. We conducted a systematic search and review of recent literature which focused on detection and tracking of mental fatigue in humans. The search yielded 57 studies (N=1082), majority of which used electroencephalography (EEG) based sensors for tracking mental fatigue. We found that EEG-based sensors can provide a moderate to good sensitivity for fatigue detection. Notably, we found no incremental benefit of using high-density EEG sensors for application in mental fatigue detection. Given the findings, we provide a critical discussion on the integration of wearable EEG and ambient sensors in the context of achieving real-world monitoring. Future work required to advance and adapt the technologies toward widespread deployment of wearable sensors and systems for fatigue monitoring in semi-autonomous and autonomous industries is examined. }
}

@article{230813936v1,
  title={ Learning Human-arm Reaching Motion Using IMU in Human-Robot   Collaboration },
  author={ Nadav D. Kahanowich and Avishai Sintov },
  journal={ arXiv preprint arXiv:2308.13936v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2308.13936v1 },
  abstract={ Many tasks performed by two humans require mutual interaction between arms such as handing-over tools and objects. In order for a robotic arm to interact with a human in the same way, it must reason about the location of the human arm in real-time. Furthermore and to acquire interaction in a timely manner, the robot must be able predict the final target of the human in order to plan and initiate motion beforehand. In this paper, we explore the use of a low-cost wearable device equipped with two inertial measurement units (IMU) for learning reaching motion for real-time applications of Human-Robot Collaboration (HRC). A wearable device can replace or be complementary to visual perception in cases of bad lighting or occlusions in a cluttered environment. We first train a neural-network model to estimate the current location of the arm. Then, we propose a novel model based on a recurrent neural-network to predict the future target of the human arm during motion in real-time. Early prediction of the target grants the robot with sufficient time to plan and initiate motion during the motion of the human. The accuracies of the models are analyzed concerning the features included in the motion representation. Through experiments and real demonstrations with a robotic arm, we show that sufficient accuracy is achieved for feasible HRC without any visual perception. Once trained, the system can be deployed in various spaces with no additional effort. The models exhibit high accuracy for various initial poses of the human arm. Moreover, the trained models are shown to provide high success rates with additional human participants not included in the model training. }
}

@article{230810559v2,
  title={ Metaverse: A Vision, Architectural Elements, and Future Directions for   Scalable and Realtime Virtual Worlds },
  author={ Leila Ismail and Rajkumar Buyya },
  journal={ arXiv preprint arXiv:2308.10559v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2308.10559v2 },
  abstract={ With the emergence of Cloud computing, Internet of Things-enabled Human-Computer Interfaces, Generative Artificial Intelligence, and high-accurate Machine and Deep-learning recognition and predictive models, along with the Post Covid-19 proliferation of social networking, and remote communications, the Metaverse gained a lot of popularity. Metaverse has the prospective to extend the physical world using virtual and augmented reality so the users can interact seamlessly with the real and virtual worlds using avatars and holograms. It has the potential to impact people in the way they interact on social media, collaborate in their work, perform marketing and business, teach, learn, and even access personalized healthcare. Several works in the literature examine Metaverse in terms of hardware wearable devices, and virtual reality gaming applications. However, the requirements of realizing the Metaverse in realtime and at a large-scale need yet to be examined for the technology to be usable. To address this limitation, this paper presents the temporal evolution of Metaverse definitions and captures its evolving requirements. Consequently, we provide insights into Metaverse requirements. In addition to enabling technologies, we lay out architectural elements for scalable, reliable, and efficient Metaverse systems, and a classification of existing Metaverse applications along with proposing required future research directions. }
}

@article{230811673v1,
  title={ WEARS: Wearable Emotion AI with Real-time Sensor data },
  author={ Dhruv Limbani and Daketi Yatin and Nitish Chaturvedi and Vaishnavi Moorthy and Pushpalatha M and Harichandana BSS and Sumit Kumar },
  journal={ arXiv preprint arXiv:2308.11673v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2308.11673v1 },
  abstract={ Emotion prediction is the field of study to understand human emotions. Existing methods focus on modalities like text, audio, facial expressions, etc., which could be private to the user. Emotion can be derived from the subject's psychological data as well. Various approaches that employ combinations of physiological sensors for emotion recognition have been proposed. Yet, not all sensors are simple to use and handy for individuals in their daily lives. Thus, we propose a system to predict user emotion using smartwatch sensors. We design a framework to collect ground truth in real-time utilizing a mix of English and regional language-based videos to invoke emotions in participants and collect the data. Further, we modeled the problem as binary classification due to the limited dataset size and experimented with multiple machine-learning models. We also did an ablation study to understand the impact of features including Heart Rate, Accelerometer, and Gyroscope sensor data on mood. From the experimental results, Multi-Layer Perceptron has shown a maximum accuracy of 93.75 percent for pleasant-unpleasant (high/low valence classification) moods. }
}

@article{230807670v1,
  title={ Ongoing Tracking of Engagement in Motor Learning },
  author={ Segev Shlomov and Jonathan Muehlstein and Nitzan Guetta and Lior Limonad },
  journal={ arXiv preprint arXiv:2308.07670v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2308.07670v1 },
  abstract={ Teaching motor skills such as playing music, handwriting, and driving, can greatly benefit from recently developed technologies such as wearable gloves for haptic feedback or robotic sensorimotor exoskeletons for the mediation of effective human-human and robot-human physical interactions. At the heart of such teacher-learner interactions still stands the critical role of the ongoing feedback a teacher can get about the student's engagement state during the learning and practice sessions. Particularly for motor learning, such feedback is an essential functionality in a system that is developed to guide a teacher on how to control the intensity of the physical interaction, and to best adapt it to the gradually evolving performance of the learner. In this paper, our focus is on the development of a near real-time machine-learning model that can acquire its input from a set of readily available, noninvasive, privacy-preserving, body-worn sensors, for the benefit of tracking the engagement of the learner in the motor task. We used the specific case of violin playing as a target domain in which data were empirically acquired, the latent construct of engagement in motor learning was carefully developed for data labeling, and a machine-learning model was rigorously trained and validated. }
}

@article{230807614v1,
  title={ Single channel based interference-free and self-powered human-machine   interactive interface using eigenfrequency-dominant mechanism },
  author={ Sen Ding and Dazhe Zhao and Yongyao Chen and Ziyi Dai and Qian Zhao and Yibo Gao and Junwen Zhong and Jianyi Luo and Bingpu Zhou },
  journal={ arXiv preprint arXiv:2308.07614v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2308.07614v1 },
  abstract={ The recent development of wearable devices is revolutionizing the way of human-machine interaction (HMI). Nowadays, an interactive interface that carries more embedded information is desired to fulfil the increasing demand in era of Internet of Things. However, present approach normally relies on sensor arrays for memory expansion, which inevitably brings the concern of wiring complexity, signal differentiation, power consumption, and miniaturization. Herein, a one-channel based self-powered HMI interface, which uses the eigenfrequency of magnetized micropillar (MMP) as identification mechanism, is reported. When manually vibrated, the inherent recovery of the MMP caused a damped oscillation that generates current signals because of Faraday's Law of induction. The time-to-frequency conversion explores the MMP-related eigenfrequency, which provides a specific solution to allocate diverse commands in an interference-free behavior even with one electric channel. A cylindrical cantilever model was built to regulate the MMP eigenfrequencies via precisely designing the dimensional parameters and material properties. We show that using one device and two electrodes, high-capacity HMI interface can be realized when the MMPs with different eigenfrequencies have been integrated. This study provides the reference value to design the future HMI system especially for situations that require a more intuitive and intelligent communication experience with high-memory demand. }
}

@article{230802282v1,
  title={ DIVERSIFY: A General Framework for Time Series Out-of-distribution   Detection and Generalization },
  author={ Wang Lu and Jindong Wang and Xinwei Sun and Yiqiang Chen and Xiangyang Ji and Qiang Yang and Xing Xie },
  journal={ arXiv preprint arXiv:2308.02282v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2308.02282v1 },
  abstract={ Time series remains one of the most challenging modalities in machine learning research. The out-of-distribution (OOD) detection and generalization on time series tend to suffer due to its non-stationary property, i.e., the distribution changes over time. The dynamic distributions inside time series pose great challenges to existing algorithms to identify invariant distributions since they mainly focus on the scenario where the domain information is given as prior knowledge. In this paper, we attempt to exploit subdomains within a whole dataset to counteract issues induced by non-stationary for generalized representation learning. We propose DIVERSIFY, a general framework, for OOD detection and generalization on dynamic distributions of time series. DIVERSIFY takes an iterative process: it first obtains the ''worst-case'' latent distribution scenario via adversarial training, then reduces the gap between these latent distributions. We implement DIVERSIFY via combining existing OOD detection methods according to either extracted features or outputs of models for detection while we also directly utilize outputs for classification. In addition, theoretical insights illustrate that DIVERSIFY is theoretically supported. Extensive experiments are conducted on seven datasets with different OOD settings across gesture recognition, speech commands recognition, wearable stress and affect detection, and sensor-based human activity recognition. Qualitative and quantitative results demonstrate that DIVERSIFY learns more generalized features and significantly outperforms other baselines. }
}

@article{230800787v1,
  title={ Evaluating Spiking Neural Network On Neuromorphic Platform For Human   Activity Recognition },
  author={ Sizhen Bian and Michele Magno },
  journal={ arXiv preprint arXiv:2308.00787v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2308.00787v1 },
  abstract={ Energy efficiency and low latency are crucial requirements for designing wearable AI-empowered human activity recognition systems, due to the hard constraints of battery operations and closed-loop feedback. While neural network models have been extensively compressed to match the stringent edge requirements, spiking neural networks and event-based sensing are recently emerging as promising solutions to further improve performance due to their inherent energy efficiency and capacity to process spatiotemporal data in very low latency. This work aims to evaluate the effectiveness of spiking neural networks on neuromorphic processors in human activity recognition for wearable applications. The case of workout recognition with wrist-worn wearable motion sensors is used as a study. A multi-threshold delta modulation approach is utilized for encoding the input sensor data into spike trains to move the pipeline into the event-based approach. The spikes trains are then fed to a spiking neural network with direct-event training, and the trained model is deployed on the research neuromorphic platform from Intel, Loihi, to evaluate energy and latency efficiency. Test results show that the spike-based workouts recognition system can achieve a comparable accuracy (87.5\\textbackslash{}\%) comparable to the popular milliwatt RISC-V bases multi-core processor GAP8 with a traditional neural network ( 88.1\\textbackslash{}\%) while achieving two times better energy-delay product (0.66 \\textbackslash{}si\{\\textbackslash{}micro\\textbackslash{}joule\\textbackslash{}second\} vs. 1.32 \\textbackslash{}si\{\\textbackslash{}micro\\textbackslash{}joule\\textbackslash{}second\}). }
}

@article{230712067v1,
  title={ Replay: Multi-modal Multi-view Acted Videos for Casual Holography },
  author={ Roman Shapovalov and Yanir Kleiman and Ignacio Rocco and David Novotny and Andrea Vedaldi and Changan Chen and Filippos Kokkinos and Ben Graham and Natalia Neverova },
  journal={ arXiv preprint arXiv:2307.12067v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2307.12067v1 },
  abstract={ We introduce Replay, a collection of multi-view, multi-modal videos of humans interacting socially. Each scene is filmed in high production quality, from different viewpoints with several static cameras, as well as wearable action cameras, and recorded with a large array of microphones at different positions in the room. Overall, the dataset contains over 4000 minutes of footage and over 7 million timestamped high-resolution frames annotated with camera poses and partially with foreground masks. The Replay dataset has many potential applications, such as novel-view synthesis, 3D reconstruction, novel-view acoustic synthesis, human body and face analysis, and training generative models. We provide a benchmark for training and evaluating novel-view synthesis, with two scenarios of different difficulty. Finally, we evaluate several baseline state-of-the-art methods on the new benchmark. }
}

@article{230711793v1,
  title={ Leveraging arbitrary mobile sensor trajectories with shallow recurrent   decoder networks for full-state reconstruction },
  author={ Megan R. Ebers and Jan P. Williams and Katherine M. Steele and J. Nathan Kutz },
  journal={ arXiv preprint arXiv:2307.11793v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2307.11793v1 },
  abstract={ Sensing is one of the most fundamental tasks for the monitoring, forecasting and control of complex, spatio-temporal systems. In many applications, a limited number of sensors are mobile and move with the dynamics, with examples including wearable technology, ocean monitoring buoys, and weather balloons. In these dynamic systems (without regions of statistical-independence), the measurement time history encodes a significant amount of information that can be extracted for critical tasks. Most model-free sensing paradigms aim to map current sparse sensor measurements to the high-dimensional state space, ignoring the time-history all together. Using modern deep learning architectures, we show that a sequence-to-vector model, such as an LSTM (long, short-term memory) network, with a decoder network, dynamic trajectory information can be mapped to full state-space estimates. Indeed, we demonstrate that by leveraging mobile sensor trajectories with shallow recurrent decoder networks, we can train the network (i) to accurately reconstruct the full state space using arbitrary dynamical trajectories of the sensors, (ii) the architecture reduces the variance of the mean-square error of the reconstruction error in comparison with immobile sensors, and (iii) the architecture also allows for rapid generalization (parameterization of dynamics) for data outside the training set. Moreover, the path of the sensor can be chosen arbitrarily, provided training data for the spatial trajectory of the sensor is available. The exceptional performance of the network architecture is demonstrated on three applications: turbulent flows, global sea-surface temperature data, and human movement biomechanics. }
}

@article{230704516v1,
  title={ An Examination of Wearable Sensors and Video Data Capture for Human   Exercise Classification },
  author={ Ashish Singh and Antonio Bevilacqua and Timilehin B. Aderinola and Thach Le Nguyen and Darragh Whelan and Martin O'Reilly and Brian Caulfield and Georgiana Ifrim },
  journal={ arXiv preprint arXiv:2307.04516v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2307.04516v1 },
  abstract={ Wearable sensors such as Inertial Measurement Units (IMUs) are often used to assess the performance of human exercise. Common approaches use handcrafted features based on domain expertise or automatically extracted features using time series analysis. Multiple sensors are required to achieve high classification accuracy, which is not very practical. These sensors require calibration and synchronization and may lead to discomfort over longer time periods. Recent work utilizing computer vision techniques has shown similar performance using video, without the need for manual feature engineering, and avoiding some pitfalls such as sensor calibration and placement on the body. In this paper, we compare the performance of IMUs to a video-based approach for human exercise classification on two real-world datasets consisting of Military Press and Rowing exercises. We compare the performance using a single camera that captures video in the frontal view versus using 5 IMUs placed on different parts of the body. We observe that an approach based on a single camera can outperform a single IMU by 10 percentage points on average. Additionally, a minimum of 3 IMUs are required to outperform a single camera. We observe that working with the raw data using multivariate time series classifiers outperforms traditional approaches based on handcrafted or automatically extracted features. Finally, we show that an ensemble model combining the data from a single camera with a single IMU outperforms either data modality. Our work opens up new and more realistic avenues for this application, where a video captured using a readily available smartphone camera, combined with a single sensor, can be used for effective human exercise classification. }
}

@article{230700789v1,
  title={ Utilizing wearable technology to characterize and facilitate occupant   collaborations in flexible workspaces },
  author={ Kristi Maisha and Mario Frei and Matias Quintana and Yun Xuan Chua and Rishee Jain and Clayton Miller },
  journal={ arXiv preprint arXiv:2307.00789v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2307.00789v1 },
  abstract={ Hybrid working strategies have become, and will continue to be, the norm for many offices. This raises two considerations: newly unoccupied spaces needlessly consume energy, and the occupied spaces need to be effectively used to facilitate meaningful interactions and create a positive, sustainable work culture. This work aims to determine when spontaneous, collaborative interactions occur within the building and the environmental factors that facilitate such interactions. This study uses smartwatch-based micro-surveys using the Cozie platform to identify the occurrence of and spatially place interactions while categorizing them as a collaboration or distraction. This method uniquely circumvents pitfalls associated with surveying and qualitative data collection: occupant behaviors are identified in real-time in a non-intrusive manner, and survey data is corroborated with quantitative sensor data. A proof-of-concept study was deployed with nine hybrid-working participants providing 100 micro-survey cluster responses over approximately two weeks. The results show the spontaneous interactions occurring in hybrid mode are split evenly among the categories of collaboration, wanted socialization, and distraction and primarily occur with coworkers at one's desk. From these data, we can establish various correlations between the occurrence of positive spontaneous interactions and different factors, such as the time of day and the locations in the building. This framework and first deployment provide the foundation for future large-scale data collection experiments and human interaction modeling. }
}

@article{230616961v1,
  title={ AI-Powered Interfaces for Extended Reality to support Remote Maintenance },
  author={ Akos Nagy and George Amponis and Konstantinos Kyranou and Thomas Lagkas and Alexandros Apostolos Boulogeorgos and Panagiotis Sarigiannidis and Vasileios Argyriou },
  journal={ arXiv preprint arXiv:2306.16961v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2306.16961v1 },
  abstract={ High-end components that conduct complicated tasks automatically are a part of modern industrial systems. However, in order for these parts to function at the desired level, they need to be maintained by qualified experts. Solutions based on Augmented Reality (AR) have been established with the goal of raising production rates and quality while lowering maintenance costs. With the introduction of two unique interaction interfaces based on wearable targets and human face orientation, we are proposing hands-free advanced interactive solutions in this study with the goal of reducing the bias towards certain users. Using traditional devices in real time, a comparison investigation using alternative interaction interfaces is conducted. The suggested solutions are supported by various AI powered methods such as novel gravity-map based motion adjustment that is made possible by predictive deep models that reduce the bias of traditional hand- or finger-based interaction interfaces }
}

@article{230607802v1,
  title={ Empirical Measurement of Aesthetic Experience of Music },
  author={ Abhishek Gupta and C. M. Markan },
  journal={ arXiv preprint arXiv:2306.07802v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2306.07802v1 },
  abstract={ Chills or goosebumps, also called frisson, is a phenomenon that is often associated with an aesthetic experience e.g., music or some other ecstatic experience. The temporal and spatial cause of frisson in the brain has been one of the biggest mysteries of human nature. Accumulating evidence suggests that aesthetic, namely subjective, affective, and evaluative processes are at play while listening to music, hence, it is an important subjective stimulus for systematic investigation. Advances in neuroimaging and cognitive neuroscience, have given impetus to neuro-aesthetics, a novel approach to music providing a phenomenological brain-based framework for the aesthetic experience of music with the potential to open the scope for future research. In this paper, we present an affordable, wearable, easy-to-carry device to measure phenomenological goosebumps intensity on our skin with respect to real-time data using IoT devices (Raspberry pi 3, model B). To test the device subjects were asked to provide a list of songs that elicit goosebumps. Wireless earphones were provided, allowing participants to walk around and dance while listening to their music. (Some subjects moved during sessions). Results indicate that goosebumps were reliably detected by the device after visual inspection of the videos/music. The effective measurement when interfaced with neurophysiological devices such as electroencephalography (EEG) can help interpret biomarkers of ecstatic emotions. The second part of the study focuses on identifying primary brain regions involved in goosebump experience during musical stimulation. }
}

@article{230602140v1,
  title={ Unsupervised Human Activity Recognition through Two-stage Prompting with   ChatGPT },
  author={ Qingxin Xia and Takuya Maekawa and Takahiro Hara },
  journal={ arXiv preprint arXiv:2306.02140v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2306.02140v1 },
  abstract={ Wearable sensor devices, which offer the advantage of recording daily objects used by a person while performing an activity, enable the feasibility of unsupervised Human Activity Recognition (HAR). Unfortunately, previous unsupervised approaches using the usage sequence of objects usually require a proper description of activities manually prepared by humans. Instead, we leverage the knowledge embedded in a Large Language Model (LLM) of ChatGPT. Because the sequence of objects robustly characterizes the activity identity, it is possible that ChatGPT already learned the association between activities and objects from existing contexts. However, previous prompt engineering for ChatGPT exhibits limited generalization ability when dealing with a list of words (i.e., sequence of objects) due to the similar weighting assigned to each word in the list. In this study, we propose a two-stage prompt engineering, which first guides ChatGPT to generate activity descriptions associated with objects while emphasizing important objects for distinguishing similar activities; then outputs activity classes and explanations for enhancing the contexts that are helpful for HAR. To the best of our knowledge, this is the first study that utilizes ChatGPT to recognize activities using objects in an unsupervised manner. We conducted our approach on three datasets and demonstrated the state-of-the-art performance. }
}

@article{221200724v2,
  title={ SWL-Adapt: An Unsupervised Domain Adaptation Model with Sample Weight   Learning for Cross-User Wearable Human Activity Recognition },
  author={ Rong Hu and Ling Chen and Shenghuan Miao and Xing Tang },
  journal={ arXiv preprint arXiv:2212.00724v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2212.00724v2 },
  abstract={ In practice, Wearable Human Activity Recognition (WHAR) models usually face performance degradation on the new user due to user variance. Unsupervised domain adaptation (UDA) becomes the natural solution to cross-user WHAR under annotation scarcity. Existing UDA models usually align samples across domains without differentiation, which ignores the difference among samples. In this paper, we propose an unsupervised domain adaptation model with sample weight learning (SWL-Adapt) for cross-user WHAR. SWL-Adapt calculates sample weights according to the classification loss and domain discrimination loss of each sample with a parameterized network. We introduce the meta-optimization based update rule to learn this network end-to-end, which is guided by meta-classification loss on the selected pseudo-labeled target samples. Therefore, this network can fit a weighting function according to the cross-user WHAR task at hand, which is superior to existing sample differentiation rules fixed for special scenarios. Extensive experiments on three public WHAR datasets demonstrate that SWL-Adapt achieves the state-of-the-art performance on the cross-user WHAR task, outperforming the best baseline by an average of 3.1\% and 5.3\% in accuracy and macro F1 score, respectively. }
}

@article{230601756v1,
  title={ CSI-Based Efficient Self-Quarantine Monitoring System Using Branchy   Convolution Neural Network },
  author={ Jingtao Guo and Ivan Wang-Hei Ho },
  journal={ arXiv preprint arXiv:2306.01756v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2306.01756v1 },
  abstract={ Nowadays, Coronavirus disease (COVID-19) has become a global pandemic because of its fast spread in various countries. To build an anti-epidemic barrier, self-isolation is required for people who have been to any at-risk places or have been in close contact with infected people. However, existing camera or wearable device-based monitoring systems may present privacy leakage risks or cause user inconvenience in some cases. In this paper, we propose a Wi-Fi-based device-free self-quarantine monitoring system. Specifically, we exploit channel state information (CSI) derived from Wi-Fi signals as human activity features. We collect CSI data in a simulated self-quarantine scenario and present BranchyGhostNet, a lightweight convolution neural network (CNN) with an early exit prediction branch, for the efficient joint task of room occupancy detection (ROD) and human activity recognition (HAR). The early exiting branch is used for ROD, and the final one is used for HAR. Our experimental results indicate that the proposed model can achieve an average accuracy of 98.19\% for classifying five different human activities. They also confirm that after leveraging the early exit prediction mechanism, the inference latency for ROD can be significantly reduced by 54.04\% when compared with the final exiting branch while guaranteeing the accuracy of ROD. }
}

@article{230514490v1,
  title={ Wital: A COTS WiFi Devices Based Vital Signs Monitoring System Using   NLOS Sensing Model },
  author={ Xiang Zhang and Yu Gu and Huan Yan and Yantong Wang and Mianxiong Dong and Kaoru Ota and Fuji Ren and Yusheng Ji },
  journal={ arXiv preprint arXiv:2305.14490v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2305.14490v1 },
  abstract={ Vital sign (breathing and heartbeat) monitoring is essential for patient care and sleep disease prevention. Most current solutions are based on wearable sensors or cameras; however, the former could affect sleep quality, while the latter often present privacy concerns. To address these shortcomings, we propose Wital, a contactless vital sign monitoring system based on low-cost and widespread commercial off-the-shelf (COTS) Wi-Fi devices. There are two challenges that need to be overcome. First, the torso deformations caused by breathing/heartbeats are weak. How can such deformations be effectively captured? Second, movements such as turning over affect the accuracy of vital sign monitoring. How can such detrimental effects be avoided? For the former, we propose a non-line-of-sight (NLOS) sensing model for modeling the relationship between the energy ratio of line-of-sight (LOS) to NLOS signals and the vital sign monitoring capability using Ricean K theory and use this model to guide the system construction to better capture the deformations caused by breathing/heartbeats. For the latter, we propose a motion segmentation method based on motion regularity detection that accurately distinguishes respiration from other motions, and we remove periods that include movements such as turning over to eliminate detrimental effects. We have implemented and validated Wital on low-cost COTS devices. The experimental results demonstrate the effectiveness of Wital in monitoring vital signs. }
}

@article{230513765v1,
  title={ Human Body Pose Estimation for Gait Identification: A Comprehensive   Survey of Datasets and Models },
  author={ Luke K. Topham and Wasiq Khan and Dhiya Al-Jumeily and Abir Hussain },
  journal={ arXiv preprint arXiv:2305.13765v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2305.13765v1 },
  abstract={ Person identification is a problem that has received substantial attention, particularly in security domains. Gait recognition is one of the most convenient approaches enabling person identification at a distance without the need of high-quality images. There are several review studies addressing person identification such as the utilization of facial images, silhouette images, and wearable sensor. Despite skeleton-based person identification gaining popularity while overcoming the challenges of traditional approaches, existing survey studies lack the comprehensive review of skeleton-based approaches to gait identification. We present a detailed review of the human pose estimation and gait analysis that make the skeleton-based approaches possible. The study covers various types of related datasets, tools, methodologies, and evaluation metrics with associated challenges, limitations, and application domains. Detailed comparisons are presented for each of these aspects with recommendations for potential research and alternatives. A common trend throughout this paper is the positive impact that deep learning techniques are beginning to have on topics such as human pose estimation and gait identification. The survey outcomes might be useful for the related research community and other stakeholders in terms of performance analysis of existing methodologies, potential research gaps, application domains, and possible contributions in the future. }
}

@article{230513541v1,
  title={ ConvBoost: Boosting ConvNets for Sensor-based Activity Recognition },
  author={ Shuai Shao and Yu Guan and Bing Zhai and Paolo Missier and Thomas Ploetz },
  journal={ arXiv preprint arXiv:2305.13541v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2305.13541v1 },
  abstract={ Human activity recognition (HAR) is one of the core research themes in ubiquitous and wearable computing. With the shift to deep learning (DL) based analysis approaches, it has become possible to extract high-level features and perform classification in an end-to-end manner. Despite their promising overall capabilities, DL-based HAR may suffer from overfitting due to the notoriously small, often inadequate, amounts of labeled sample data that are available for typical HAR applications. In response to such challenges, we propose ConvBoost -- a novel, three-layer, structured model architecture and boosting framework for convolutional network based HAR. Our framework generates additional training data from three different perspectives for improved HAR, aiming to alleviate the shortness of labeled training data in the field. Specifically, with the introduction of three conceptual layers--Sampling Layer, Data Augmentation Layer, and Resilient Layer -- we develop three ''boosters'' -- R-Frame, Mix-up, and C-Drop -- to enrich the per-epoch training data by dense-sampling, synthesizing, and simulating, respectively. These new conceptual layers and boosters, that are universally applicable for any kind of convolutional network, have been designed based on the characteristics of the sensor data and the concept of frame-wise HAR. In our experimental evaluation on three standard benchmarks (Opportunity, PAMAP2, GOTOV) we demonstrate the effectiveness of our ConvBoost framework for HAR applications based on variants of convolutional networks: vanilla CNN, ConvLSTM, and Attention Models. We achieved substantial performance gains for all of them, which suggests that the proposed approach is generic and can serve as a practical solution for boosting the performance of existing ConvNet-based HAR models. This is an open-source project, and the code can be found at https://github.com/sshao2013/ConvBoost }
}

@article{230508499v1,
  title={ A Control Approach for Human-Robot Ergonomic Payload Lifting },
  author={ Lorenzo Rapetti and Carlotta Sartore and Mohamed Elobaid and Yeshasvi Tirupachuri and Francesco Draicchio and Tomohiro Kawakami and Takahide Yoshiike and Daniele Pucci },
  journal={ arXiv preprint arXiv:2305.08499v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2305.08499v1 },
  abstract={ Collaborative robots can relief human operators from excessive efforts during payload lifting activities. Modelling the human partner allows the design of safe and efficient collaborative strategies. In this paper, we present a control approach for human-robot collaboration based on human monitoring through whole-body wearable sensors, and interaction modelling through coupled rigid-body dynamics. Moreover, a trajectory advancement strategy is proposed, allowing for online adaptation of the robot trajectory depending on the human motion. The resulting framework allows us to perform payload lifting tasks, taking into account the ergonomic requirements of the agents. Validation has been performed in an experimental scenario using the iCub3 humanoid robot and a human subject sensorized with the iFeel wearable system. }
}

@article{230503058v1,
  title={ Plug-and-Play Multilingual Few-shot Spoken Words Recognition },
  author={ Aaqib Saeed and Vasileios Tsouvalas },
  journal={ arXiv preprint arXiv:2305.03058v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2305.03058v1 },
  abstract={ As technology advances and digital devices become prevalent, seamless human-machine communication is increasingly gaining significance. The growing adoption of mobile, wearable, and other Internet of Things (IoT) devices has changed how we interact with these smart devices, making accurate spoken words recognition a crucial component for effective interaction. However, building robust spoken words detection system that can handle novel keywords remains challenging, especially for low-resource languages with limited training data. Here, we propose PLiX, a multilingual and plug-and-play keyword spotting system that leverages few-shot learning to harness massive real-world data and enable the recognition of unseen spoken words at test-time. Our few-shot deep models are learned with millions of one-second audio clips across 20 languages, achieving state-of-the-art performance while being highly efficient. Extensive evaluations show that PLiX can generalize to novel spoken words given as few as just one support example and performs well on unseen languages out of the box. We release models and inference code to serve as a foundation for future research and voice-enabled user interface development for emerging devices. }
}

@article{230410068v1,
  title={ Fruit Picker Activity Recognition with Wearable Sensors and Machine   Learning },
  author={ Joel Janek Dabrowski and Ashfaqur Rahman },
  journal={ arXiv preprint arXiv:2304.10068v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2304.10068v1 },
  abstract={ In this paper we present a novel application of detecting fruit picker activities based on time series data generated from wearable sensors. During harvesting, fruit pickers pick fruit into wearable bags and empty these bags into harvesting bins located in the orchard. Once full, these bins are quickly transported to a cooled pack house to improve the shelf life of picked fruits. For farmers and managers, the knowledge of when a picker bag is emptied is important for managing harvesting bins more effectively to minimise the time the picked fruit is left out in the heat (resulting in reduced shelf life). We propose a means to detect these bag-emptying events using human activity recognition with wearable sensors and machine learning methods. We develop a semi-supervised approach to labelling the data. A feature-based machine learning ensemble model and a deep recurrent convolutional neural network are developed and tested on a real-world dataset. When compared, the neural network achieves 86\% detection accuracy. }
}

@article{230409530v1,
  title={ SelfAct: Personalized Activity Recognition based on Self-Supervised and   Active Learning },
  author={ Luca Arrotta and Gabriele Civitarese and Samuele Valente and Claudio Bettini },
  journal={ arXiv preprint arXiv:2304.09530v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2304.09530v1 },
  abstract={ Supervised Deep Learning (DL) models are currently the leading approach for sensor-based Human Activity Recognition (HAR) on wearable and mobile devices. However, training them requires large amounts of labeled data whose collection is often time-consuming, expensive, and error-prone. At the same time, due to the intra- and inter-variability of activity execution, activity models should be personalized for each user. In this work, we propose SelfAct: a novel framework for HAR combining self-supervised and active learning to mitigate these problems. SelfAct leverages a large pool of unlabeled data collected from many users to pre-train through self-supervision a DL model, with the goal of learning a meaningful and efficient latent representation of sensor data. The resulting pre-trained model can be locally used by new users, which will fine-tune it thanks to a novel unsupervised active learning strategy. Our experiments on two publicly available HAR datasets demonstrate that SelfAct achieves results that are close to or even better than the ones of fully supervised approaches with a small number of active learning queries. }
}

@article{230406777v1,
  title={ Online Recognition of Incomplete Gesture Data to Interface Collaborative   Robots },
  author={ M. A. Simão and O. Gibaru and P. Neto },
  journal={ arXiv preprint arXiv:2304.06777v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2304.06777v1 },
  abstract={ Online recognition of gestures is critical for intuitive human-robot interaction (HRI) and further push collaborative robotics into the market, making robots accessible to more people. The problem is that it is difficult to achieve accurate gesture recognition in real unstructured environments, often using distorted and incomplete multisensory data. This paper introduces an HRI framework to classify large vocabularies of interwoven static gestures (SGs) and dynamic gestures (DGs) captured with wearable sensors. DG features are obtained by applying data dimensionality reduction to raw data from sensors (resampling with cubic interpolation and principal component analysis). Experimental tests were conducted using the UC2017 hand gesture dataset with samples from eight different subjects. The classification models show an accuracy of 95.6\% for a library of 24 SGs with a random forest and 99.3\% for 10 DGs using artificial neural networks. These results compare equally or favorably with different commonly used classifiers. Long short-term memory deep networks achieved similar performance in online frame-by-frame classification using raw incomplete data, performing better in terms of accuracy than static models with specially crafted features, but worse in training and inference time. The recognized gestures are used to teleoperate a robot in a collaborative process that consists in preparing a breakfast meal. }
}

@article{221206301v2,
  title={ Egocentric Video Task Translation },
  author={ Zihui Xue and Yale Song and Kristen Grauman and Lorenzo Torresani },
  journal={ arXiv preprint arXiv:2212.06301v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2212.06301v2 },
  abstract={ Different video understanding tasks are typically treated in isolation, and even with distinct types of curated data (e.g., classifying sports in one dataset, tracking animals in another). However, in wearable cameras, the immersive egocentric perspective of a person engaging with the world around them presents an interconnected web of video understanding tasks -- hand-object manipulations, navigation in the space, or human-human interactions -- that unfold continuously, driven by the person's goals. We argue that this calls for a much more unified approach. We propose EgoTask Translation (EgoT2), which takes a collection of models optimized on separate tasks and learns to translate their outputs for improved performance on any or all of them at once. Unlike traditional transfer or multi-task learning, EgoT2's flipped design entails separate task-specific backbones and a task translator shared across all tasks, which captures synergies between even heterogeneous tasks and mitigates task competition. Demonstrating our model on a wide array of video tasks from Ego4D, we show its advantages over existing transfer paradigms and achieve top-ranked results on four of the Ego4D 2022 benchmark challenges. }
}

@article{230317845v1,
  title={ WSense: A Robust Feature Learning Module for Lightweight Human Activity   Recognition },
  author={ Ayokunle Olalekan Ige and Mohd Halim Mohd Noor },
  journal={ arXiv preprint arXiv:2303.17845v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2303.17845v1 },
  abstract={ In recent times, various modules such as squeeze-and-excitation, and others have been proposed to improve the quality of features learned from wearable sensor signals. However, these modules often cause the number of parameters to be large, which is not suitable for building lightweight human activity recognition models which can be easily deployed on end devices. In this research, we propose a feature learning module, termed WSense, which uses two 1D CNN and global max pooling layers to extract similar quality features from wearable sensor data while ignoring the difference in activity recognition models caused by the size of the sliding window. Experiments were carried out using CNN and ConvLSTM feature learning pipelines on a dataset obtained with a single accelerometer (WISDM) and another obtained using the fusion of accelerometers, gyroscopes, and magnetometers (PAMAP2) under various sliding window sizes. A total of nine hundred sixty (960) experiments were conducted to validate the WSense module against baselines and existing methods on the two datasets. The results showed that the WSense module aided pipelines in learning similar quality features and outperformed the baselines and existing models with a minimal and uniform model size across all sliding window segmentations. The code is available at https://github.com/AOige/WSense. }
}

@article{221001781v2,
  title={ COPILOT: Human-Environment Collision Prediction and Localization from   Egocentric Videos },
  author={ Boxiao Pan and Bokui Shen and Davis Rempe and Despoina Paschalidou and Kaichun Mo and Yanchao Yang and Leonidas J. Guibas },
  journal={ arXiv preprint arXiv:2210.01781v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2210.01781v2 },
  abstract={ The ability to forecast human-environment collisions from egocentric observations is vital to enable collision avoidance in applications such as VR, AR, and wearable assistive robotics. In this work, we introduce the challenging problem of predicting collisions in diverse environments from multi-view egocentric videos captured from body-mounted cameras. Solving this problem requires a generalizable perception system that can classify which human body joints will collide and estimate a collision region heatmap to localize collisions in the environment. To achieve this, we propose a transformer-based model called COPILOT to perform collision prediction and localization simultaneously, which accumulates information across multi-view inputs through a novel 4D space-time-viewpoint attention mechanism. To train our model and enable future research on this task, we develop a synthetic data generation framework that produces egocentric videos of virtual humans moving and colliding within diverse 3D environments. This framework is then used to establish a large-scale dataset consisting of 8.6M egocentric RGBD frames. Extensive experiments show that COPILOT generalizes to unseen synthetic as well as real-world scenes. We further demonstrate COPILOT outputs are useful for downstream collision avoidance through simple closed-loop control. Please visit our project webpage at https://sites.google.com/stanford.edu/copilot. }
}

@article{230311690v1,
  title={ Exposure Assessment for Wearable Patch Antenna Arrays at Millimeter   Waves },
  author={ Silvia Gallucci and Marta Bonato and Martina Benini and Marta Parazzini and Maxim Zhadobov },
  journal={ arXiv preprint arXiv:2303.11690v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2303.11690v1 },
  abstract={ Since the spread of the wearable systems and the implementation of the forthcoming 5G in many devices, the question about the assessment of the exposure in wearable typical usage to millimeter waves is crucial and timely. For such frequencies, the power absorption becomes strongly superficial and involves only the most superficial tissue of the human body, i.e., the skin. In literature there are some models able to describe the layered structure of the skin but, until now, there is no literature consensus on the skin model to employ in computational exposure assessment studies. For these reasons, the present work aimed to simulate four different models of the most superficial tissues with different degree of detail exposed to two wearable patch antennas at different frequencies i.e., 28 GHz and 39 GHz. This allows to investigate the impact that the choice of a layered model rather than the homogeneous one has on the exposure. Simulations were performed through the FDTD method, implemented in the Sim4life platform and the exposure was assessed with the absorbed power density averaged over 1 cm2 and 4 cm2 (Sab). The data showed that the homogeneous model underestimates the peak value of Sab obtained for multi-layer models in the stratum corneum (by 14\% to 21\% depending on the number of layers of the model and the frequency). This finding was confirmed by an analytical approach with two impinging plane wave TEM-polarized with normal incidence at 28 GHz and 39 GHz respectively. Conversely, there are no substantial differences in the exposure levels between the layered models }
}

@article{230307655v1,
  title={ Simultaneous Action Recognition and Human Whole-Body Motion and Dynamics   Prediction from Wearable Sensors },
  author={ Kourosh Darvish and Serena Ivaldi and Daniele Pucci },
  journal={ arXiv preprint arXiv:2303.07655v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2303.07655v1 },
  abstract={ This paper presents a novel approach to solve simultaneously the problems of human activity recognition and whole-body motion and dynamics prediction for real-time applications. Starting from the dynamics of human motion and motor system theory, the notion of mixture of experts from deep learning has been extended to address this problem. In the proposed approach, experts are modelled as a sequence-to-sequence recurrent neural networks (RNN) architecture. Experiments show the results of 66-DoF real-world human motion prediction and action recognition during different tasks like walking and rotating. The code associated with this paper is available at: \\textbackslash{}url\{github.com/ami-iit/paper\_darvish\_2022\_humanoids\_action-kindyn-predicition\} }
}

@article{230306246v1,
  title={ Zone-based Federated Learning for Mobile Sensing Data },
  author={ Xiaopeng Jiang and Thinh On and NhatHai Phan and Hessamaldin Mohammadi and Vijaya Datta Mayyuri and An Chen and Ruoming Jin and Cristian Borcea },
  journal={ arXiv preprint arXiv:2303.06246v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2303.06246v1 },
  abstract={ Mobile apps, such as mHealth and wellness applications, can benefit from deep learning (DL) models trained with mobile sensing data collected by smart phones or wearable devices. However, currently there is no mobile sensing DL system that simultaneously achieves good model accuracy while adapting to user mobility behavior, scales well as the number of users increases, and protects user data privacy. We propose Zone-based Federated Learning (ZoneFL) to address these requirements. ZoneFL divides the physical space into geographical zones mapped to a mobile-edge-cloud system architecture for good model accuracy and scalability. Each zone has a federated training model, called a zone model, which adapts well to data and behaviors of users in that zone. Benefiting from the FL design, the user data privacy is protected during the ZoneFL training. We propose two novel zone-based federated training algorithms to optimize zone models to user mobility behavior: Zone Merge and Split (ZMS) and Zone Gradient Diffusion (ZGD). ZMS optimizes zone models by adapting the zone geographical partitions through merging of neighboring zones or splitting of large zones into smaller ones. Different from ZMS, ZGD maintains fixed zones and optimizes a zone model by incorporating the gradients derived from neighboring zones' data. ZGD uses a self-attention mechanism to dynamically control the impact of one zone on its neighbors. Extensive analysis and experimental results demonstrate that ZoneFL significantly outperforms traditional FL in two models for heart rate prediction and human activity recognition. In addition, we developed a ZoneFL system using Android phones and AWS cloud. The system was used in a heart rate prediction field study with 63 users for 4 months, and we demonstrated the feasibility of ZoneFL in real-life. }
}

@article{220808672v2,
  title={ RRWaveNet: A Compact End-to-End Multi-Scale Residual CNN for Robust PPG   Respiratory Rate Estimation },
  author={ Pongpanut Osathitporn and Guntitat Sawadwuthikul and Punnawish Thuwajit and Kawisara Ueafuea and Thee Mateepithaktham and Narin Kunaseth and Tanut Choksatchawathi and Proadpran Punyabukkana and Emmanuel Mignot and Theerawit Wilaiprasitporn },
  journal={ arXiv preprint arXiv:2208.08672v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2208.08672v2 },
  abstract={ Respiratory rate (RR) is an important biomarker as RR changes can reflect severe medical events such as heart disease, lung disease, and sleep disorders. Unfortunately, standard manual RR counting is prone to human error and cannot be performed continuously. This study proposes a method for continuously estimating RR, RRWaveNet. The method is a compact end-to-end deep learning model which does not require feature engineering and can use low-cost raw photoplethysmography (PPG) as input signal. RRWaveNet was tested subject-independently and compared to baseline in four datasets (BIDMC, CapnoBase, WESAD, and SensAI) and using three window sizes (16, 32, and 64 seconds). RRWaveNet outperformed current state-of-the-art methods with mean absolute errors at optimal window size of 1.66 \\textbackslash{}pm 1.01, 1.59 \\textbackslash{}pm 1.08, 1.92 \\textbackslash{}pm 0.96 and 1.23 \\textbackslash{}pm 0.61 breaths per minute for each dataset. In remote monitoring settings, such as in the WESAD and SensAI datasets, we apply transfer learning to improve the performance using two other ICU datasets as pretraining datasets, reducing the MAE by up to 21\$\\textbackslash{}\%\$. This shows that this model allows accurate and practical estimation of RR on affordable and wearable devices. Our study also shows feasibility of remote RR monitoring in the context of telemedicine and at home. }
}

@article{221102733v2,
  title={ GLOBEM Dataset: Multi-Year Datasets for Longitudinal Human Behavior   Modeling Generalization },
  author={ Xuhai Xu and Han Zhang and Yasaman Sefidgar and Yiyi Ren and Xin Liu and Woosuk Seo and Jennifer Brown and Kevin Kuehn and Mike Merrill and Paula Nurius and Shwetak Patel and Tim Althoff and Margaret E. Morris and Eve Riskin and Jennifer Mankoff and Anind K. Dey },
  journal={ arXiv preprint arXiv:2211.02733v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2211.02733v2 },
  abstract={ Recent research has demonstrated the capability of behavior signals captured by smartphones and wearables for longitudinal behavior modeling. However, there is a lack of a comprehensive public dataset that serves as an open testbed for fair comparison among algorithms. Moreover, prior studies mainly evaluate algorithms using data from a single population within a short period, without measuring the cross-dataset generalizability of these algorithms. We present the first multi-year passive sensing datasets, containing over 700 user-years and 497 unique users' data collected from mobile and wearable sensors, together with a wide range of well-being metrics. Our datasets can support multiple cross-dataset evaluations of behavior modeling algorithms' generalizability across different users and years. As a starting point, we provide the benchmark results of 18 algorithms on the task of depression detection. Our results indicate that both prior depression detection algorithms and domain generalization techniques show potential but need further research to achieve adequate cross-dataset generalizability. We envision our multi-year datasets can support the ML community in developing generalizable longitudinal behavior modeling algorithms. }
}

@article{230306048v1,
  title={ VALERIAN: Invariant Feature Learning for IMU Sensor-based Human Activity   Recognition in the Wild },
  author={ Yujiao Hao and Boyu Wang and Rong Zheng },
  journal={ arXiv preprint arXiv:2303.06048v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2303.06048v1 },
  abstract={ Deep neural network models for IMU sensor-based human activity recognition (HAR) that are trained from controlled, well-curated datasets suffer from poor generalizability in practical deployments. However, data collected from naturalistic settings often contains significant label noise. In this work, we examine two in-the-wild HAR datasets and DivideMix, a state-of-the-art learning with noise labels (LNL) method to understand the extent and impacts of noisy labels in training data. Our empirical analysis reveals that the substantial domain gaps among diverse subjects cause LNL methods to violate a key underlying assumption, namely, neural networks tend to fit simpler (and thus clean) data in early training epochs. Motivated by the insights, we design VALERIAN, an invariant feature learning method for in-the-wild wearable sensor-based HAR. By training a multi-task model with separate task-specific layers for each subject, VALERIAN allows noisy labels to be dealt with individually while benefiting from shared feature representation across subjects. We evaluated VALERIAN on four datasets, two collected in a controlled environment and two in the wild. }
}

@article{230301995v1,
  title={ Spatiotemporal modeling of grip forces captures proficiency in manual   robot control },
  author={ Rongrong Liu and John M. Wandeto and Florent Nageotte and Philippe Zanne and Michel de Mathelin and Birgitta Dresp-Langley },
  journal={ arXiv preprint arXiv:2303.01995v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2303.01995v1 },
  abstract={ This paper builds on our previous work by exploiting Artificial Intelligence to predict individual grip force variability in manual robot control. Grip forces were recorded from various loci in the dominant and non dominant hands of individuals by means of wearable wireless sensor technology. Statistical analyses bring to the fore skill specific temporal variations in thousands of grip forces of a complete novice and a highly proficient expert in manual robot control. A brain inspired neural network model that uses the output metric of a Self Organizing Map with unsupervised winner take all learning was run on the sensor output from both hands of each user. The neural network metric expresses the difference between an input representation and its model representation at any given moment in time t and reliably captures the differences between novice and expert performance in terms of grip force variability.Functionally motivated spatiotemporal analysis of individual average grip forces, computed for time windows of constant size in the output of a restricted amount of task-relevant sensors in the dominant (preferred) hand, reveal finger-specific synergies reflecting robotic task skill. The analyses lead the way towards grip force monitoring in real time to permit tracking task skill evolution in trainees, or identify individual proficiency levels in human robot interaction in environmental contexts of high sensory uncertainty. Parsimonious Artificial Intelligence (AI) assistance will contribute to the outcome of new types of surgery, in particular single-port approaches such as NOTES (Natural Orifice Transluminal Endoscopic Surgery) and SILS (Single Incision Laparoscopic Surgery). }
}

@article{220907027v4,
  title={ Out-of-Distribution Representation Learning for Time Series   Classification },
  author={ Wang Lu and Jindong Wang and Xinwei Sun and Yiqiang Chen and Xing Xie },
  journal={ arXiv preprint arXiv:2209.07027v4 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2209.07027v4 },
  abstract={ Time series classification is an important problem in real world. Due to its non-stationary property that the distribution changes over time, it remains challenging to build models for generalization to unseen distributions. In this paper, we propose to view the time series classification problem from the distribution perspective. We argue that the temporal complexity attributes to the unknown latent distributions within. To this end, we propose DIVERSIFY to learn generalized representations for time series classification. DIVERSIFY takes an iterative process: it first obtains the worst-case distribution scenario via adversarial training, then matches the distributions of the obtained sub-domains. We also present some theoretical insights. We conduct experiments on gesture recognition, speech commands recognition, wearable stress and affect detection, and sensor-based human activity recognition with a total of seven datasets in different settings. Results demonstrate that DIVERSIFY significantly outperforms other baselines and effectively characterizes the latent distributions by qualitative and quantitative analysis. Code is available at: https://github.com/microsoft/robustlearn. }
}

@article{230210672v1,
  title={ Importance of methodological choices in data manipulation for validating   epileptic seizure detection models },
  author={ Una Pale and Tomas Teijeiro and David Atienza },
  journal={ arXiv preprint arXiv:2302.10672v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2302.10672v1 },
  abstract={ Epilepsy is a chronic neurological disorder that affects a significant portion of the human population and imposes serious risks in the daily life of patients. Despite advances in machine learning and IoT, small, nonstigmatizing wearable devices for continuous monitoring and detection in outpatient environments are not yet available. Part of the reason is the complexity of epilepsy itself, including highly imbalanced data, multimodal nature, and very subject-specific signatures. However, another problem is the heterogeneity of methodological approaches in research, leading to slower progress, difficulty comparing results, and low reproducibility. Therefore, this article identifies a wide range of methodological decisions that must be made and reported when training and evaluating the performance of epilepsy detection systems. We characterize the influence of individual choices using a typical ensemble random-forest model and the publicly available CHB-MIT database, providing a broader picture of each decision and giving good-practice recommendations, based on our experience, where possible. }
}

@article{201113370v2,
  title={ Tensional twist-folding of sheets into multilayered scrolled yarns },
  author={ Julien Chopin and Arshad Kudrolli },
  journal={ arXiv preprint arXiv:2011.13370v2 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2011.13370v2 },
  abstract={ Twisting sheets as a strategy to form functional yarns relies on millennia of human practice in making catguts and fabric wearables, but still lacks overarching principles to guide their intricate architectures. We show that twisted hyperelastic sheets form multilayered self-scrolled yarns, through recursive folding and twist localization, that can be reconfigured and redeployed. We combine weakly nonlinear elasticity and origami to explain the observed ordered progression beyond the realm of perturbative models. Incorporating dominant stretching modes with folding kinematics, we explain the measured torque and energetics originating from geometric nonlinearities due to large displacements. Complementarily, we show that the resulting structures can be algorithmically generated using Schl\\textbackslash{}''afli symbols for star-shaped polygons. A geometric model is then introduced to explain the formation and structure of self-scrolled yarns. Our tensional twist-folding framework shows that origami can be harnessed to understand the transformation of stretchable sheets into self-assembled architectures with a simple twist. }
}

@article{221206673v3,
  title={ How long do we stand our colleagues? A universal behavior in   face-to-face relations },
  author={ Stephane Plaszczynski and Gilberto Nakamura },
  journal={ arXiv preprint arXiv:2212.06673v3 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2212.06673v3 },
  abstract={ We compare face-to-face interaction data recorded by wearable sensors in various sociological environments. The interactions among individuals display a clear environment-dependent diversity in agreement with previous analyses. The contact durations follow heavy-tailed distributions although not exactly of power-law type as previously suggested. Guided by the common patterns observed for each relation, we introduce a variable named the duration contrast, which reveals a common behavior among all datasets. This suggests that our tendency to spend more or less time than usual with a given individual in a face-to-face relation is not governed by social rules but by a common human trait. Additional data shows that it is the same for baboons. Furthermore, we propose a new kind of model to describe the contacts in a given relation based on the recently introduced concept of Levy Geometric Graphs. It reproduces the data at an impressive level. The associated Levy index is found to be alpha = 1.1 on all the datasets, suggesting a universal law for primates and opening many exciting perspectives. }
}

@article{230208505v1,
  title={ Rapid-Motion-Track: Markerless Tracking of Fast Human Motion with Deeper   Learning },
  author={ Renjie Li and Chun Yu Lao and Rebecca St. George and Katherine Lawler and Saurabh Garg and Son N. Tran and Quan Bai and Jane Alty },
  journal={ arXiv preprint arXiv:2302.08505v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2302.08505v1 },
  abstract={ Objective The coordination of human movement directly reflects function of the central nervous system. Small deficits in movement are often the first sign of an underlying neurological problem. The objective of this research is to develop a new end-to-end, deep learning-based system, Rapid-Motion-Track (RMT) that can track the fastest human movement accurately when webcams or laptop cameras are used.   Materials and Methods We applied RMT to finger tapping, a well-validated test of motor control that is one of the most challenging human motions to track with computer vision due to the small keypoints of digits and the high velocities that are generated. We recorded 160 finger tapping assessments simultaneously with a standard 2D laptop camera (30 frames/sec) and a high-speed wearable sensor-based 3D motion tracking system (250 frames/sec). RMT and a range of DLC models were applied to the video data with tapping frequencies up to 8Hz to extract movement features.   Results The movement features (e.g. speed, rhythm, variance) identified with the new RMT system exhibited very high concurrent validity with the gold-standard measurements (97.3\\textbackslash{}\% of RMT measures were within +/-0.5Hz of the Optotrak measures), and outperformed DLC and other advanced computer vision tools (around 88.2\\textbackslash{}\% of DLC measures were within +/-0.5Hz of the Optotrak measures). RMT also accurately tracked a range of other rapid human movements such as foot tapping, head turning and sit-to -stand movements.   Conclusion: With the ubiquity of video technology in smart devices, the RMT method holds potential to transform access and accuracy of human movement assessment. }
}

@article{210315990v7,
  title={ An Overview of Human Activity Recognition Using Wearable Sensors:   Healthcare and Artificial Intelligence },
  author={ Rex Liu and Albara Ah Ramli and Huanle Zhang and Erik Henricson and Xin Liu },
  journal={ arXiv preprint arXiv:2103.15990v7 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2103.15990v7 },
  abstract={ With the rapid development of the internet of things (IoT) and artificial intelligence (AI) technologies, human activity recognition (HAR) has been applied in a variety of domains such as security and surveillance, human-robot interaction, and entertainment. Even though a number of surveys and review papers have been published, there is a lack of HAR overview papers focusing on healthcare applications that use wearable sensors. Therefore, we fill in the gap by presenting this overview paper. In particular, we present our projects to illustrate the system design of HAR applications for healthcare. Our projects include early mobility identification of human activities for intensive care unit (ICU) patients and gait analysis of Duchenne muscular dystrophy (DMD) patients. We cover essential components of designing HAR systems including sensor factors (e.g., type, number, and placement location), AI model selection (e.g., classical machine learning models versus deep learning models), and feature engineering. In addition, we highlight the challenges of such healthcare-oriented HAR systems and propose several research opportunities for both the medical and the computer science community. }
}

@article{200411690v3,
  title={ Q-EEGNet: an Energy-Efficient 8-bit Quantized Parallel EEGNet   Implementation for Edge Motor-Imagery Brain--Machine Interfaces },
  author={ Tibor Schneider and Xiaying Wang and Michael Hersche and Lukas Cavigelli and Luca Benini },
  journal={ arXiv preprint arXiv:2004.11690v3 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2004.11690v3 },
  abstract={ Motor-Imagery Brain--Machine Interfaces (MI-BMIs)promise direct and accessible communication between human brains and machines by analyzing brain activities recorded with Electroencephalography (EEG). Latency, reliability, and privacy constraints make it unsuitable to offload the computation to the cloud. Practical use cases demand a wearable, battery-operated device with low average power consumption for long-term use. Recently, sophisticated algorithms, in particular deep learning models, have emerged for classifying EEG signals. While reaching outstanding accuracy, these models often exceed the limitations of edge devices due to their memory and computational requirements. In this paper, we demonstrate algorithmic and implementation optimizations for EEGNET, a compact Convolutional Neural Network (CNN) suitable for many BMI paradigms. We quantize weights and activations to 8-bit fixed-point with a negligible accuracy loss of 0.4\% on 4-class MI, and present an energy-efficient hardware-aware implementation on the Mr.Wolf parallel ultra-low power (PULP) System-on-Chip (SoC) by utilizing its custom RISC-V ISA extensions and 8-core compute cluster. With our proposed optimization steps, we can obtain an overall speedup of 64x and a reduction of up to 85\% in memory footprint with respect to a single-core layer-wise baseline implementation. Our implementation takes only 5.82 ms and consumes 0.627 mJ per inference. With 21.0GMAC/s/W, it is 256x more energy-efficient than an EEGNET implementation on an ARM Cortex-M7 (0.082GMAC/s/W). }
}

@article{221212660v2,
  title={ Risk assessment and mitigation of e-scooter crashes with naturalistic   driving data },
  author={ Avinash Prabu and Zhengming Zhang and Renran Tian and Stanley Chien and Lingxi Li and Yaobin Chen and Rini Sherony },
  journal={ arXiv preprint arXiv:2212.12660v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2212.12660v2 },
  abstract={ Recently, e-scooter-involved crashes have increased significantly but little information is available about the behaviors of on-road e-scooter riders. Most existing e-scooter crash research was based on retrospectively descriptive media reports, emergency room patient records, and crash reports. This paper presents a naturalistic driving study with a focus on e-scooter and vehicle encounters. The goal is to quantitatively measure the behaviors of e-scooter riders in different encounters to help facilitate crash scenario modeling, baseline behavior modeling, and the potential future development of in-vehicle mitigation algorithms. The data was collected using an instrumented vehicle and an e-scooter rider wearable system, respectively. A three-step data analysis process is developed. First, semi-automatic data labeling extracts e-scooter rider images and non-rider human images in similar environments to train an e-scooter-rider classifier. Then, a multi-step scene reconstruction pipeline generates vehicle and e-scooter trajectories in all encounters. The final step is to model e-scooter rider behaviors and e-scooter-vehicle encounter scenarios. A total of 500 vehicle to e-scooter interactions are analyzed. The variables pertaining to the same are also discussed in this paper. }
}

@article{230105748v1,
  title={ Exploring Automatic Gym Workouts Recognition Locally On Wearable   Resource-Constrained Devices },
  author={ Sizhen Bian and Xiaying Wang and Tommaso Polonelli and Michele Magno },
  journal={ arXiv preprint arXiv:2301.05748v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2301.05748v1 },
  abstract={ Automatic gym activity recognition on energy- and resource-constrained wearable devices removes the human-interaction requirement during intense gym sessions - like soft-touch tapping and swiping. This work presents a tiny and highly accurate residual convolutional neural network that runs in milliwatt microcontrollers for automatic workouts classification. We evaluated the inference performance of the deep model with quantization on three resource-constrained devices: two microcontrollers with ARM-Cortex M4 and M7 core from ST Microelectronics, and a GAP8 system on chip, which is an open-sourced, multi-core RISC-V computing platform from GreenWaves Technologies. Experimental results show an accuracy of up to 90.4\% for eleven workouts recognition with full precision inference. The paper also presents the trade-off performance of the resource-constrained system. While keeping the recognition accuracy (88.1\%) with minimal loss, each inference takes only 3.2 ms on GAP8, benefiting from the 8 RISC-V cluster cores. We measured that it features an execution time that is 18.9x and 6.5x faster than the Cortex-M4 and Cortex-M7 cores, showing the feasibility of real-time on-board workouts recognition based on the described data set with 20 Hz sampling rate. The energy consumed for each inference on GAP8 is 0.41 mJ compared to 5.17 mJ on Cortex-M4 and 8.07 mJ on Cortex-M7 with the maximum clock. It can lead to longer battery life when the system is battery-operated. We also introduced an open data set composed of fifty sessions of eleven gym workouts collected from ten subjects that is publicly available. }
}

@article{230105417v1,
  title={ Analysis of LGM Model for sEMG Signals related to Weight Training },
  author={ Durgesh Kusuru and Anish C. Turlapaty and Mainak Thakur },
  journal={ arXiv preprint arXiv:2301.05417v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2301.05417v1 },
  abstract={ Statistical models of Surface electromyography (sEMG) signals have several applications such as better understanding of sEMG signal generation, improved pattern recognition based control of wearable exoskeletons and prostheses, improving training strategies in sports activities, and EMG simulation studies. Most of the existing studies analysed the statistical model of sEMG signals acquired under isometric contractions. However, there is no study that addresses the statistical model under isotonic contractions. In this work, a new dataset, electromyography analysis of human activities - database 2 (EMAHA-DB2) is developed. It consists of two experiments based on both isometric and isotonic activities during weight training. Previously, a novel Laplacian-Gaussian Mixture (LGM) model was demonstrated for a few benchmark datasets consisting of basic movements and gestures. In this work, the model suitability analysis is extended to the EMAHA-DB2 dataset. Further, the LGM model is compared with three existing statistical models including the recent scale-mixture model. According to qualitative and quantitative analyses, the LGM model has a better fit to the empirical pdf of the recorded sEMG signals compared with the scale mixture model and the other standard models. The variance and mixing weight of the Laplacian component of the signal are analyzed with respect to the type of muscle, type of muscle contraction, dumb-bell weight and training experience of the subjects. The sEMG variance (the Laplacian component) increases with respect to the weights, is greater for isotonic activity especially for the biceps. For isotonic activity, the signal variance increases with training experience. Importantly, the ratio of the variances from the two muscle sites is observed to be nearly independent of the lifted weight and consistently increases with the training experience. }
}

@article{230110168v1,
  title={ CovidRhythm: A Deep Learning Model for Passive Prediction of Covid-19   using Biobehavioral Rhythms Derived from Wearable Physiological Data },
  author={ Atifa Sarwar and Emmanuel O. Agu },
  journal={ arXiv preprint arXiv:2301.10168v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2301.10168v1 },
  abstract={ To investigate whether a deep learning model can detect Covid-19 from disruptions in the human body's physiological (heart rate) and rest-activity rhythms (rhythmic dysregulation) caused by the SARS-CoV-2 virus. We propose CovidRhythm, a novel Gated Recurrent Unit (GRU) Network with Multi-Head Self-Attention (MHSA) that combines sensor and rhythmic features extracted from heart rate and activity (steps) data gathered passively using consumer-grade smart wearable to predict Covid-19. A total of 39 features were extracted (standard deviation, mean, min/max/avg length of sedentary and active bouts) from wearable sensor data. Biobehavioral rhythms were modeled using nine parameters (mesor, amplitude, acrophase, and intra-daily variability). These features were then input to CovidRhythm for predicting Covid-19 in the incubation phase (one day before biological symptoms manifest). A combination of sensor and biobehavioral rhythm features achieved the highest AUC-ROC of 0.79 [Sensitivity = 0.69, Specificity=0.89, F\$\_\{0.1\}\$ = 0.76], outperforming prior approaches in discriminating Covid-positive patients from healthy controls using 24 hours of historical wearable physiological. Rhythmic features were the most predictive of Covid-19 infection when utilized either alone or in conjunction with sensor features. Sensor features predicted healthy subjects best. Circadian rest-activity rhythms that combine 24h activity and sleep information were the most disrupted. CovidRhythm demonstrates that biobehavioral rhythms derived from consumer-grade wearable data can facilitate timely Covid-19 detection. To the best of our knowledge, our work is the first to detect Covid-19 using deep learning and biobehavioral rhythms features derived from consumer-grade wearable data. }
}

@article{221213918v1,
  title={ Simple Yet Surprisingly Effective Training Strategies for LSTMs in   Sensor-Based Human Activity Recognition },
  author={ Shuai Shao and Yu Guan and Xin Guan and Paolo Missier and Thomas Ploetz },
  journal={ arXiv preprint arXiv:2212.13918v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2212.13918v1 },
  abstract={ Human Activity Recognition (HAR) is one of the core research areas in mobile and wearable computing. With the application of deep learning (DL) techniques such as CNN, recognizing periodic or static activities (e.g, walking, lying, cycling, etc.) has become a well studied problem. What remains a major challenge though is the sporadic activity recognition (SAR) problem, where activities of interest tend to be non periodic, and occur less frequently when compared with the often large amount of irrelevant background activities. Recent works suggested that sequential DL models (such as LSTMs) have great potential for modeling nonperiodic behaviours, and in this paper we studied some LSTM training strategies for SAR. Specifically, we proposed two simple yet effective LSTM variants, namely delay model and inverse model, for two SAR scenarios (with and without time critical requirement). For time critical SAR, the delay model can effectively exploit predefined delay intervals (within tolerance) in form of contextual information for improved performance. For regular SAR task, the second proposed, inverse model can learn patterns from the time series in an inverse manner, which can be complementary to the forward model (i.e.,LSTM), and combining both can boost the performance. These two LSTM variants are very practical, and they can be deemed as training strategies without alteration of the LSTM fundamentals. We also studied some additional LSTM training strategies, which can further improve the accuracy. We evaluated our models on two SAR and one non-SAR datasets, and the promising results demonstrated the effectiveness of our approaches in HAR applications. }
}

@article{221209832v1,
  title={ Denoising instrumented mouthguard measurements of head impact kinematics   with a convolutional neural network },
  author={ Xianghao Zhan and Yuzhe Liu and Nicholas J. Cecchi and Ashlyn A. Callan and Enora Le Flao and Olivier Gevaert and Michael M. Zeineh and Gerald A. Grant and David B. Camarillo },
  journal={ arXiv preprint arXiv:2212.09832v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2212.09832v1 },
  abstract={ Wearable sensors for measuring head kinematics can be noisy due to imperfect interfaces with the body. Mouthguards are used to measure head kinematics during impacts in traumatic brain injury (TBI) studies, but deviations from reference kinematics can still occur due to potential looseness. In this study, deep learning is used to compensate for the imperfect interface and improve measurement accuracy. A set of one-dimensional convolutional neural network (1D-CNN) models was developed to denoise mouthguard kinematics measurements along three spatial axes of linear acceleration and angular velocity. The denoised kinematics had significantly reduced errors compared to reference kinematics, and reduced errors in brain injury criteria and tissue strain and strain rate calculated via finite element modeling. The 1D-CNN models were also tested on an on-field dataset of college football impacts and a post-mortem human subject dataset, with similar denoising effects observed. The models can be used to improve detection of head impacts and TBI risk evaluation, and potentially extended to other sensors measuring kinematics. }
}

@article{221206100v1,
  title={ Realistic Modeling of Human Timings for Wearable Cognitive Assistance },
  author={ Manuel O. J. Olguín Muñoz and Vishnu N. Moothedath and Jaya Prakash Champati and Roberta Klatzky and Mahadev Satyanarayanan and James Gross },
  journal={ arXiv preprint arXiv:2212.06100v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2212.06100v1 },
  abstract={ Wearable Cognitive Assistance (WCA) applications present a challenge to benchmark and characterize due to their human-in-the-loop nature. Employing user testing to optimize system parameters is generally not feasible, given the scope of the problem and the number of observations needed to detect small but important effects in controlled experiments. Considering the intended mass-scale deployment of WCA applications in the future, there exists a need for tools enabling human-independent benchmarking.   We present in this paper the first model for the complete end-to-end emulation of humans in WCA. We build this model through statistical analysis of data collected from previous work in this field, and demonstrate its utility by studying application task durations. Compared to first-order approximations, our model shows a \~{}36\% larger gap between step execution times at high system impairment versus low. We further introduce a novel framework for stochastic optimization of resource consumption-responsiveness tradeoffs in WCA, and show that by combining this framework with our realistic model of human behavior, significant reductions of up to 50\% in number processed frame samples and 20\% in energy consumption can be achieved with respect to the state-of-the-art. }
}

@article{220315720v3,
  title={ Transformer Inertial Poser: Real-time Human Motion Reconstruction from   Sparse IMUs with Simultaneous Terrain Generation },
  author={ Yifeng Jiang and Yuting Ye and Deepak Gopinath and Jungdam Won and Alexander W. Winkler and C. Karen Liu },
  journal={ arXiv preprint arXiv:2203.15720v3 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2203.15720v3 },
  abstract={ Real-time human motion reconstruction from a sparse set of (e.g. six) wearable IMUs provides a non-intrusive and economic approach to motion capture. Without the ability to acquire position information directly from IMUs, recent works took data-driven approaches that utilize large human motion datasets to tackle this under-determined problem. Still, challenges remain such as temporal consistency, drifting of global and joint motions, and diverse coverage of motion types on various terrains. We propose a novel method to simultaneously estimate full-body motion and generate plausible visited terrain from only six IMU sensors in real-time. Our method incorporates 1. a conditional Transformer decoder model giving consistent predictions by explicitly reasoning prediction history, 2. a simple yet general learning target named ''stationary body points'' (SBPs) which can be stably predicted by the Transformer model and utilized by analytical routines to correct joint and global drifting, and 3. an algorithm to generate regularized terrain height maps from noisy SBP predictions which can in turn correct noisy global motion estimation. We evaluate our framework extensively on synthesized and real IMU data, and with real-time live demos, and show superior performance over strong baseline methods. }
}

@article{220212938v2,
  title={ Assessing the State of Self-Supervised Human Activity Recognition using   Wearables },
  author={ Harish Haresamudram and Irfan Essa and Thomas Plötz },
  journal={ arXiv preprint arXiv:2202.12938v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2202.12938v2 },
  abstract={ The emergence of self-supervised learning in the field of wearables-based human activity recognition (HAR) has opened up opportunities to tackle the most pressing challenges in the field, namely to exploit unlabeled data to derive reliable recognition systems for scenarios where only small amounts of labeled training samples can be collected. As such, self-supervision, i.e., the paradigm of 'pretrain-then-finetune' has the potential to become a strong alternative to the predominant end-to-end training approaches, let alone hand-crafted features for the classic activity recognition chain. Recently a number of contributions have been made that introduced self-supervised learning into the field of HAR, including, Multi-task self-supervision, Masked Reconstruction, CPC, and SimCLR, to name but a few. With the initial success of these methods, the time has come for a systematic inventory and analysis of the potential self-supervised learning has for the field. This paper provides exactly that. We assess the progress of self-supervised HAR research by introducing a framework that performs a multi-faceted exploration of model performance. We organize the framework into three dimensions, each containing three constituent criteria, such that each dimension captures specific aspects of performance, including the robustness to differing source and target conditions, the influence of dataset characteristics, and the feature space characteristics. We utilize this framework to assess seven state-of-the-art self-supervised methods for HAR, leading to the formulation of insights into the properties of these techniques and to establish their value towards learning representations for diverse scenarios. }
}

@article{210605969v3,
  title={ Dynamics-Regulated Kinematic Policy for Egocentric Pose Estimation },
  author={ Zhengyi Luo and Ryo Hachiuma and Ye Yuan and Kris Kitani },
  journal={ arXiv preprint arXiv:2106.05969v3 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2106.05969v3 },
  abstract={ We propose a method for object-aware 3D egocentric pose estimation that tightly integrates kinematics modeling, dynamics modeling, and scene object information. Unlike prior kinematics or dynamics-based approaches where the two components are used disjointly, we synergize the two approaches via dynamics-regulated training. At each timestep, a kinematic model is used to provide a target pose using video evidence and simulation state. Then, a prelearned dynamics model attempts to mimic the kinematic pose in a physics simulator. By comparing the pose instructed by the kinematic model against the pose generated by the dynamics model, we can use their misalignment to further improve the kinematic model. By factoring in the 6DoF pose of objects (e.g., chairs, boxes) in the scene, we demonstrate for the first time, the ability to estimate physically-plausible 3D human-object interactions using a single wearable camera. We evaluate our egocentric pose estimation method in both controlled laboratory settings and real-world scenarios. }
}

@article{220614976v2,
  title={ Semi-Supervised Generative Adversarial Network for Stress Detection   Using Partially Labeled Physiological Data },
  author={ Nibraas Khan and Nilanjan Sarkar },
  journal={ arXiv preprint arXiv:2206.14976v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2206.14976v2 },
  abstract={ Physiological measurements involves observing variables that attribute to the normative functioning of human systems and subsystems directly or indirectly. The measurements can be used to detect affective states of a person with aims such as improving human-computer interactions. There are several methods of collecting physiological data, but wearable sensors are a common, non-invasive tool for accurate readings. However, valuable information is hard to extract from the raw physiological data, especially for affective state detection. Machine Learning techniques are used to detect the affective state of a person through labeled physiological data. A clear problem with using labeled data is creating accurate labels. An expert is needed to analyze a form of recording of participants and mark sections with different states such as stress and calm. While expensive, this method delivers a complete dataset with labeled data that can be used in any number of supervised algorithms. An interesting question arises from the expensive labeling: how can we reduce the cost while maintaining high accuracy? Semi-Supervised learning (SSL) is a potential solution to this problem. These algorithms allow for machine learning models to be trained with only a small subset of labeled data (unlike unsupervised which use no labels). They provide a way of avoiding expensive labeling. This paper compares a fully supervised algorithm to a SSL on the public WESAD (Wearable Stress and Affect Detection) Dataset for stress detection. This paper shows that Semi-Supervised algorithms are a viable method for inexpensive affective state detection systems with accurate results. }
}

@article{221015119v1,
  title={ Light-weighted CNN-Attention based architecture for Hand Gesture   Recognition via ElectroMyography },
  author={ Soheil Zabihi and Elahe Rahimian and Amir Asif and Arash Mohammadi },
  journal={ arXiv preprint arXiv:2210.15119v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2210.15119v1 },
  abstract={ Advancements in Biological Signal Processing (BSP) and Machine-Learning (ML) models have paved the path for development of novel immersive Human-Machine Interfaces (HMI). In this context, there has been a surge of significant interest in Hand Gesture Recognition (HGR) utilizing Surface-Electromyogram (sEMG) signals. This is due to its unique potential for decoding wearable data to interpret human intent for immersion in Mixed Reality (MR) environments. To achieve the highest possible accuracy, complicated and heavy-weighted Deep Neural Networks (DNNs) are typically developed, which restricts their practical application in low-power and resource-constrained wearable systems. In this work, we propose a light-weighted hybrid architecture (HDCAM) based on Convolutional Neural Network (CNN) and attention mechanism to effectively extract local and global representations of the input. The proposed HDCAM model with 58,441 parameters reached a new state-of-the-art (SOTA) performance with 82.91\% and 81.28\% accuracy on window sizes of 300 ms and 200 ms for classifying 17 hand gestures. The number of parameters to train the proposed HDCAM architecture is 18.87 times less than its previous SOTA counterpart. }
}

@article{220500287v2,
  title={ Assessing Fatigue with Multimodal Wearable Sensors and Machine Learning },
  author={ Ashish Jaiswal and Mohammad Zaki Zadeh and Aref Hebri and Fillia Makedon },
  journal={ arXiv preprint arXiv:2205.00287v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2205.00287v2 },
  abstract={ Fatigue is a loss in cognitive or physical performance due to physiological factors such as insufficient sleep, long work hours, stress, and physical exertion. It adversely affects the human body and can slow reaction times, reduce attention, and limit short-term memory. Hence, there is a need to monitor a person's state to avoid extreme fatigue conditions that can result in physiological complications. However, tools to understand and assess fatigue are minimal. This paper primarily focuses on building an experimental setup that induces cognitive fatigue (CF) and physical fatigue (PF) through multiple cognitive and physical tasks while simultaneously recording physiological data. First, we built a prototype sensor suit embedded with numerous physiological sensors for easy use during data collection. Second, participants' self-reported visual analog scores (VAS) are reported after each task to confirm fatigue induction. Finally, an evaluation system is built that utilizes machine learning (ML) models to detect states of CF and PF from sensor data, thus providing an objective measure. Our methods beat state-of-the-art approaches, where Random Forest performs the best in detecting PF with an accuracy of 80.5\% while correctly predicting the true PF condition 88\% of the time. On the other hand, the long short-term memory (LSTM) recurrent neural network produces the best results in detecting CF in the subjects (with 84.1\% accuracy, 0.9 recall). }
}

@article{221001839v1,
  title={ Reactive fungal insoles },
  author={ Anna Nikolaidou and Neil Phillips and Michail-Antisthenis Tsompanas and Andrew Adamatzky },
  journal={ arXiv preprint arXiv:2210.01839v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2210.01839v1 },
  abstract={ Mycelium bound composites are promising materials for a diverse range of applications including wearables and building elements. Their functionality surpasses some of the capabilities of traditionally passive materials, such as synthetic fibres, reconstituted cellulose fibres and natural fibres. Thereby, creating novel propositions including augmented functionality (sensory) and aesthetic (personal fashion). Biomaterials can offer multiple modal sensing capability such as mechanical loading (compressive and tensile) and moisture content. To assess the sensing potential of fungal insoles we undertook laboratory experiments on electrical response of bespoke insoles made from capillary matting colonised with oyster fungi Pleurotus ostreatus to compressive stress which mimics human loading when standing and walking. We have shown changes in electrical activity with compressive loading. The results advance the development of intelligent sensing insoles which are a building block towards more generic reactive fungal wearables. Using FitzhHugh-Nagumo model we numerically illustrated how excitation wave-fronts behave in a mycelium network colonising an insole and shown that it may be possible to discern pressure points from the mycelium electrical activity. }
}

@article{221000808v1,
  title={ A Multi Camera Unsupervised Domain Adaptation Pipeline for Object   Detection in Cultural Sites through Adversarial Learning and Self-Training },
  author={ Giovanni Pasqualino and Antonino Furnari and Giovanni Maria Farinella },
  journal={ arXiv preprint arXiv:2210.00808v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2210.00808v1 },
  abstract={ Object detection algorithms allow to enable many interesting applications which can be implemented in different devices, such as smartphones and wearable devices. In the context of a cultural site, implementing these algorithms in a wearable device, such as a pair of smart glasses, allow to enable the use of augmented reality (AR) to show extra information about the artworks and enrich the visitors' experience during their tour. However, object detection algorithms require to be trained on many well annotated examples to achieve reasonable results. This brings a major limitation since the annotation process requires human supervision which makes it expensive in terms of time and costs. A possible solution to reduce these costs consist in exploiting tools to automatically generate synthetic labeled images from a 3D model of the site. However, models trained with synthetic data do not generalize on real images acquired in the target scenario in which they are supposed to be used. Furthermore, object detectors should be able to work with different wearable devices or different mobile devices, which makes generalization even harder. In this paper, we present a new dataset collected in a cultural site to study the problem of domain adaptation for object detection in the presence of multiple unlabeled target domains corresponding to different cameras and a labeled source domain obtained considering synthetic images for training purposes. We present a new domain adaptation method which outperforms current state-of-the-art approaches combining the benefits of aligning the domains at the feature and pixel level with a self-training process. We release the dataset at the following link https://iplab.dmi.unict.it/OBJ-MDA/ and the code of the proposed architecture at https://github.com/fpv-iplab/STMDA-RetinaNet. }
}

@article{221000507v1,
  title={ Fast and Robust Video-Based Exercise Classification via Body Pose   Tracking and Scalable Multivariate Time Series Classifiers },
  author={ Ashish Singh and Antonio Bevilacqua and Thach Le Nguyen and Feiyan Hu and Kevin McGuinness and Martin OReilly and Darragh Whelan and Brian Caulfield and Georgiana Ifrim },
  journal={ arXiv preprint arXiv:2210.00507v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2210.00507v1 },
  abstract={ Technological advancements have spurred the usage of machine learning based applications in sports science. Physiotherapists, sports coaches and athletes actively look to incorporate the latest technologies in order to further improve performance and avoid injuries. While wearable sensors are very popular, their use is hindered by constraints on battery power and sensor calibration, especially for use cases which require multiple sensors to be placed on the body. Hence, there is renewed interest in video-based data capture and analysis for sports science. In this paper, we present the application of classifying S\\textbackslash{}\&C exercises using video. We focus on the popular Military Press exercise, where the execution is captured with a video-camera using a mobile device, such as a mobile phone, and the goal is to classify the execution into different types. Since video recordings need a lot of storage and computation, this use case requires data reduction, while preserving the classification accuracy and enabling fast prediction. To this end, we propose an approach named BodyMTS to turn video into time series by employing body pose tracking, followed by training and prediction using multivariate time series classifiers. We analyze the accuracy and robustness of BodyMTS and show that it is robust to different types of noise caused by either video quality or pose estimation factors. We compare BodyMTS to state-of-the-art deep learning methods which classify human activity directly from videos and show that BodyMTS achieves similar accuracy, but with reduced running time and model engineering effort. Finally, we discuss some of the practical aspects of employing BodyMTS in this application in terms of accuracy and robustness under reduced data quality and size. We show that BodyMTS achieves an average accuracy of 87\\textbackslash{}\%, which is significantly higher than the accuracy of human domain experts. }
}

@article{220911579v1,
  title={ A Probabilistic Model of Activity Recognition with Loose Clothing },
  author={ Tianchen Shen and Irene Di Giulio and Matthew Howard },
  journal={ arXiv preprint arXiv:2209.11579v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2209.11579v1 },
  abstract={ Human activity recognition has become an attractive research area with the development of on-body wearable sensing technology. With comfortable electronic-textiles, sensors can be embedded into clothing so that it is possible to record human movement outside the laboratory for long periods. However, a long-standing issue is how to deal with motion artefacts introduced by movement of clothing with respect to the body. Surprisingly, recent empirical findings suggest that cloth-attached sensor can actually achieve higher accuracy of activity recognition than rigid-attached sensor, particularly when predicting from short time-windows. In this work, a probabilistic model is introduced in which this improved accuracy and resposiveness is explained by the increased statistical distance between movements recorded via fabric sensing. The predictions of the model are verified in simulated and real human motion capture experiments, where it is evident that this counterintuitive effect is closely captured. }
}

@article{220208922v2,
  title={ FLAME: Federated Learning Across Multi-device Environments },
  author={ Hyunsung Cho and Akhil Mathur and Fahim Kawsar },
  journal={ arXiv preprint arXiv:2202.08922v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2202.08922v2 },
  abstract={ Federated Learning (FL) enables distributed training of machine learning models while keeping personal data on user devices private. While we witness increasing applications of FL in the area of mobile sensing, such as human activity recognition (HAR), FL has not been studied in the context of a multi-device environment (MDE), wherein each user owns multiple data-producing devices. With the proliferation of mobile and wearable devices, MDEs are increasingly becoming popular in ubicomp settings, therefore necessitating the study of FL in them. FL in MDEs is characterized by being not independent and identically distributed (non-IID) across clients, complicated by the presence of both user and device heterogeneities. Further, ensuring efficient utilization of system resources on FL clients in a MDE remains an important challenge. In this paper, we propose FLAME, a user-centered FL training approach to counter statistical and system heterogeneity in MDEs, and bring consistency in inference performance across devices. FLAME features (i) user-centered FL training utilizing the time alignment across devices from the same user; (ii) accuracy- and efficiency-aware device selection; and (iii) model personalization to devices. We also present an FL evaluation testbed with realistic energy drain and network bandwidth profiles, and a novel class-based data partitioning scheme to extend existing HAR datasets to a federated setup. Our experiment results on three multi-device HAR datasets show that FLAME outperforms various baselines by 4.3-25.8\% higher F1 score, 1.02-2.86x greater energy efficiency, and up to 2.06x speedup in convergence to target accuracy through fair distribution of the FL workload. }
}

@article{220908335v1,
  title={ Efficient Deep Clustering of Human Activities and How to Improve   Evaluation },
  author={ Louis Mahon and Thomas Lukasiewicz },
  journal={ arXiv preprint arXiv:2209.08335v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2209.08335v1 },
  abstract={ There has been much recent research on human activity re\\textbackslash{}-cog\\textbackslash{}-ni\\textbackslash{}-tion (HAR), due to the proliferation of wearable sensors in watches and phones, and the advances of deep learning methods, which avoid the need to manually extract features from raw sensor signals. A significant disadvantage of deep learning applied to HAR is the need for manually labelled training data, which is especially difficult to obtain for HAR datasets. Progress is starting to be made in the unsupervised setting, in the form of deep HAR clustering models, which can assign labels to data without having been given any labels to train on, but there are problems with evaluating deep HAR clustering models, which makes assessing the field and devising new methods difficult. In this paper, we highlight several distinct problems with how deep HAR clustering models are evaluated, describing these problems in detail and conducting careful experiments to explicate the effect that they can have on results. We then discuss solutions to these problems, and suggest standard evaluation settings for future deep HAR clustering models. Additionally, we present a new deep clustering model for HAR. When tested under our proposed settings, our model performs better than (or on par with) existing models, while also being more efficient and better able to scale to more complex datasets by avoiding the need for an autoencoder. }
}

@article{220907898v1,
  title={ SpikeSEE: An Energy-Efficient Dynamic Scenes Processing Framework for   Retinal Prostheses },
  author={ Chuanqing Wang and Chaoming Fang and Yong Zou and Jie Yang and Mohamad Sawan },
  journal={ arXiv preprint arXiv:2209.07898v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2209.07898v1 },
  abstract={ Intelligent and low-power retinal prostheses are highly demanded in this era, where wearable and implantable devices are used for numerous healthcare applications. In this paper, we propose an energy-efficient dynamic scenes processing framework (SpikeSEE) that combines a spike representation encoding technique and a bio-inspired spiking recurrent neural network (SRNN) model to achieve intelligent processing and extreme low-power computation for retinal prostheses. The spike representation encoding technique could interpret dynamic scenes with sparse spike trains, decreasing the data volume. The SRNN model, inspired by the human retina special structure and spike processing method, is adopted to predict the response of ganglion cells to dynamic scenes. Experimental results show that the Pearson correlation coefficient of the proposed SRNN model achieves 0.93, which outperforms the state of the art processing framework for retinal prostheses. Thanks to the spike representation and SRNN processing, the model can extract visual features in a multiplication-free fashion. The framework achieves 12 times power reduction compared with the convolutional recurrent neural network (CRNN) processing-based framework. Our proposed SpikeSEE predicts the response of ganglion cells more accurately with lower energy consumption, which alleviates the precision and power issues of retinal prostheses and provides a potential solution for wearable or implantable prostheses. }
}

@article{220905077v1,
  title={ BON: An extended public domain dataset for human activity recognition },
  author={ Girmaw Abebe Tadesse and Oliver Bent and Komminist Weldemariam and Md. Abrar Istiak and Taufiq Hasan and Andrea Cavallaro },
  journal={ arXiv preprint arXiv:2209.05077v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2209.05077v1 },
  abstract={ Body-worn first-person vision (FPV) camera enables to extract a rich source of information on the environment from the subject's viewpoint. However, the research progress in wearable camera-based egocentric office activity understanding is slow compared to other activity environments (e.g., kitchen and outdoor ambulatory), mainly due to the lack of adequate datasets to train more sophisticated (e.g., deep learning) models for human activity recognition in office environments. This paper provides details of a large and publicly available office activity dataset (BON) collected in different office settings across three geographical locations: Barcelona (Spain), Oxford (UK) and Nairobi (Kenya), using a chest-mounted GoPro Hero camera. The BON dataset contains eighteen common office activities that can be categorised into person-to-person interactions (e.g., Chat with colleagues), person-to-object (e.g., Writing on a whiteboard), and proprioceptive (e.g., Walking). Annotation is provided for each segment of video with 5-seconds duration. Generally, BON contains 25 subjects and 2639 total segments. In order to facilitate further research in the sub-domain, we have also provided results that could be used as baselines for future studies. }
}

@article{220501904v2,
  title={ ImAiR: Airwriting Recognition framework using Image Representation of   IMU Signals },
  author={ Ayush Tripathi and Arnab Kumar Mondal and Lalan Kumar and Prathosh A. P },
  journal={ arXiv preprint arXiv:2205.01904v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2205.01904v2 },
  abstract={ The problem of Airwriting Recognition is focused on identifying letters written by movement of finger in free space. It is a type of gesture recognition where the dictionary corresponds to letters in a specific language. In particular, airwriting recognition using sensor data from wrist-worn devices can be used as a medium of user input for applications in Human-Computer Interaction (HCI). Recognition of in-air trajectories using such wrist-worn devices is limited in literature and forms the basis of the current work. In this paper, we propose an airwriting recognition framework by first encoding the time-series data obtained from a wearable Inertial Measurement Unit (IMU) on the wrist as images and then utilizing deep learning-based models for identifying the written alphabets. The signals recorded from 3-axis accelerometer and gyroscope in IMU are encoded as images using different techniques such as Self Similarity Matrix (SSM), Gramian Angular Field (GAF) and Markov Transition Field (MTF) to form two sets of 3-channel images. These are then fed to two separate classification models and letter prediction is made based on an average of the class conditional probabilities obtained from the two models. Several standard model architectures for image classification such as variants of ResNet, DenseNet, VGGNet, AlexNet and GoogleNet have been utilized. Experiments performed on two publicly available datasets demonstrate the efficacy of the proposed strategy. The code for our implementation will be made available at https://github.com/ayushayt/ImAiR. }
}

@article{220808433v1,
  title={ Label Flipping Data Poisoning Attack Against Wearable Human Activity   Recognition System },
  author={ Abdur R. Shahid and Ahmed Imteaj and Peter Y. Wu and Diane A. Igoche and Tauhidul Alam },
  journal={ arXiv preprint arXiv:2208.08433v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2208.08433v1 },
  abstract={ Human Activity Recognition (HAR) is a problem of interpreting sensor data to human movement using an efficient machine learning (ML) approach. The HAR systems rely on data from untrusted users, making them susceptible to data poisoning attacks. In a poisoning attack, attackers manipulate the sensor readings to contaminate the training set, misleading the HAR to produce erroneous outcomes. This paper presents the design of a label flipping data poisoning attack for a HAR system, where the label of a sensor reading is maliciously changed in the data collection phase. Due to high noise and uncertainty in the sensing environment, such an attack poses a severe threat to the recognition system. Besides, vulnerability to label flipping attacks is dangerous when activity recognition models are deployed in safety-critical applications. This paper shades light on how to carry out the attack in practice through smartphone-based sensor data collection applications. This is an earlier research work, to our knowledge, that explores attacking the HAR models via label flipping poisoning. We implement the proposed attack and test it on activity recognition models based on the following machine learning algorithms: multi-layer perceptron, decision tree, random forest, and XGBoost. Finally, we evaluate the effectiveness of K-nearest neighbors (KNN)-based defense mechanism against the proposed attack. }
}

@article{220808090v1,
  title={ Progressive Cross-modal Knowledge Distillation for Human Action   Recognition },
  author={ Jianyuan Ni and Anne H. H. Ngu and Yan Yan },
  journal={ arXiv preprint arXiv:2208.08090v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2208.08090v1 },
  abstract={ Wearable sensor-based Human Action Recognition (HAR) has achieved remarkable success recently. However, the accuracy performance of wearable sensor-based HAR is still far behind the ones from the visual modalities-based system (i.e., RGB video, skeleton, and depth). Diverse input modalities can provide complementary cues and thus improve the accuracy performance of HAR, but how to take advantage of multi-modal data on wearable sensor-based HAR has rarely been explored. Currently, wearable devices, i.e., smartwatches, can only capture limited kinds of non-visual modality data. This hinders the multi-modal HAR association as it is unable to simultaneously use both visual and non-visual modality data. Another major challenge lies in how to efficiently utilize multimodal data on wearable devices with their limited computation resources. In this work, we propose a novel Progressive Skeleton-to-sensor Knowledge Distillation (PSKD) model which utilizes only time-series data, i.e., accelerometer data, from a smartwatch for solving the wearable sensor-based HAR problem. Specifically, we construct multiple teacher models using data from both teacher (human skeleton sequence) and student (time-series accelerometer data) modalities. In addition, we propose an effective progressive learning scheme to eliminate the performance gap between teacher and student models. We also designed a novel loss function called Adaptive-Confidence Semantic (ACS), to allow the student model to adaptively select either one of the teacher models or the ground-truth label it needs to mimic. To demonstrate the effectiveness of our proposed PSKD method, we conduct extensive experiments on Berkeley-MHAD, UTD-MHAD, and MMAct datasets. The results confirm that the proposed PSKD method has competitive performance compared to the previous mono sensor-based HAR methods. }
}

@article{220807547v1,
  title={ Multi-level Contrast Network for Wearables-based Joint Activity   Segmentation and Recognition },
  author={ Songpengcheng Xia and Lei Chu and Ling Pei and Wenxian Yu and Robert C. Qiu },
  journal={ arXiv preprint arXiv:2208.07547v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2208.07547v1 },
  abstract={ Human activity recognition (HAR) with wearables is promising research that can be widely adopted in many smart healthcare applications. In recent years, the deep learning-based HAR models have achieved impressive recognition performance. However, most HAR algorithms are susceptible to the multi-class windows problem that is essential yet rarely exploited. In this paper, we propose to relieve this challenging problem by introducing the segmentation technology into HAR, yielding joint activity segmentation and recognition. Especially, we introduce the Multi-Stage Temporal Convolutional Network (MS-TCN) architecture for sample-level activity prediction to joint segment and recognize the activity sequence. Furthermore, to enhance the robustness of HAR against the inter-class similarity and intra-class heterogeneity, a multi-level contrastive loss, containing the sample-level and segment-level contrast, has been proposed to learn a well-structured embedding space for better activity segmentation and recognition performance. Finally, with comprehensive experiments, we verify the effectiveness of the proposed method on two public HAR datasets, achieving significant improvements in the various evaluation metrics. }
}

@article{211206651v2,
  title={ Accoustate: Auto-annotation of IMU-generated Activity Signatures under   Smart Infrastructure },
  author={ Soumyajit Chatterjee and Arun Singh and Bivas Mitra and Sandip Chakraborty },
  journal={ arXiv preprint arXiv:2112.06651v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2112.06651v2 },
  abstract={ Human activities within smart infrastructures generate a vast amount of IMU data from the wearables worn by individuals. Many existing studies rely on such sensory data for human activity recognition (HAR); however, one of the major bottlenecks is their reliance on pre-annotated or labeled data. Manual human-driven annotations are neither scalable nor efficient, whereas existing auto-annotation techniques heavily depend on video signatures. Still, video-based auto-annotation needs high computation resources and has privacy concerns when the data from a personal space, like a smart-home, is transferred to the cloud. This paper exploits the acoustic signatures generated from human activities to label the wearables' IMU data at the edge, thus mitigating resource requirement and data privacy concerns. We utilize acoustic-based pre-trained HAR models for cross-modal labeling of the IMU data even when two individuals perform simultaneous but different activities under the same environmental context. We observe that non-overlapping acoustic gaps exist with a high probability during the simultaneous activities performed by two individuals in the environment's acoustic context, which helps us resolve the overlapping activity signatures to label them individually. A principled evaluation of the proposed approach on two real-life in-house datasets further augmented to create a dual occupant setup, shows that the framework can correctly annotate a significant volume of unlabeled IMU data from both individuals with an accuracy of \$\\textbackslash{}mathbf\{82.59\\textbackslash{}\%\}\$ (\$\\textbackslash{}mathbf\{\\textbackslash{}pm 17.94\\textbackslash{}\%\}\$) and \$\\textbackslash{}mathbf\{98.32\\textbackslash{}\%\}\$ (\$\\textbackslash{}mathbf\{\\textbackslash{}pm 3.68\\textbackslash{}\%\}\$), respectively, for a workshop and a kitchen environment. }
}

@article{220710915v1,
  title={ Optimization of Forcemyography Sensor Placement for Arm Movement   Recognition },
  author={ Xiaohao Xu and Zihao Du and Huaxin Zhang and Ruichao Zhang and Zihan Hong and Qin Huang and Bin Han },
  journal={ arXiv preprint arXiv:2207.10915v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2207.10915v1 },
  abstract={ How to design an optimal wearable device for human movement recognition is vital to reliable and accurate human-machine collaboration. Previous works mainly fabricate wearable devices heuristically. Instead, this paper raises an academic question: can we design an optimization algorithm to optimize the fabrication of wearable devices such as figuring out the best sensor arrangement automatically? Specifically, this work focuses on optimizing the placement of Forcemyography (FMG) sensors for FMG armbands in the application of arm movement recognition. Firstly, based on graph theory, the armband is modeled considering sensors' signals and connectivity. Then, a Graph-based Armband Modeling Network (GAM-Net) is introduced for arm movement recognition. Afterward, the sensor placement optimization for FMG armbands is formulated and an optimization algorithm with greedy local search is proposed. To study the effectiveness of our optimization algorithm, a dataset for mechanical maintenance tasks using FMG armbands with 16 sensors is collected. Our experiments show that using only 4 sensors optimized with our algorithm can help maintain a comparable recognition accuracy to using all sensors. Finally, the optimized sensor placement result is verified from a physiological view. This work would like to shed light on the automatic fabrication of wearable devices considering downstream tasks, such as human biological signal collection and movement recognition. Our code and dataset are available at https://github.com/JerryX1110/IROS22-FMG-Sensor-Optimization }
}

@article{220707806v1,
  title={ CHARM: A Hierarchical Deep Learning Model for Classification of Complex   Human Activities Using Motion Sensors },
  author={ Eric Rosen and Doruk Senkal },
  journal={ arXiv preprint arXiv:2207.07806v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2207.07806v1 },
  abstract={ In this paper, we report a hierarchical deep learning model for classification of complex human activities using motion sensors. In contrast to traditional Human Activity Recognition (HAR) models used for event-based activity recognition, such as step counting, fall detection, and gesture identification, this new deep learning model, which we refer to as CHARM (Complex Human Activity Recognition Model), is aimed for recognition of high-level human activities that are composed of multiple different low-level activities in a non-deterministic sequence, such as meal preparation, house chores, and daily routines. CHARM not only quantitatively outperforms state-of-the-art supervised learning approaches for high-level activity recognition in terms of average accuracy and F1 scores, but also automatically learns to recognize low-level activities, such as manipulation gestures and locomotion modes, without any explicit labels for such activities. This opens new avenues for Human-Machine Interaction (HMI) modalities using wearable sensors, where the user can choose to associate an automated task with a high-level activity, such as controlling home automation (e.g., robotic vacuum cleaners, lights, and thermostats) or presenting contextually relevant information at the right time (e.g., reminders, status updates, and weather/news reports). In addition, the ability to learn low-level user activities when trained using only high-level activity labels may pave the way to semi-supervised learning of HAR tasks that are inherently difficult to label. }
}

@article{211203030v2,
  title={ Pose2Room: Understanding 3D Scenes from Human Activities },
  author={ Yinyu Nie and Angela Dai and Xiaoguang Han and Matthias Nießner },
  journal={ arXiv preprint arXiv:2112.03030v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2112.03030v2 },
  abstract={ With wearable IMU sensors, one can estimate human poses from wearable devices without requiring visual input\~{}\\textbackslash{}cite\{von2017sparse\}. In this work, we pose the question: Can we reason about object structure in real-world environments solely from human trajectory information? Crucially, we observe that human motion and interactions tend to give strong information about the objects in a scene -- for instance a person sitting indicates the likely presence of a chair or sofa. To this end, we propose P2R-Net to learn a probabilistic 3D model of the objects in a scene characterized by their class categories and oriented 3D bounding boxes, based on an input observed human trajectory in the environment. P2R-Net models the probability distribution of object class as well as a deep Gaussian mixture model for object boxes, enabling sampling of multiple, diverse, likely modes of object configurations from an observed human trajectory. In our experiments we show that P2R-Net can effectively learn multi-modal distributions of likely objects for human motions, and produce a variety of plausible object structures of the environment, even without any visual information. The results demonstrate that P2R-Net consistently outperforms the baselines on the PROX dataset and the VirtualHome platform. }
}

@article{211100993v3,
  title={ Egocentric Human Trajectory Forecasting with a Wearable Camera and   Multi-Modal Fusion },
  author={ Jianing Qiu and Lipeng Chen and Xiao Gu and Frank P. -W. Lo and Ya-Yen Tsai and Jiankai Sun and Jiaqi Liu and Benny Lo },
  journal={ arXiv preprint arXiv:2111.00993v3 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2111.00993v3 },
  abstract={ In this paper, we address the problem of forecasting the trajectory of an egocentric camera wearer (ego-person) in crowded spaces. The trajectory forecasting ability learned from the data of different camera wearers walking around in the real world can be transferred to assist visually impaired people in navigation, as well as to instill human navigation behaviours in mobile robots, enabling better human-robot interactions. To this end, a novel egocentric human trajectory forecasting dataset was constructed, containing real trajectories of people navigating in crowded spaces wearing a camera, as well as extracted rich contextual data. We extract and utilize three different modalities to forecast the trajectory of the camera wearer, i.e., his/her past trajectory, the past trajectories of nearby people, and the environment such as the scene semantics or the depth of the scene. A Transformer-based encoder-decoder neural network model, integrated with a novel cascaded cross-attention mechanism that fuses multiple modalities, has been designed to predict the future trajectory of the camera wearer. Extensive experiments have been conducted, with results showing that our model outperforms the state-of-the-art methods in egocentric human trajectory forecasting. }
}

@article{220613056v1,
  title={ A Coupled Neural Circuit Design for Guillain-Barre Syndrome },
  author={ Oguzhan Derebasi and Murat Isik and Oguzhan Demirag and Dilek Goksel Duru and Anup Das },
  journal={ arXiv preprint arXiv:2206.13056v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2206.13056v1 },
  abstract={ Guillain-Barre syndrome is a rare neurological condition in which the human immune system attacks the peripheral nervous system. A peripheral nervous system appears as a diffusively connected system of mathematical models of neuron models, and the system's period becomes shorter than the periods of each neural circuit. The stimuli in the conduction path that will address the myelin sheath that has lost its function are received by the axons and are conveyed externally to the target organ, aiming to solve the problem of decreased nerve conduction. In the NEURON simulation environment, one can create a neuron model and define biophysical events that take place within the system for study. In this environment, signal transmission between cells and dendrites is obtained graphically. The simulated potassium and sodium conductance are replicated adequately, and the electronic action potentials are quite comparable to those measured experimentally. In this work, we propose an analog and digital coupled neuron model comprising individual excitatory and inhibitory neural circuit blocks for a low-cost and energy-efficient system. Compared to digital design, our analog design performs in lower frequency but gives a 32.3\\textbackslash{}\% decreased energy efficiency. Thus, the resulting coupled analog hardware neuron model can be a proposed model for the simulation of reduced nerve conduction. As a result, the analog coupled neuron, (even with its greater design complexity) serious contender for the future development of a wearable sensor device that could help with Guillain-Barre syndrome and other neurologic diseases. }
}

@article{220606629v1,
  title={ Semantic-Discriminative Mixup for Generalizable Sensor-based   Cross-domain Activity Recognition },
  author={ Wang Lu and Jindong Wang and Yiqiang Chen and Sinno Jialin Pan and Chunyu Hu and Xin Qin },
  journal={ arXiv preprint arXiv:2206.06629v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2206.06629v1 },
  abstract={ It is expensive and time-consuming to collect sufficient labeled data to build human activity recognition (HAR) models. Training on existing data often makes the model biased towards the distribution of the training data, thus the model might perform terribly on test data with different distributions. Although existing efforts on transfer learning and domain adaptation try to solve the above problem, they still need access to unlabeled data on the target domain, which may not be possible in real scenarios. Few works pay attention to training a model that can generalize well to unseen target domains for HAR. In this paper, we propose a novel method called Semantic-Discriminative Mixup (SDMix) for generalizable cross-domain HAR. Firstly, we introduce semantic-aware Mixup that considers the activity semantic ranges to overcome the semantic inconsistency brought by domain differences. Secondly, we introduce the large margin loss to enhance the discrimination of Mixup to prevent misclassification brought by noisy virtual labels. Comprehensive generalization experiments on five public datasets demonstrate that our SDMix substantially outperforms the state-of-the-art approaches with 6\% average accuracy improvement on cross-person, cross-dataset, and cross-position HAR. }
}

@article{220604855v1,
  title={ Beyond the Gates of Euclidean Space: Temporal-Discrimination-Fusions and   Attention-based Graph Neural Network for Human Activity Recognition },
  author={ Nafees Ahmad and Savio Ho-Chit Chow and Ho-fung Leung },
  journal={ arXiv preprint arXiv:2206.04855v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2206.04855v1 },
  abstract={ Human activity recognition (HAR) through wearable devices has received much interest due to its numerous applications in fitness tracking, wellness screening, and supported living. As a result, we have seen a great deal of work in this field. Traditional deep learning (DL) has set a state of the art performance for HAR domain. However, it ignores the data's structure and the association between consecutive time stamps. To address this constraint, we offer an approach based on Graph Neural Networks (GNNs) for structuring the input representation and exploiting the relations among the samples. However, even when using a simple graph convolution network to eliminate this shortage, there are still several limiting factors, such as inter-class activities issues, skewed class distribution, and a lack of consideration for sensor data priority, all of which harm the HAR model's performance. To improve the current HAR model's performance, we investigate novel possibilities within the framework of graph structure to achieve highly discriminated and rich activity features. We propose a model for (1) time-series-graph module that converts raw data from HAR dataset into graphs; (2) Graph Convolutional Neural Networks (GCNs) to discover local dependencies and correlations between neighboring nodes; and (3) self-attention GNN encoder to identify sensors interactions and data priorities. To the best of our knowledge, this is the first work for HAR, which introduces a GNN-based approach that incorporates both the GCN and the attention mechanism. By employing a uniform evaluation method, our framework significantly improves the performance on hospital patient's activities dataset comparatively considered other state of the art baseline methods. }
}

@article{210800980v2,
  title={ Neuromechanical model-based adaptive control of bi-lateral ankle   exoskeletons: biological joint torque and electromyogram reduction across   walking conditions },
  author={ Guillaume Durandau and Wolfgang Rampeltshammer and Herman van der Kooij and Massimo Sartori },
  journal={ arXiv preprint arXiv:2108.00980v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2108.00980v2 },
  abstract={ To enable the broad adoption of wearable robotic exoskeletons in medical and industrial settings, it is crucial they can adaptively support large repertoires of movements. We propose a new human-machine interface to simultaneously drive bilateral ankle exoskeletons during a range of 'unseen' walking conditions and transitions that were not used for establishing the control interface. The proposed approach used person-specific neuromechanical models to estimate biological ankle joint torques in real-time from measured electromyograms (EMGS) and joint angles. A low-level controller based on a disturbance observer translated biological torque estimates into exoskeleton commands. We call this 'neuromechanical model-based control' (NMBC). NMBC enabled six individuals to voluntarily control a bilateral ankle exoskeleton across six walking conditions, including all intermediate transitions, i.e., two walking speeds, each performed at three ground elevations, with no need for predefined torque profiles, nor a priori chosen neuromuscular reflex rules, or state machines as common in literature. A single subject case-study was carried out on a dexterous locomotion tasks involving moonwalking. NMBC always enabled reducing biological ankle torques, as well as eight ankle muscle EMGs both within (22\% torque; 12\% EMG) and between walking conditions (24\% torque; 14\% EMG) when compared to non-assisted conditions. Torque and EMG reductions in novel walking conditions indicated that the exoskeleton operated symbiotically, as exomuscles controlled by the operator's neuromuscular system. This opens new avenues for the systematic adoption of wearable robots as part of out-of-the-lab medical and occupational settings. }
}

@article{220604480v1,
  title={ Comparison Study of Inertial Sensor Signal Combination for Human   Activity Recognition based on Convolutional Neural Networks },
  author={ Farhad Nazari and Navid Mohajer and Darius Nahavandi and Abbas Khosravi and Saeid Nahavandi },
  journal={ arXiv preprint arXiv:2206.04480v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2206.04480v1 },
  abstract={ Human Activity Recognition (HAR) is one of the essential building blocks of so many applications like security, monitoring, the internet of things and human-robot interaction. The research community has developed various methodologies to detect human activity based on various input types. However, most of the research in the field has been focused on applications other than human-in-the-centre applications. This paper focused on optimising the input signals to maximise the HAR performance from wearable sensors. A model based on Convolutional Neural Networks (CNN) has been proposed and trained on different signal combinations of three Inertial Measurement Units (IMU) that exhibit the movements of the dominant hand, leg and chest of the subject. The results demonstrate k-fold cross-validation accuracy between 99.77 and 99.98\% for signals with the modality of 12 or higher. The performance of lower dimension signals, except signals containing information from both chest and ankle, was far inferior, showing between 73 and 85\% accuracy. }
}

@article{220507835v1,
  title={ Whole-Body Human Kinematics Estimation using Dynamical Inverse   Kinematics and Contact-Aided Lie Group Kalman Filter },
  author={ Prashanth Ramadoss and Lorenzo Rapetti and Yeshasvi Tirupachuri and Riccardo Grieco and Gianluca Milani and Enrico Valli and Stefano Dafarra and Silvio Traversaro and Daniele Pucci },
  journal={ arXiv preprint arXiv:2205.07835v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2205.07835v1 },
  abstract={ Full-body motion estimation of a human through wearable sensing technologies is challenging in the absence of position sensors. This paper contributes to the development of a model-based whole-body kinematics estimation algorithm using wearable distributed inertial and force-torque sensing. This is done by extending the existing dynamical optimization-based Inverse Kinematics (IK) approach for joint state estimation, in cascade, to include a center of pressure-based contact detector and a contact-aided Kalman filter on Lie groups for floating base pose estimation. The proposed method is tested in an experimental scenario where a human equipped with a sensorized suit and shoes performs walking motions. The proposed method is demonstrated to obtain a reliable reconstruction of the whole-body human motion. }
}

@article{220501652v1,
  title={ Episodic Memory Question Answering },
  author={ Samyak Datta and Sameer Dharur and Vincent Cartillier and Ruta Desai and Mukul Khanna and Dhruv Batra and Devi Parikh },
  journal={ arXiv preprint arXiv:2205.01652v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2205.01652v1 },
  abstract={ Egocentric augmented reality devices such as wearable glasses passively capture visual data as a human wearer tours a home environment. We envision a scenario wherein the human communicates with an AI agent powering such a device by asking questions (e.g., where did you last see my keys?). In order to succeed at this task, the egocentric AI assistant must (1) construct semantically rich and efficient scene memories that encode spatio-temporal information about objects seen during the tour and (2) possess the ability to understand the question and ground its answer into the semantic memory representation. Towards that end, we introduce (1) a new task - Episodic Memory Question Answering (EMQA) wherein an egocentric AI assistant is provided with a video sequence (the tour) and a question as an input and is asked to localize its answer to the question within the tour, (2) a dataset of grounded questions designed to probe the agent's spatio-temporal understanding of the tour, and (3) a model for the task that encodes the scene as an allocentric, top-down semantic feature map and grounds the question into the map to localize the answer. We show that our choice of episodic scene memory outperforms naive, off-the-shelf solutions for the task as well as a host of very competitive baselines and is robust to noise in depth, pose as well as camera jitter. The project page can be found at: https://samyak-268.github.io/emqa . }
}

@article{210810213v2,
  title={ SALIENCE: An Unsupervised User Adaptation Model for Multiple Wearable   Sensors Based Human Activity Recognition },
  author={ Ling Chen and Yi Zhang and Shenghuan Miao and Sirou Zhu and Rong Hu and Liangying Peng and Mingqi Lv },
  journal={ arXiv preprint arXiv:2108.10213v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2108.10213v2 },
  abstract={ Unsupervised user adaptation aligns the feature distributions of the data from training users and the new user, so a well-trained wearable human activity recognition (WHAR) model can be well adapted to the new user. With the development of wearable sensors, multiple wearable sensors based WHAR is gaining more and more attention. In order to address the challenge that the transferabilities of different sensors are different, we propose SALIENCE (unsupervised user adaptation model for multiple wearable sensors based human activity recognition) model. It aligns the data of each sensor separately to achieve local alignment, while uniformly aligning the data of all sensors to ensure global alignment. In addition, an attention mechanism is proposed to focus the activity classifier of SALIENCE on the sensors with strong feature discrimination and well distribution alignment. Experiments are conducted on two public WHAR datasets, and the experimental results show that our model can yield a competitive performance. }
}

@article{210701281v3,
  title={ Prescient teleoperation of humanoid robots },
  author={ Luigi Penco and Jean-Baptiste Mouret and Serena Ivaldi },
  journal={ arXiv preprint arXiv:2107.01281v3 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2107.01281v3 },
  abstract={ Humanoid robots could be versatile and intuitive human avatars that operate remotely in inaccessible places: the robot could reproduce in the remote location the movements of an operator equipped with a wearable motion capture device while sending visual feedback to the operator. While substantial progress has been made on transferring (''retargeting'') human motions to humanoid robots, a major problem preventing the deployment of such systems in real applications is the presence of communication delays between the human input and the feedback from the robot: even a few hundred milliseconds of delay can irreversibly disturb the operator, let alone a few seconds. To overcome these delays, we introduce a system in which a humanoid robot executes commands before it actually receives them, so that the visual feedback appears to be synchronized to the operator, whereas the robot executed the commands in the past. To do so, the robot continuously predicts future commands by querying a machine learning model that is trained on past trajectories and conditioned on the last received commands. In our experiments, an operator was able to successfully control a humanoid robot (32 degrees of freedom) with stochastic delays up to 2 seconds in several whole-body manipulation tasks, including reaching different targets, picking up a bottle, and placing a box at distinct locations. }
}

@article{200900647v4,
  title={ Lifelong Graph Learning },
  author={ Chen Wang and Yuheng Qiu and Dasong Gao and Sebastian Scherer },
  journal={ arXiv preprint arXiv:2009.00647v4 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2009.00647v4 },
  abstract={ Graph neural networks (GNN) are powerful models for many graph-structured tasks. Existing models often assume that the complete structure of the graph is available during training. In practice, however, graph-structured data is usually formed in a streaming fashion so that learning a graph continuously is often necessary. In this paper, we bridge GNN and lifelong learning by converting a continual graph learning problem to a regular graph learning problem so GNN can inherit the lifelong learning techniques developed for convolutional neural networks (CNN). We propose a new topology, the feature graph, which takes features as new nodes and turns nodes into independent graphs. This successfully converts the original problem of node classification to graph classification. In the experiments, we demonstrate the efficiency and effectiveness of feature graph networks (FGN) by continuously learning a sequence of classical graph datasets. We also show that FGN achieves superior performance in two applications, i.e., lifelong human action recognition with wearable devices and feature matching. To the best of our knowledge, FGN is the first method to bridge graph learning and lifelong learning via a novel graph topology. Source code is available at https://github.com/wang-chen/LGL }
}

@article{220309663v1,
  title={ An Improved Subject-Independent Stress Detection Model Applied to   Consumer-grade Wearable Devices },
  author={ Van-Tu Ninh and Manh-Duy Nguyen and Sinéad Smyth and Minh-Triet Tran and Graham Healy and Binh T. Nguyen and Cathal Gurrin },
  journal={ arXiv preprint arXiv:2203.09663v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2203.09663v1 },
  abstract={ Stress is a complex issue with wide-ranging physical and psychological impacts on human daily performance. Specifically, acute stress detection is becoming a valuable application in contextual human understanding. Two common approaches to training a stress detection model are subject-dependent and subject-independent training methods. Although subject-dependent training methods have proven to be the most accurate approach to build stress detection models, subject-independent models are a more practical and cost-efficient method, as they allow for the deployment of stress level detection and management systems in consumer-grade wearable devices without requiring training data for the end-user. To improve the performance of subject-independent stress detection models, in this paper, we introduce a stress-related bio-signal processing pipeline with a simple neural network architecture using statistical features extracted from multimodal contextual sensing sources including Electrodermal Activity (EDA), Blood Volume Pulse (BVP), and Skin Temperature (ST) captured from a consumer-grade wearable device. Using our proposed model architecture, we compare the accuracy between stress detection models that use measures from each individual signal source, and one model employing the fusion of multiple sensor sources. Extensive experiments on the publicly available WESAD dataset demonstrate that our proposed model outperforms conventional methods as well as providing 1.63\% higher mean accuracy score compared to the state-of-the-art model while maintaining a low standard deviation. Our experiments also show that combining features from multiple sources produce more accurate predictions than using only one sensor source individually. }
}

@article{220308217v1,
  title={ A Wearables-Driven Attack on Examination Proctoring },
  author={ Tasnia Ashrafi Heya and Abdul Serwadda and Isaac Griswold-Steiner and Richard Matovu },
  journal={ arXiv preprint arXiv:2203.08217v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2203.08217v1 },
  abstract={ Multiple choice questions are at the heart of many standardized tests and examinations at academic institutions allover the world. In this paper, we argue that recent advancements in sensing and human-computer interaction expose these types of questions to highly effective attacks that today's proctor's are simply not equipped to detect. We design one such attack based on a protocol of carefully orchestrated wrist movements combined with haptic and visual feedback mechanisms designed for stealthiness. The attack is done through collaboration between a knowledgeable student (i.e., a mercenary) and a weak student (i.e., the beneficiary) who depends on the mercenary for solutions. Through a combination of experiments and theoretical modeling, we show the attack to be highly effective. The paper makes the case for an outright ban on all tech gadgets inside examination rooms, irrespective of whether their usage appears benign to the plain eye. }
}

@article{220210562v1,
  title={ CROMOSim: A Deep Learning-based Cross-modality Inertial Measurement   Simulator },
  author={ Yujiao Hao and Boyu Wang and Rong Zheng },
  journal={ arXiv preprint arXiv:2202.10562v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2202.10562v1 },
  abstract={ With the prevalence of wearable devices, inertial measurement unit (IMU) data has been utilized in monitoring and assessment of human mobility such as human activity recognition (HAR). Training deep neural network (DNN) models for these tasks require a large amount of labeled data, which are hard to acquire in uncontrolled environments. To mitigate the data scarcity problem, we design CROMOSim, a cross-modality sensor simulator that simulates high fidelity virtual IMU sensor data from motion capture systems or monocular RGB cameras. It utilizes a skinned multi-person linear model (SMPL) for 3D body pose and shape representations, to enable simulation from arbitrary on-body positions. A DNN model is trained to learn the functional mapping from imperfect trajectory estimations in a 3D SMPL body tri-mesh due to measurement noise, calibration errors, occlusion and other modeling artifacts, to IMU data. We evaluate the fidelity of CROMOSim simulated data and its utility in data augmentation on various HAR datasets. Extensive experiment results show that the proposed model achieves a 6.7\% improvement over baseline methods in a HAR task. }
}

@article{220206547v1,
  title={ Video2IMU: Realistic IMU features and signals from videos },
  author={ Arttu Lämsä and Jaakko Tervonen and Jussi Liikka and Constantino Álvarez Casado and Miguel Bordallo López },
  journal={ arXiv preprint arXiv:2202.06547v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2202.06547v1 },
  abstract={ Human Activity Recognition (HAR) from wearable sensor data identifies movements or activities in unconstrained environments. HAR is a challenging problem as it presents great variability across subjects. Obtaining large amounts of labelled data is not straightforward, since wearable sensor signals are not easy to label upon simple human inspection. In our work, we propose the use of neural networks for the generation of realistic signals and features using human activity monocular videos. We show how these generated features and signals can be utilized, instead of their real counterparts, to train HAR models that can recognize activities using signals obtained with wearable sensors. To prove the validity of our methods, we perform experiments on an activity recognition dataset created for the improvement of industrial work safety. We show that our model is able to realistically generate virtual sensor signals and features usable to train a HAR classifier with comparable performance as the one trained using real sensor data. Our results enable the use of available, labelled video data for training HAR models to classify signals from wearable sensors. }
}

@article{220109135v1,
  title={ MIDAS: Deep learning human action intention prediction from natural eye   movement patterns },
  author={ Paul Festor and Ali Shafti and Alex Harston and Michey Li and Pavel Orlov and A. Aldo Faisal },
  journal={ arXiv preprint arXiv:2201.09135v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2201.09135v1 },
  abstract={ Eye movements have long been studied as a window into the attentional mechanisms of the human brain and made accessible as novelty style human-machine interfaces. However, not everything that we gaze upon, is something we want to interact with; this is known as the Midas Touch problem for gaze interfaces. To overcome the Midas Touch problem, present interfaces tend not to rely on natural gaze cues, but rather use dwell time or gaze gestures. Here we present an entirely data-driven approach to decode human intention for object manipulation tasks based solely on natural gaze cues. We run data collection experiments where 16 participants are given manipulation and inspection tasks to be performed on various objects on a table in front of them. The subjects' eye movements are recorded using wearable eye-trackers allowing the participants to freely move their head and gaze upon the scene. We use our Semantic Fovea, a convolutional neural network model to obtain the objects in the scene and their relation to gaze traces at every frame. We then evaluate the data and examine several ways to model the classification task for intention prediction. Our evaluation shows that intention prediction is not a naive result of the data, but rather relies on non-linear temporal processing of gaze cues. We model the task as a time series classification problem and design a bidirectional Long-Short-Term-Memory (LSTM) network architecture to decode intentions. Our results show that we can decode human intention of motion purely from natural gaze cues and object relative position, with \$91.9\\textbackslash{}\%\$ accuracy. Our work demonstrates the feasibility of natural gaze as a Zero-UI interface for human-machine interaction, i.e., users will only need to act naturally, and do not need to interact with the interface itself or deviate from their natural eye movement patterns. }
}

@article{220100111v1,
  title={ Role of Data Augmentation Strategies in Knowledge Distillation for   Wearable Sensor Data },
  author={ Eun Som Jeon and Anirudh Som and Ankita Shukla and Kristina Hasanaj and Matthew P. Buman and Pavan Turaga },
  journal={ arXiv preprint arXiv:2201.00111v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2201.00111v1 },
  abstract={ Deep neural networks are parametrized by several thousands or millions of parameters, and have shown tremendous success in many classification problems. However, the large number of parameters makes it difficult to integrate these models into edge devices such as smartphones and wearable devices. To address this problem, knowledge distillation (KD) has been widely employed, that uses a pre-trained high capacity network to train a much smaller network, suitable for edge devices. In this paper, for the first time, we study the applicability and challenges of using KD for time-series data for wearable devices. Successful application of KD requires specific choices of data augmentation methods during training. However, it is not yet known if there exists a coherent strategy for choosing an augmentation approach during KD. In this paper, we report the results of a detailed study that compares and contrasts various common choices and some hybrid data augmentation strategies in KD based human activity analysis. Research in this area is often limited as there are not many comprehensive databases available in the public domain from wearable devices. Our study considers databases from small scale publicly available to one derived from a large scale interventional study into human activity and sedentary behavior. We find that the choice of data augmentation techniques during KD have a variable level of impact on end performance, and find that the optimal network choice as well as data augmentation strategies are specific to a dataset at hand. However, we also conclude with a general set of recommendations that can provide a strong baseline performance across databases. }
}

@article{211214885v1,
  title={ A Test Bench For Evaluating Exoskeletons For Upper Limb Rehabilitation },
  author={ Clautilde Nguiadem and Maxime Raison and Sofiane Achiche },
  journal={ arXiv preprint arXiv:2112.14885v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2112.14885v1 },
  abstract={ The potential of wearable robotics technology is undeniable. However, quantifying its value is difficult. Various types of exoskeleton robots have already been developed and tested for upper limb rehabilitation but, evaluations are not standardized, particularly in pediatric rehabilitation. This paper proposes a methodology for the quantitative evaluation of upper limb exoskeletons that, like a test bench, would serve for replicable testing. We determined the range of motion (ROM) and joint torques using both kinematic modeling and experimental measurements (using sensors integrated into Dynamixel actuators). The proposed test bench can provide an accurate range of motion (ROM) and joint torques during the pronation-supination (PS) task. The range of motion obtained with the physical prototype was approximately 156.26 +- 4.71\{\\textbackslash{}deg\} during the PS task, while it was approximately 146.84 +- 14.32\{\\textbackslash{}deg\} for the multibody model. The results show that the average range of experimental torques (0.28 +- 0.06 N.m) was overestimated by 40\% and just 3.4\%, respectively, when compared to the average range of simulated torques (0.2 +- 0.05 N.m) and to the highest range of simulated torques (0.29 N.m). For the experimental measurements, test-retest reliability was excellent (0.96-0.98) within sessions and excellent (0.93) or good (0.81-0.86) between sessions. Finally, the suggested approach provides a ROM close to the normal ROM necessary during PS tasks. These results validate the measurements' accuracy and underline the proposed methodology's relevance. The proposed test bench could become a reference standard for evaluating exoskeletons. This study also addresses a methodological aspect on the accurate assessment of joint torques that can serve in applications such as the sizing of actuators in exoskeletons or the non-invasive evaluation of muscle forces in the human body. }
}

@article{210514680v2,
  title={ ThumbTrak: Recognizing Micro-finger Poses Using a Ring with Proximity   Sensing },
  author={ Wei Sun and Franklin Mingzhe Li and Congshu Huang and Zhenyu Lei and Benjamin Steeper and Songyun Tao and Feng Tian and Cheng Zhang },
  journal={ arXiv preprint arXiv:2105.14680v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2105.14680v2 },
  abstract={ ThumbTrak is a novel wearable input device that recognizes 12 micro-finger poses in real-time. Poses are characterized by the thumb touching each of the 12 phalanges on the hand. It uses a thumb-ring, built with a flexible printed circuit board, which hosts nine proximity sensors. Each sensor measures the distance from the thumb to various parts of the palm or other fingers. ThumbTrak uses a support-vector-machine (SVM) model to classify finger poses based on distance measurements in real-time. A user study with ten participants showed that ThumbTrak could recognize 12 micro finger poses with an average accuracy of 93.6\%. We also discuss potential opportunities and challenges in applying ThumbTrak in real-world applications. }
}

@article{211211330v2,
  title={ PrimSeq: a deep learning-based pipeline to quantitate rehabilitation   training },
  author={ Avinash Parnandi and Aakash Kaku and Anita Venkatesan and Natasha Pandit and Audre Wirtanen and Haresh Rajamohan and Kannan Venkataramanan and Dawn Nilsen and Carlos Fernandez-Granda and Heidi Schambra },
  journal={ arXiv preprint arXiv:2112.11330v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2112.11330v2 },
  abstract={ Stroke rehabilitation seeks to increase neuroplasticity through the repeated practice of functional motions, but may have minimal impact on recovery because of insufficient repetitions. The optimal training content and quantity are currently unknown because no practical tools exist to measure them. Here, we present PrimSeq, a pipeline to classify and count functional motions trained in stroke rehabilitation. Our approach integrates wearable sensors to capture upper-body motion, a deep learning model to predict motion sequences, and an algorithm to tally motions. The trained model accurately decomposes rehabilitation activities into component functional motions, outperforming competitive machine learning methods. PrimSeq furthermore quantifies these motions at a fraction of the time and labor costs of human experts. We demonstrate the capabilities of PrimSeq in previously unseen stroke patients with a range of upper extremity motor impairment. We expect that these advances will support the rigorous measurement required for quantitative dosing trials in stroke rehabilitation. }
}

@article{211205893v1,
  title={ Hybrid Neural Networks for On-device Directional Hearing },
  author={ Anran Wang and Maruchi Kim and Hao Zhang and Shyamnath Gollakota },
  journal={ arXiv preprint arXiv:2112.05893v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2112.05893v1 },
  abstract={ On-device directional hearing requires audio source separation from a given direction while achieving stringent human-imperceptible latency requirements. While neural nets can achieve significantly better performance than traditional beamformers, all existing models fall short of supporting low-latency causal inference on computationally-constrained wearables. We present DeepBeam, a hybrid model that combines traditional beamformers with a custom lightweight neural net. The former reduces the computational burden of the latter and also improves its generalizability, while the latter is designed to further reduce the memory and computational overhead to enable real-time and low-latency operations. Our evaluation shows comparable performance to state-of-the-art causal inference models on synthetic data while achieving a 5x reduction of model size, 4x reduction of computation per second, 5x reduction in processing time and generalizing better to real hardware data. Further, our real-time hybrid model runs in 8 ms on mobile CPUs designed for low-power wearable devices and achieves an end-to-end latency of 17.5 ms. }
}

@article{210700389v2,
  title={ Investigating the Reliability of Self-report Data in the Wild: The Quest   for Ground Truth },
  author={ Nan Gao and Mohammad Saiedur Rahaman and Wei Shao and Flora D. Salim },
  journal={ arXiv preprint arXiv:2107.00389v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2107.00389v2 },
  abstract={ Inferring human mental state (e.g., emotion, depression, engagement) with sensing technology is one of the most valuable challenges in the affective computing area, which has a profound impact in all industries interacting with humans. The self-report survey is the most common way to quantify how people think, but prone to subjectivity and various responses bias. It is usually used as the ground truth for human mental state prediction. In recent years, many data-driven machine learning models are built based on self-report annotations as the target value. In this research, we investigate the reliability of self-report surveys in the wild by studying the confidence level of responses and survey completion time. We conduct a case study (i.e., student engagement inference) by recruiting 23 students in a high school setting over a period of 4 weeks. Our participants volunteered 488 self-reported responses and data from their wearable sensors. We also find the physiologically measured student engagement and perceived student engagement are not always consistent. The findings from this research have great potential to benefit future studies in predicting engagement, depression, stress, and other emotion-related states in the field of affective computing and sensing technologies. }
}

@article{211114960v1,
  title={ Validating CircaCP: a Generic Sleep-Wake Cycle Detection Algorithm },
  author={ Shanshan Chen and Xinxin Sun },
  journal={ arXiv preprint arXiv:2111.14960v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2111.14960v1 },
  abstract={ Sleep-wake cycle detection is a key step when extrapolating sleep patterns from actigraphy data. Numerous supervised detection algorithms have been developed with parameters estimated from and optimized for a particular dataset, yet their generalizability from sensor to sensor or study to study is unknown. In this paper, we propose and validate an unsupervised algorithm -- CircaCP -- to detect sleep-wake cycles from minute-by-minute actigraphy data. It first uses a robust cosinor model to estimate circadian rhythm, then searches for a single change point (CP) within each cycle. We used CircaCP to estimate sleep/wake onset times (S/WOTs) from 2125 indviduals' data in the MESA Sleep study and compared the estimated S/WOTs against self-reported S/WOT event markers. Lastly, we quantified the biases between estimated and self-reported S/WOTs, as well as variation in S/WOTs contributed by the two methods, using linear mixed-effects models and variance component analysis.   On average, SOTs estimated by CircaCP were five minutes behind those reported by event markers, and WOTs estimated by CircaCP were less than one minute behind those reported by markers. These differences accounted for less than 0.2\% variability in SOTs and in WOTs, taking into account other sources of between-subject variations. By focusing on the commonality in human circadian rhythms captured by actigraphy, our algorithm transferred seamlessly from hip-worn ActiGraph data collected from children in our previous study to wrist-worn Actiwatch data collected from adults. The large between- and within-subject variability highlights the need for estimating individual-level S/WOTs when conducting actigraphy research. The generalizability of our algorithm also suggests that it could be widely applied to actigraphy data collected by other wearable sensors. }
}

@article{211112685v1,
  title={ EgoRenderer: Rendering Human Avatars from Egocentric Camera Images },
  author={ Tao Hu and Kripasindhu Sarkar and Lingjie Liu and Matthias Zwicker and Christian Theobalt },
  journal={ arXiv preprint arXiv:2111.12685v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2111.12685v1 },
  abstract={ We present EgoRenderer, a system for rendering full-body neural avatars of a person captured by a wearable, egocentric fisheye camera that is mounted on a cap or a VR headset. Our system renders photorealistic novel views of the actor and her motion from arbitrary virtual camera locations. Rendering full-body avatars from such egocentric images come with unique challenges due to the top-down view and large distortions. We tackle these challenges by decomposing the rendering process into several steps, including texture synthesis, pose construction, and neural image translation. For texture synthesis, we propose Ego-DPNet, a neural network that infers dense correspondences between the input fisheye images and an underlying parametric body model, and to extract textures from egocentric inputs. In addition, to encode dynamic appearances, our approach also learns an implicit texture stack that captures detailed appearance variation across poses and viewpoints. For correct pose generation, we first estimate body pose from the egocentric view using a parametric model. We then synthesize an external free-viewpoint pose image by projecting the parametric model to the user-specified target viewpoint. We next combine the target pose image and the textures into a combined feature image, which is transformed into the output color image using a neural image translation network. Experimental evaluations show that EgoRenderer is capable of generating realistic free-viewpoint avatars of a person wearing an egocentric camera. Comparisons to several baselines demonstrate the advantages of our approach. }
}

@article{211108502v1,
  title={ Human-error-potential Estimation based on Wearable Biometric Sensors },
  author={ Hiroki Ohashi and Hiroto Nagayoshi },
  journal={ arXiv preprint arXiv:2111.08502v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2111.08502v1 },
  abstract={ This study tackles on a new problem of estimating human-error potential on a shop floor on the basis of wearable sensors. Unlike existing studies that utilize biometric sensing technology to estimate people's internal state such as fatigue and mental stress, we attempt to estimate the human-error potential in a situation where a target person does not stay calm, which is much more difficult as sensor noise significantly increases. We propose a novel formulation, in which the human-error-potential estimation problem is reduced to a classification problem, and introduce a new method that can be used for solving the classification problem even with noisy sensing data. The key ideas are to model the process of calculating biometric indices probabilistically so that the prior knowledge on the biometric indices can be integrated, and to utilize the features that represent the movement of target persons in combination with biometric features. The experimental analysis showed that our method effectively estimates the human-error potential. }
}

@article{211014307v2,
  title={ RF-Based Human Activity Recognition Using Signal Adapted Convolutional   Neural Network },
  author={ Zhe Chen and Chao Cai and Tianyue Zheng and Jun Luo and Jie Xiong and Xin Wang },
  journal={ arXiv preprint arXiv:2110.14307v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2110.14307v2 },
  abstract={ Human Activity Recognition (HAR) plays a critical role in a wide range of real-world applications, and it is traditionally achieved via wearable sensing. Recently, to avoid the burden and discomfort caused by wearable devices, device-free approaches exploiting RF signals arise as a promising alternative for HAR. Most of the latest device-free approaches require training a large deep neural network model in either time or frequency domain, entailing extensive storage to contain the model and intensive computations to infer activities. Consequently, even with some major advances on device-free HAR, current device-free approaches are still far from practical in real-world scenarios where the computation and storage resources possessed by, for example, edge devices, are limited. Therefore, we introduce HAR-SAnet which is a novel RF-based HAR framework. It adopts an original signal adapted convolutional neural network architecture: instead of feeding the handcraft features of RF signals into a classifier, HAR-SAnet fuses them adaptively from both time and frequency domains to design an end-to-end neural network model. We apply point-wise grouped convolution and depth-wise separable convolutions to confine the model scale and to speed up the inference execution time. The experiment results show that the recognition accuracy of HAR-SAnet outperforms state-of-the-art algorithms and systems. }
}

@article{211012163v1,
  title={ Adversarial Deep Feature Extraction Network for User Independent Human   Activity Recognition },
  author={ Sungho Suh and Vitor Fortes Rey and Paul Lukowicz },
  journal={ arXiv preprint arXiv:2110.12163v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2110.12163v1 },
  abstract={ User dependence remains one of the most difficult general problems in Human Activity Recognition (HAR), in particular when using wearable sensors. This is due to the huge variability of the way different people execute even the simplest actions. In addition, detailed sensor fixtures and placement will be different for different people or even at different times for the same users. In theory, the problem can be solved by a large enough data set. However, recording data sets that capture the entire diversity of complex activity sets is seldom practicable. Instead, models are needed that focus on features that are invariant across users. To this end, we present an adversarial subject-independent feature extraction method with the maximum mean discrepancy (MMD) regularization for human activity recognition. The proposed model is capable of learning a subject-independent embedding feature representation from multiple subjects datasets and generalizing it to unseen target subjects. The proposed network is based on the adversarial encoder-decoder structure with the MMD realign the data distribution over multiple subjects. Experimental results show that the proposed method not only outperforms state-of-the-art methods over the four real-world datasets but also improves the subject generalization effectively. We evaluate the method on well-known public data sets showing that it significantly improves user-independent performance and reduces variance in results. }
}

@article{211201849v1,
  title={ Cross-modal Knowledge Distillation for Vision-to-Sensor Action   Recognition },
  author={ Jianyuan Ni and Raunak Sarbajna and Yang Liu and Anne H. H. Ngu and Yan Yan },
  journal={ arXiv preprint arXiv:2112.01849v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2112.01849v1 },
  abstract={ Human activity recognition (HAR) based on multi-modal approach has been recently shown to improve the accuracy performance of HAR. However, restricted computational resources associated with wearable devices, i.e., smartwatch, failed to directly support such advanced methods. To tackle this issue, this study introduces an end-to-end Vision-to-Sensor Knowledge Distillation (VSKD) framework. In this VSKD framework, only time-series data, i.e., accelerometer data, is needed from wearable devices during the testing phase. Therefore, this framework will not only reduce the computational demands on edge devices, but also produce a learning model that closely matches the performance of the computational expensive multi-modal approach. In order to retain the local temporal relationship and facilitate visual deep learning models, we first convert time-series data to two-dimensional images by applying the Gramian Angular Field ( GAF) based encoding method. We adopted ResNet18 and multi-scale TRN with BN-Inception as teacher and student network in this study, respectively. A novel loss function, named Distance and Angle-wised Semantic Knowledge loss (DASK), is proposed to mitigate the modality variations between the vision and the sensor domain. Extensive experimental results on UTD-MHAD, MMAct, and Berkeley-MHAD datasets demonstrate the effectiveness and competitiveness of the proposed VSKD model which can deployed on wearable sensors. }
}

@article{210906177v1,
  title={ Towards a Computational Framework for Automated Discovery and Modeling   of Biological Rhythms from Wearable Data Streams },
  author={ Runze Yan and Afsaneh Doryab },
  journal={ arXiv preprint arXiv:2109.06177v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2109.06177v1 },
  abstract={ Modeling biological rhythms helps understand the complex principles behind the physical and psychological abnormalities of human bodies, to plan life schedules, and avoid persisting fatigue and mood and sleep alterations due to the desynchronization of those rhythms. The first step in modeling biological rhythms is to identify their characteristics, such as cyclic periods, phase, and amplitude. However, human rhythms are susceptible to external events, which cause irregular fluctuations in waveforms and affect the characterization of each rhythm. In this paper, we present our exploratory work towards developing a computational framework for automated discovery and modeling of human rhythms. We first identify cyclic periods in time series data using three different methods and test their performance on both synthetic data and real fine-grained biological data. We observe consistent periods are detected by all three methods. We then model inner cycles within each period through identifying change points to observe fluctuations in biological data that may inform the impact of external events on human rhythms. The results provide initial insights into the design of a computational framework for discovering and modeling human rhythms. }
}

@article{210812305v1,
  title={ EarGate: Gait-based User Identification with In-ear Microphones },
  author={ Andrea Ferlini and Dong Ma and Robert Harle and Cecilia Mascolo },
  journal={ arXiv preprint arXiv:2108.12305v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2108.12305v1 },
  abstract={ Human gait is a widely used biometric trait for user identification and recognition. Given the wide-spreading, steady diffusion of ear-worn wearables (Earables) as the new frontier of wearable devices, we investigate the feasibility of earable-based gait identification. Specifically, we look at gait-based identification from the sounds induced by walking and propagated through the musculoskeletal system in the body. Our system, EarGate, leverages an in-ear facing microphone which exploits the earable's occlusion effect to reliably detect the user's gait from inside the ear canal, without impairing the general usage of earphones. With data collected from 31 subjects, we show that EarGate achieves up to 97.26\% Balanced Accuracy (BAC) with very low False Acceptance Rate (FAR) and False Rejection Rate (FRR) of 3.23\% and 2.25\%, respectively. Further, our measurement of power consumption and latency investigates how this gait identification model could live both as a stand-alone or cloud-coupled earable system. }
}

@article{200503948v3,
  title={ Layer-wise training convolutional neural networks with smaller filters   for human activity recognition using wearable sensors },
  author={ Yin Tang and Qi Teng and Lei Zhang and Fuhong Min and Jun He },
  journal={ arXiv preprint arXiv:2005.03948v3 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2005.03948v3 },
  abstract={ Recently, convolutional neural networks (CNNs) have set latest state-of-the-art on various human activity recognition (HAR) datasets. However, deep CNNs often require more computing resources, which limits their applications in embedded HAR. Although many successful methods have been proposed to reduce memory and FLOPs of CNNs, they often involve special network architectures designed for visual tasks, which are not suitable for deep HAR tasks with time series sensor signals, due to remarkable discrepancy. Therefore, it is necessary to develop lightweight deep models to perform HAR. As filter is the basic unit in constructing CNNs, it deserves further research whether re-designing smaller filters is applicable for deep HAR. In the paper, inspired by the idea, we proposed a lightweight CNN using Lego filters for HAR. A set of lower-dimensional filters is used as Lego bricks to be stacked for conventional filters, which does not rely on any special network structure. The local loss function is used to train model. To our knowledge, this is the first paper that proposes lightweight CNN for HAR in ubiquitous and wearable computing arena. The experiment results on five public HAR datasets, UCI-HAR dataset, OPPORTUNITY dataset, UNIMIB-SHAR dataset, PAMAP2 dataset, and WISDM dataset collected from either smartphones or multiple sensor nodes, indicate that our novel Lego CNN with local loss can greatly reduce memory and computation cost over CNN, while achieving higher accuracy. That is to say, the proposed model is smaller, faster and more accurate. Finally, we evaluate the actual performance on an Android smartphone. }
}

@article{200405768v3,
  title={ Sequential Weakly Labeled Multi-Activity Localization and Recognition on   Wearable Sensors using Recurrent Attention Networks },
  author={ Kun Wang and Jun He and Lei Zhang },
  journal={ arXiv preprint arXiv:2004.05768v3 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2004.05768v3 },
  abstract={ With the popularity and development of the wearable devices such as smartphones, human activity recognition (HAR) based on sensors has become as a key research area in human computer interaction and ubiquitous computing. The emergence of deep learning leads to a recent shift in the research of HAR, which requires massive strictly labeled data. In comparison with video data, activity data recorded from accelerometer or gyroscope is often more difficult to interpret and segment. Recently, several attention mechanisms are proposed to handle the weakly labeled human activity data, which do not require accurate data annotation. However, these attention-based models can only handle the weakly labeled dataset whose sample includes one target activity, as a result it limits efficiency and practicality. In the paper, we propose a recurrent attention networks (RAN) to handle sequential weakly labeled multi-activity recognition and location tasks. The model can repeatedly perform steps of attention on multiple activities of one sample and each step is corresponding to the current focused activity. The effectiveness of the RAN model is validated on a collected sequential weakly labeled multi-activity dataset and the other two public datasets. The experiment results show that our RAN model can simultaneously infer multi-activity types from the coarse-grained sequential weak labels and determine specific locations of every target activity with only knowledge of which types of activities contained in the long sequence. It will greatly reduce the burden of manual labeling. The code of our work is available at https://github.com/KennCoder7/RAN. }
}

@article{210713411v1,
  title={ Predicting the Future from First Person (Egocentric) Vision: A Survey },
  author={ Ivan Rodin and Antonino Furnari and Dimitrios Mavroedis and Giovanni Maria Farinella },
  journal={ arXiv preprint arXiv:2107.13411v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2107.13411v1 },
  abstract={ Egocentric videos can bring a lot of information about how humans perceive the world and interact with the environment, which can be beneficial for the analysis of human behaviour. The research in egocentric video analysis is developing rapidly thanks to the increasing availability of wearable devices and the opportunities offered by new large-scale egocentric datasets. As computer vision techniques continue to develop at an increasing pace, the tasks related to the prediction of future are starting to evolve from the need of understanding the present. Predicting future human activities, trajectories and interactions with objects is crucial in applications such as human-robot interaction, assistive wearable technologies for both industrial and daily living scenarios, entertainment and virtual or augmented reality. This survey summarises the evolution of studies in the context of future prediction from egocentric vision making an overview of applications, devices, existing problems, commonly used datasets, models and input modalities. Our analysis highlights that methods for future prediction from egocentric vision can have a significant impact in a range of applications and that further research efforts should be devoted to the standardisation of tasks and the proposal of datasets considering real-world scenarios such as the ones with an industrial vocation. }
}

@article{210706859v1,
  title={ A novel approach for modelling and classifying sit-to-stand kinematics   using inertial sensors },
  author={ Maitreyee Wairagkar and Emma Villeneuve and Rachel King and Balazs Janko and Malcolm Burnett and Ann Ashburn and Veena Agarwal and R. Simon Sherratt and William Holderbaum and William Harwin },
  journal={ arXiv preprint arXiv:2107.06859v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2107.06859v1 },
  abstract={ Sit-to-stand transitions are an important part of activities of daily living and play a key role in functional mobility in humans. The sit-to-stand movement is often affected in older adults due to frailty and in patients with motor impairments such as Parkinson's disease leading to falls. Studying kinematics of sit-to-stand transitions can provide insight in assessment, monitoring and developing rehabilitation strategies for the affected populations. We propose a three-segment body model for estimating sit-to-stand kinematics using only two wearable inertial sensors, placed on the shank and back. Reducing the number of sensors to two instead of one per body segment facilitates monitoring and classifying movements over extended periods, making it more comfortable to wear while reducing the power requirements of sensors. We applied this model on 10 younger healthy adults (YH), 12 older healthy adults (OH) and 12 people with Parkinson's disease (PwP). We have achieved this by incorporating unique sit-to-stand classification technique using unsupervised learning in the model based reconstruction of angular kinematics using extended Kalman filter. Our proposed model showed that it was possible to successfully estimate thigh kinematics despite not measuring the thigh motion with inertial sensor. We classified sit-to-stand transitions, sitting and standing states with the accuracies of 98.67\%, 94.20\% and 91.41\% for YH, OH and PwP respectively. We have proposed a novel integrated approach of modelling and classification for estimating the body kinematics during sit-to-stand motion and successfully applied it on YH, OH and PwP groups. }
}

@article{210413514v5,
  title={ A Perceptual Model for Eccentricity-dependent Spatio-temporal Flicker   Fusion and its Applications to Foveated Graphics },
  author={ Brooke Krajancich and Petr Kellnhofer and Gordon Wetzstein },
  journal={ arXiv preprint arXiv:2104.13514v5 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2104.13514v5 },
  abstract={ Virtual and augmented reality (VR/AR) displays strive to provide a resolution, framerate and field of view that matches the perceptual capabilities of the human visual system, all while constrained by limited compute budgets and transmission bandwidths of wearable computing systems. Foveated graphics techniques have emerged that could achieve these goals by exploiting the falloff of spatial acuity in the periphery of the visual field. However, considerably less attention has been given to temporal aspects of human vision, which also vary across the retina. This is in part due to limitations of current eccentricity-dependent models of the visual system. We introduce a new model, experimentally measuring and computationally fitting eccentricity-dependent critical flicker fusion thresholds jointly for both space and time. In this way, our model is unique in enabling the prediction of temporal information that is imperceptible for a certain spatial frequency, eccentricity, and range of luminance levels. We validate our model with an image quality user study, and use it to predict potential bandwidth savings 7x higher than those afforded by current spatial-only foveated models. As such, this work forms the enabling foundation for new temporally foveated graphics techniques. }
}

@article{200900210v5,
  title={ Semantics-aware Adaptive Knowledge Distillation for Sensor-to-Vision   Action Recognition },
  author={ Yang Liu and Keze Wang and Guanbin Li and Liang Lin },
  journal={ arXiv preprint arXiv:2009.00210v5 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2009.00210v5 },
  abstract={ Existing vision-based action recognition is susceptible to occlusion and appearance variations, while wearable sensors can alleviate these challenges by capturing human motion with one-dimensional time-series signal. For the same action, the knowledge learned from vision sensors and wearable sensors, may be related and complementary. However, there exists significantly large modality difference between action data captured by wearable-sensor and vision-sensor in data dimension, data distribution and inherent information content. In this paper, we propose a novel framework, named Semantics-aware Adaptive Knowledge Distillation Networks (SAKDN), to enhance action recognition in vision-sensor modality (videos) by adaptively transferring and distilling the knowledge from multiple wearable sensors. The SAKDN uses multiple wearable-sensors as teacher modalities and uses RGB videos as student modality. To preserve local temporal relationship and facilitate employing visual deep learning model, we transform one-dimensional time-series signals of wearable sensors to two-dimensional images by designing a gramian angular field based virtual image generation model. Then, we build a novel Similarity-Preserving Adaptive Multi-modal Fusion Module to adaptively fuse intermediate representation knowledge from different teacher networks. Finally, to fully exploit and transfer the knowledge of multiple well-trained teacher networks to the student network, we propose a novel Graph-guided Semantically Discriminative Mapping loss, which utilizes graph-guided ablation analysis to produce a good visual explanation highlighting the important regions across modalities and concurrently preserving the interrelations of original data. Experimental results on Berkeley-MHAD, UTD-MHAD and MMAct datasets well demonstrate the effectiveness of our proposed SAKDN. }
}

@article{210409396v1,
  title={ Continual Learning in Sensor-based Human Activity Recognition: an   Empirical Benchmark Analysis },
  author={ Saurav Jha and Martin Schiemer and Franco Zambonelli and Juan Ye },
  journal={ arXiv preprint arXiv:2104.09396v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2104.09396v1 },
  abstract={ Sensor-based human activity recognition (HAR), i.e., the ability to discover human daily activity patterns from wearable or embedded sensors, is a key enabler for many real-world applications in smart homes, personal healthcare, and urban planning. However, with an increasing number of applications being deployed, an important question arises: how can a HAR system autonomously learn new activities over a long period of time without being re-engineered from scratch? This problem is known as continual learning and has been particularly popular in the domain of computer vision, where several techniques to attack it have been developed. This paper aims to assess to what extent such continual learning techniques can be applied to the HAR domain. To this end, we propose a general framework to evaluate the performance of such techniques on various types of commonly used HAR datasets. We then present a comprehensive empirical analysis of their computational cost and effectiveness of tackling HAR-specific challenges (i.e., sensor noise and labels' scarcity). The presented results uncover useful insights on their applicability and suggest future research directions for HAR systems. Our code, models and data are available at https://github.com/srvCodes/continual-learning-benchmark. }
}

@article{210315704v2,
  title={ Are Multilevel functional models the next step in sports biomechanics   and wearable technology? A case study of Knee Biomechanics patterns in   typical training sessions of recreational runners },
  author={ Marcos Matabuena and Sherveen Riazati and Nick Caplan and Phil Hayes },
  journal={ arXiv preprint arXiv:2103.15704v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2103.15704v2 },
  abstract={ This paper illustrates how multilevel functional models can detect and characterize biomechanical changes along different sport training sessions. Our analysis focuses on the relevant cases to identify differences in knee biomechanics in recreational runners during low and high-intensity exercise sessions with the same energy expenditure by recording \$20\$ steps. To do so, we review the existing literature of multilevel models, and then, we propose a new hypothesis test to look at the changes between different levels of the multilevel model as low and high-intensity training sessions. We also evaluate the reliability of measures recorded in three-dimension knee angles from the functional intra-class correlation coefficient (ICC) obtained from the decomposition performed with the multilevel funcional model taking into account \$20\$ measures recorded in each test. The results show that there are no statistically significant differences between the two modes of exercise. However, we have to be careful with the conclusions since, as we have shown, human gait-patterns are very individual and heterogeneous between groups of athletes, and other alternatives to the p-value may be more appropriate to detect statistical differences in biomechanical changes in this context. }
}

@article{210401793v1,
  title={ Analysis of bio-electro-chemical signals from passive sweat-based   wearable electro-impedance spectroscopy (EIS) towards assessing blood glucose   modulations },
  author={ Devangsingh Sankhala and Madhavi Pali and Kai-Chun Lin and Badrinath Jagannath and Sriram Muthukumar and Shalini Prasad },
  journal={ arXiv preprint arXiv:2104.01793v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2104.01793v1 },
  abstract={ There has been a recent tremendous interest in label-free detection of biomarkers which is a critical enabler of point-of-need diagnostics. A low-power, small form factor, multiplexed wearable system is proposed for continuous detection of glucose in passively expressed sweat using electrochemical impedance spectroscopy (EIS) measurement. The wearable EIS system consists of a sensing analog front end integrated with low-volume (1-5 \$\\textbackslash{}mu\$L) ultra-sensitive flexible biosensors. A passive sweat sensor was designed to integrate a glucose oxidase electrochemical system on active semiconducting material. The non-faradaic EIS response of the biosensor was used to calibrate the analog front end response using ratiometric Discrete Fourier Transform (DFT) for a shorter measurement time. In this work, a stringent assessment of a continuous glucose sensing platform is performed in a bottom-up approach, going from the biosensor to the system to the interaction with a human subject. The active semiconductor-based biosensors are dosed with glucose concentrations ranging from 5-200 mg/dL and detection is performed using the analog front end. In addition, a detailed analysis of battery life and performance of a wearable EIS system is discussed to define a figure of merit for an optimally integrated design. Moreover, a continuous glucose detection test is performed on a healthy human subject cohort to investigate the stability of the sensor-system mechanism for an 8-hour period, and a time-series-based, auto-regressive (AR) model was created for the system. }
}

@article{201007432v2,
  title={ Viewmaker Networks: Learning Views for Unsupervised Representation   Learning },
  author={ Alex Tamkin and Mike Wu and Noah Goodman },
  journal={ arXiv preprint arXiv:2010.07432v2 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2010.07432v2 },
  abstract={ Many recent methods for unsupervised representation learning train models to be invariant to different ''views,'' or distorted versions of an input. However, designing these views requires considerable trial and error by human experts, hindering widespread adoption of unsupervised representation learning methods across domains and modalities. To address this, we propose viewmaker networks: generative models that learn to produce useful views from a given input. Viewmakers are stochastic bounded adversaries: they produce views by generating and then adding an \$\\textbackslash{}ell\_p\$-bounded perturbation to the input, and are trained adversarially with respect to the main encoder network. Remarkably, when pretraining on CIFAR-10, our learned views enable comparable transfer accuracy to the well-tuned SimCLR augmentations -- despite not including transformations like cropping or color jitter. Furthermore, our learned views significantly outperform baseline augmentations on speech recordings (+9\% points, on average) and wearable sensor data (+17\% points). Viewmakers can also be combined with handcrafted views: they improve robustness to common image corruptions and can increase transfer performance in cases where handcrafted views are less explored. These results suggest that viewmakers may provide a path towards more general representation learning algorithms -- reducing the domain expertise and effort needed to pretrain on a much wider set of domains. Code is available at https://github.com/alextamkin/viewmaker. }
}

@article{210300599v2,
  title={ Machine learning for detection of stenoses and aneurysms: application in   a physiologically realistic virtual patient database },
  author={ Gareth Jones and Jim Parr and Perumal Nithiarasu and Sanjay Pant },
  journal={ arXiv preprint arXiv:2103.00599v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2103.00599v2 },
  abstract={ This study presents an application of machine learning (ML) methods for detecting the presence of stenoses and aneurysms in the human arterial system. Four major forms of arterial disease -- carotid artery stenosis (CAS), subclavian artery stenosis (SAC), peripheral arterial disease (PAD), and abdominal aortic aneurysms (AAA) -- are considered. The ML methods are trained and tested on a physiologically realistic virtual patient database (VPD) containing 28,868 healthy subjects, which is adapted from the authors previous work and augmented to include the four disease forms. Six ML methods -- Naive Bayes, Logistic Regression, Support Vector Machine, Multi-layer Perceptron, Random Forests, and Gradient Boosting -- are compared with respect to classification accuracies and it is found that the tree-based methods of Random Forest and Gradient Boosting outperform other approaches. The performance of ML methods is quantified through the F1 score and computation of sensitivities and specificities. When using all the six measurements, it is found that maximum F1 scores larger than 0.9 are achieved for CAS and PAD, larger than 0.85 for SAS, and larger than 0.98 for both low- and high-severity AAAs. Corresponding sensitivities and specificities are larger than 90\% for CAS and PAD, larger than 85\% for SAS, and larger than 98\% for both low- and high-severity AAAs. When reducing the number of measurements, it is found that the performance is degraded by less than 5\% when three measurements are used, and less than 10\% when only two measurements are used for classification. For AAA, it is shown that F1 scores larger than 0.85 and corresponding sensitivities and specificities larger than 85\% are achievable when using only a single measurement. The results are encouraging to pursue AAA monitoring and screening through wearable devices which can reliably measure pressure or flow-rates }
}

@article{210305267v1,
  title={ Memory-Efficient, Limb Position-Aware Hand Gesture Recognition using   Hyperdimensional Computing },
  author={ Andy Zhou and Rikky Muller and Jan Rabaey },
  journal={ arXiv preprint arXiv:2103.05267v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2103.05267v1 },
  abstract={ Electromyogram (EMG) pattern recognition can be used to classify hand gestures and movements for human-machine interface and prosthetics applications, but it often faces reliability issues resulting from limb position change. One method to address this is dual-stage classification, in which the limb position is first determined using additional sensors to select between multiple position-specific gesture classifiers. While improving performance, this also increases model complexity and memory footprint, making a dual-stage classifier difficult to implement in a wearable device with limited resources. In this paper, we present sensor fusion of accelerometer and EMG signals using a hyperdimensional computing model to emulate dual-stage classification in a memory-efficient way. We demonstrate two methods of encoding accelerometer features to act as keys for retrieval of position-specific parameters from multiple models stored in superposition. Through validation on a dataset of 13 gestures in 8 limb positions, we obtain a classification accuracy of up to 93.34\%, an improvement of 17.79\% over using a model trained solely on EMG. We achieve this while only marginally increasing memory footprint over a single limb position model, requiring \$8\\textbackslash{}times\$ less memory than a traditional dual-stage classification architecture. }
}

@article{210304279v1,
  title={ Hierarchical Self Attention Based Autoencoder for Open-Set Human   Activity Recognition },
  author={ M Tanjid Hasan Tonmoy and Saif Mahmud and A K M Mahbubur Rahman and M Ashraful Amin and Amin Ahsan Ali },
  journal={ arXiv preprint arXiv:2103.04279v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2103.04279v1 },
  abstract={ Wearable sensor based human activity recognition is a challenging problem due to difficulty in modeling spatial and temporal dependencies of sensor signals. Recognition models in closed-set assumption are forced to yield members of known activity classes as prediction. However, activity recognition models can encounter an unseen activity due to body-worn sensor malfunction or disability of the subject performing the activities. This problem can be addressed through modeling solution according to the assumption of open-set recognition. Hence, the proposed self attention based approach combines data hierarchically from different sensor placements across time to classify closed-set activities and it obtains notable performance improvement over state-of-the-art models on five publicly available datasets. The decoder in this autoencoder architecture incorporates self-attention based feature representations from encoder to detect unseen activity classes in open-set recognition setting. Furthermore, attention maps generated by the hierarchical model demonstrate explainable selection of features in activity recognition. We conduct extensive leave one subject out validation experiments that indicate significantly improved robustness to noise and subject specific variability in body-worn sensor signals. The source code is available at: github.com/saif-mahmud/hierarchical-attention-HAR }
}

@article{210105028v2,
  title={ I Can See it in Your Eyes: Gaze as an Implicit Cue of Uncanniness and   Task Performance in Repeated Interactions },
  author={ Giulia Perugia and Maike Paetzel-Prüsmann and Madelene Alanenpää and Ginevra Castellano },
  journal={ arXiv preprint arXiv:2101.05028v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2101.05028v2 },
  abstract={ Over the past years, extensive research has been dedicated to developing robust platforms and data-driven dialog models to support long-term human-robot interactions. However, little is known about how people's perception of robots and engagement with them develop over time and how these can be accurately assessed through implicit and continuous measurement techniques. In this paper, we explore this by involving participants in three interaction sessions with multiple days of zero exposure in between. Each session consists of a joint task with a robot as well as two short social chats with it before and after the task. We measure participants' gaze patterns with a wearable eye-tracker and gauge their perception of the robot and engagement with it and the joint task using questionnaires. Results disclose that aversion of gaze in a social chat is an indicator of a robot's uncanniness and that the more people gaze at the robot in a joint task, the worse they perform. In contrast with most HRI literature, our results show that gaze towards an object of shared attention, rather than gaze towards a robotic partner, is the most meaningful predictor of engagement in a joint task. Furthermore, the analyses of gaze patterns in repeated interactions disclose that people's mutual gaze in a social chat develops congruently with their perceptions of the robot over time. These are key findings for the HRI community as they entail that gaze behavior can be used as an implicit measure of people's perception of robots in a social chat and of their engagement and task performance in a joint task. }
}

@article{210206882v1,
  title={ Saliency-Aware Class-Agnostic Food Image Segmentation },
  author={ Sri Kalyan Yarlagadda and Daniel Mas Montserrat and David Guerra and Carol J. Boushey and Deborah A. Kerr and Fengqing Zhu },
  journal={ arXiv preprint arXiv:2102.06882v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2102.06882v1 },
  abstract={ Advances in image-based dietary assessment methods have allowed nutrition professionals and researchers to improve the accuracy of dietary assessment, where images of food consumed are captured using smartphones or wearable devices. These images are then analyzed using computer vision methods to estimate energy and nutrition content of the foods. Food image segmentation, which determines the regions in an image where foods are located, plays an important role in this process. Current methods are data dependent, thus cannot generalize well for different food types. To address this problem, we propose a class-agnostic food image segmentation method. Our method uses a pair of eating scene images, one before start eating and one after eating is completed. Using information from both the before and after eating images, we can segment food images by finding the salient missing objects without any prior information about the food class. We model a paradigm of top down saliency which guides the attention of the human visual system (HVS) based on a task to find the salient missing objects in a pair of images. Our method is validated on food images collected from a dietary study which showed promising results. }
}

@article{210206073v1,
  title={ SelfHAR: Improving Human Activity Recognition through Self-training with   Unlabeled Data },
  author={ Chi Ian Tang and Ignacio Perez-Pozuelo and Dimitris Spathis and Soren Brage and Nick Wareham and Cecilia Mascolo },
  journal={ arXiv preprint arXiv:2102.06073v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2102.06073v1 },
  abstract={ Machine learning and deep learning have shown great promise in mobile sensing applications, including Human Activity Recognition. However, the performance of such models in real-world settings largely depends on the availability of large datasets that captures diverse behaviors. Recently, studies in computer vision and natural language processing have shown that leveraging massive amounts of unlabeled data enables performance on par with state-of-the-art supervised models.   In this work, we present SelfHAR, a semi-supervised model that effectively learns to leverage unlabeled mobile sensing datasets to complement small labeled datasets. Our approach combines teacher-student self-training, which distills the knowledge of unlabeled and labeled datasets while allowing for data augmentation, and multi-task self-supervision, which learns robust signal-level representations by predicting distorted versions of the input.   We evaluated SelfHAR on various HAR datasets and showed state-of-the-art performance over supervised and previous semi-supervised approaches, with up to 12\% increase in F1 score using the same number of model parameters at inference. Furthermore, SelfHAR is data-efficient, reaching similar performance using up to 10 times less labeled data compared to supervised approaches. Our work not only achieves state-of-the-art performance in a diverse set of HAR datasets, but also sheds light on how pre-training tasks may affect downstream performance. }
}

@article{200201791v2,
  title={ Force-guided High-precision Grasping Control of Fragile and Deformable   Objects using sEMG-based Force Prediction },
  author={ Ruoshi Wen and Kai Yuan and Qiang Wang and Shuai Heng and Zhibin Li },
  journal={ arXiv preprint arXiv:2002.01791v2 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2002.01791v2 },
  abstract={ Regulating contact forces with high precision is crucial for grasping and manipulating fragile or deformable objects. We aim to utilize the dexterity of human hands to regulate the contact forces for robotic hands and exploit human sensory-motor synergies in a wearable and non-invasive way. We extracted force information from the electric activities of skeletal muscles during their voluntary contractions through surface electromyography (sEMG). We built a regression model based on a Neural Network to predict the gripping force from the preprocessed sEMG signals and achieved high accuracy (R2 = 0.982). Based on the force command predicted from human muscles, we developed a force-guided control framework, where force control was realized via an admittance controller that tracked the predicted gripping force reference to grasp delicate and deformable objects. We demonstrated the effectiveness of the proposed method on a set of representative fragile and deformable objects from daily life, all of which were successfully grasped without any damage or deformation. }
}

@article{210111800v1,
  title={ AdaSpring: Context-adaptive and Runtime-evolutionary Deep Model   Compression for Mobile Applications },
  author={ Sicong Liu and Bin Guo and Ke Ma and Zhiwen Yu and Junzhao Du },
  journal={ arXiv preprint arXiv:2101.11800v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2101.11800v1 },
  abstract={ There are many deep learning (e.g., DNN) powered mobile and wearable applications today continuously and unobtrusively sensing the ambient surroundings to enhance all aspects of human lives. To enable robust and private mobile sensing, DNN tends to be deployed locally on the resource-constrained mobile devices via model compression. The current practice either hand-crafted DNN compression techniques, i.e., for optimizing DNN-relative performance (e.g., parameter size), or on-demand DNN compression methods, i.e., for optimizing hardware-dependent metrics (e.g., latency), cannot be locally online because they require offline retraining to ensure accuracy. Also, none of them have correlated their efforts with runtime adaptive compression to consider the dynamic nature of the deployment context of mobile applications. To address those challenges, we present AdaSpring, a context-adaptive and self-evolutionary DNN compression framework. It enables the runtime adaptive DNN compression locally online. Specifically, it presents the ensemble training of a retraining-free and self-evolutionary network to integrate multiple alternative DNN compression configurations (i.e., compressed architectures and weights). It then introduces the runtime search strategy to quickly search for the most suitable compression configurations and evolve the corresponding weights. With evaluation on five tasks across three platforms and a real-world case study, experiment outcomes show that AdaSpring obtains up to 3.1x latency reduction, 4.2 x energy efficiency improvement in DNNs, compared to hand-crafted compression techniques, while only incurring <= 6.2ms runtime-evolution latency. }
}

@article{200512178v2,
  title={ Incremental Real-Time Personalization in Human Activity Recognition   Using Domain Adaptive Batch Normalization },
  author={ Alan Mazankiewicz and Klemens Böhm and Mario Bergés },
  journal={ arXiv preprint arXiv:2005.12178v2 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2005.12178v2 },
  abstract={ Human Activity Recognition (HAR) from devices like smartphone accelerometers is a fundamental problem in ubiquitous computing. Machine learning based recognition models often perform poorly when applied to new users that were not part of the training data. Previous work has addressed this challenge by personalizing general recognition models to the unique motion pattern of a new user in a static batch setting. They require target user data to be available upfront. The more challenging online setting has received less attention. No samples from the target user are available in advance, but they arrive sequentially. Additionally, the motion pattern of users may change over time. Thus, adapting to new and forgetting old information must be traded off. Finally, the target user should not have to do any work to use the recognition system by, say, labeling any activities. Our work addresses all of these challenges by proposing an unsupervised online domain adaptation algorithm. Both classification and personalization happen continuously and incrementally in real time. Our solution works by aligning the feature distributions of all subjects, be they sources or the target, in hidden neural network layers. To this end, we normalize the input of a layer with user-specific mean and variance statistics. During training, these statistics are computed over user-specific batches. In the online phase, they are estimated incrementally for any new target user. }
}

@article{201006584v2,
  title={ Jointly Optimizing Sensing Pipelines for Multimodal Mixed Reality   Interaction },
  author={ Darshana Rathnayake and Ashen de Silva and Dasun Puwakdandawa and Lakmal Meegahapola and Archan Misra and Indika Perera },
  journal={ arXiv preprint arXiv:2010.06584v2 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2010.06584v2 },
  abstract={ Natural human interactions for Mixed Reality Applications are overwhelmingly multimodal: humans communicate intent and instructions via a combination of visual, aural and gestural cues. However, supporting low-latency and accurate comprehension of such multimodal instructions (MMI), on resource-constrained wearable devices, remains an open challenge, especially as the state-of-the-art comprehension techniques for each individual modality increasingly utilize complex Deep Neural Network models. We demonstrate the possibility of overcoming the core limitation of latency--vs.--accuracy tradeoff by exploiting cross-modal dependencies -- i.e., by compensating for the inferior performance of one model with an increased accuracy of more complex model of a different modality. We present a sensor fusion architecture that performs MMI comprehension in a quasi-synchronous fashion, by fusing visual, speech and gestural input. The architecture is reconfigurable and supports dynamic modification of the complexity of the data processing pipeline for each individual modality in response to contextual changes. Using a representative ''classroom'' context and a set of four common interaction primitives, we then demonstrate how the choices between low and high complexity models for each individual modality are coupled. In particular, we show that (a) a judicious combination of low and high complexity models across modalities can offer a dramatic 3-fold decrease in comprehension latency together with an increase 10-15\% in accuracy, and (b) the right collective choice of models is context dependent, with the performance of some model combinations being significantly more sensitive to changes in scene context or choice of interaction. }
}

@article{200500698v2,
  title={ Deep ConvLSTM with self-attention for human activity decoding using   wearables },
  author={ Satya P. Singh and Aimé Lay-Ekuakille and Deepak Gangwar and Madan Kumar Sharma and Sukrit Gupta },
  journal={ arXiv preprint arXiv:2005.00698v2 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2005.00698v2 },
  abstract={ Decoding human activity accurately from wearable sensors can aid in applications related to healthcare and context awareness. The present approaches in this domain use recurrent and/or convolutional models to capture the spatio-temporal features from time-series data from multiple sensors. We propose a deep neural network architecture that not only captures the spatio-temporal features of multiple sensor time-series data but also selects, learns important time points by utilizing a self-attention mechanism. We show the validity of the proposed approach across different data sampling strategies on six public datasets and demonstrate that the self-attention mechanism gave a significant improvement in performance over deep networks using a combination of recurrent and convolution networks. We also show that the proposed approach gave a statistically significant performance enhancement over previous state-of-the-art methods for the tested datasets. The proposed methods open avenues for better decoding of human activity from multiple body sensors over extended periods of time. The code implementation for the proposed model is available at https://github.com/isukrit/encodingHumanActivity. }
}

@article{201207963v1,
  title={ Invariant Feature Learning for Sensor-based Human Activity Recognition },
  author={ Yujiao Hao and Boyu Wang and Rong Zheng },
  journal={ arXiv preprint arXiv:2012.07963v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2012.07963v1 },
  abstract={ Wearable sensor-based human activity recognition (HAR) has been a research focus in the field of ubiquitous and mobile computing for years. In recent years, many deep models have been applied to HAR problems. However, deep learning methods typically require a large amount of data for models to generalize well. Significant variances caused by different participants or diverse sensor devices limit the direct application of a pre-trained model to a subject or device that has not been seen before. To address these problems, we present an invariant feature learning framework (IFLF) that extracts common information shared across subjects and devices. IFLF incorporates two learning paradigms: 1) meta-learning to capture robust features across seen domains and adapt to an unseen one with similarity-based data selection; 2) multi-task learning to deal with data shortage and enhance overall performance via knowledge sharing among different subjects. Experiments demonstrated that IFLF is effective in handling both subject and device diversion across popular open datasets and an in-house dataset. It outperforms a baseline model of up to 40\% in test accuracy. }
}

@article{201111600v1,
  title={ Yet it moves: Learning from Generic Motions to Generate IMU data from   YouTube videos },
  author={ Vitor Fortes Rey and Kamalveer Kaur Garewal and Paul Lukowicz },
  journal={ arXiv preprint arXiv:2011.11600v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2011.11600v1 },
  abstract={ Human activity recognition (HAR) using wearable sensors has benefited much less from recent advances in Machine Learning than fields such as computer vision and natural language processing. This is to a large extent due to the lack of large scale repositories of labeled training data. In our research we aim to facilitate the use of online videos, which exists in ample quantity for most activities and are much easier to label than sensor data, to simulate labeled wearable motion sensor data. In previous work we already demonstrate some preliminary results in this direction focusing on very simple, activity specific simulation models and a single sensor modality (acceleration norm)\\textbackslash{}cite\{10.1145/3341162.3345590\}. In this paper we show how we can train a regression model on generic motions for both accelerometer and gyro signals and then apply it to videos of the target activities to generate synthetic IMU data (acceleration and gyro norms) that can be used to train and/or improve HAR models. We demonstrate that systems trained on simulated data generated by our regression model can come to within around 10\% of the mean F1 score of a system trained on real sensor data. Furthermore we show that by either including a small amount of real sensor data for model calibration or simply leveraging the fact that (in general) we can easily generate much more simulated data from video than we can collect in terms of real sensor data the advantage of real sensor data can be eventually equalized. }
}

@article{201111552v1,
  title={ MoGaze: A Dataset of Full-Body Motions that Includes Workspace Geometry   and Eye-Gaze },
  author={ Philipp Kratzer and Simon Bihlmaier and Niteesh Balachandra Midlagajni and Rohit Prakash and Marc Toussaint and Jim Mainprice },
  journal={ arXiv preprint arXiv:2011.11552v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2011.11552v1 },
  abstract={ As robots become more present in open human environments, it will become crucial for robotic systems to understand and predict human motion. Such capabilities depend heavily on the quality and availability of motion capture data. However, existing datasets of full-body motion rarely include 1) long sequences of manipulation tasks, 2) the 3D model of the workspace geometry, and 3) eye-gaze, which are all important when a robot needs to predict the movements of humans in close proximity. Hence, in this paper, we present a novel dataset of full-body motion for everyday manipulation tasks, which includes the above. The motion data was captured using a traditional motion capture system based on reflective markers. We additionally captured eye-gaze using a wearable pupil-tracking device. As we show in experiments, the dataset can be used for the design and evaluation of full-body motion prediction algorithms. Furthermore, our experiments show eye-gaze as a powerful predictor of human intent. The dataset includes 180 min of motion capture data with 1627 pick and place actions being performed. It is available at https://humans-to-robots-motion.github.io/mogaze and is planned to be extended to collaborative tasks with two humans in the near future. }
}

@article{201112137v1,
  title={ Self-Supervised Transformers for Activity Classification using Ambient   Sensors },
  author={ Luke Hicks and Ariel Ruiz-Garcia and Vasile Palade and Ibrahim Almakky },
  journal={ arXiv preprint arXiv:2011.12137v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2011.12137v1 },
  abstract={ Providing care for ageing populations is an onerous task, and as life expectancy estimates continue to rise, the number of people that require senior care is growing rapidly. This paper proposes a methodology based on Transformer Neural Networks to classify the activities of a resident within an ambient sensor based environment. We also propose a methodology to pre-train Transformers in a self-supervised manner, as a hybrid autoencoder-classifier model instead of using contrastive loss. The social impact of the research is considered with wider benefits of the approach and next steps for identifying transitions in human behaviour. In recent years there has been an increasing drive for integrating sensor based technologies within care facilities for data collection. This allows for employing machine learning for many aspects including activity recognition and anomaly detection. Due to the sensitivity of healthcare environments, some methods of data collection used in current research are considered to be intrusive within the senior care industry, including cameras for image based activity recognition, and wearables for activity tracking, but recent studies have shown that using these methods commonly result in poor data quality due to the lack of resident interest in participating in data gathering. This has led to a focus on ambient sensors, such as binary PIR motion, connected domestic appliances, and electricity and water metering. By having consistency in ambient data collection, the quality of data is considerably more reliable, presenting the opportunity to perform classification with enhanced accuracy. Therefore, in this research we looked to find an optimal way of using deep learning to classify human activity with ambient sensor data. }
}

@article{201100300v1,
  title={ In-The-Wild Interference Characterization and Modelling for   Electro-Quasistatic-HBC with Miniaturized Wearables },
  author={ Parikha Mehrotra and David Yang and Scott Weigand and Shreyas Sen },
  journal={ arXiv preprint arXiv:2011.00300v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2011.00300v1 },
  abstract={ The emergence of Human Body Communication (HBC) as an alternative to wireless body area networks (WBAN) has led to the development of small sized, energy efficient and more secure wearable and implantable devices forming a network in and around the body. Previous studies claim that though HBC is comparatively more secure than WBAN, nevertheless, the electromagnetic (EM) radiative nature of HBC in >10MHz region makes the information susceptible to eavesdropping. Furthermore, interferences may be picked up by the body due to the human body antenna effect in the 40-400MHz range. Alternatively, electro-quasistatic (EQS) mode of HBC forms an attractive way for covert data transmission in the sub 10MHz region by allowing the signal to be contained within the body. However, there is a gap in the knowledge about the mechanism and sources of interference in this region (crucial in allowing for proper choice of data transmission band). In this paper, the interference coupling modality in the EQS region is explained along with its possible sources. Interferences seen by the wearable in the actual scenario is a non-trivial problem and a suitable measurement EQS HBC setup is designed to recreate it by employing a wearable sized measurement setup having a small ground plane. For the first time, a human biophysical interference pickup model is proposed and interference measurement results using a wearable device are presented up to 250kHz in different environmental settings. }
}

@article{201005654v1,
  title={ The MECCANO Dataset: Understanding Human-Object Interactions from   Egocentric Videos in an Industrial-like Domain },
  author={ Francesco Ragusa and Antonino Furnari and Salvatore Livatino and Giovanni Maria Farinella },
  journal={ arXiv preprint arXiv:2010.05654v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2010.05654v1 },
  abstract={ Wearable cameras allow to collect images and videos of humans interacting with the world. While human-object interactions have been thoroughly investigated in third person vision, the problem has been understudied in egocentric settings and in industrial scenarios. To fill this gap, we introduce MECCANO, the first dataset of egocentric videos to study human-object interactions in industrial-like settings. MECCANO has been acquired by 20 participants who were asked to build a motorbike model, for which they had to interact with tiny objects and tools. The dataset has been explicitly labeled for the task of recognizing human-object interactions from an egocentric perspective. Specifically, each interaction has been labeled both temporally (with action segments) and spatially (with active object bounding boxes). With the proposed dataset, we investigate four different tasks including 1) action recognition, 2) active object detection, 3) active object recognition and 4) egocentric human-object interaction detection, which is a revisited version of the standard human-object interaction detection task. Baseline results show that the MECCANO dataset is a challenging benchmark to study egocentric human-object interactions in industrial-like scenarios. We publicy release the dataset at https://iplab.dmi.unict.it/MECCANO. }
}

@article{200909404v2,
  title={ MARS: Mixed Virtual and Real Wearable Sensors for Human Activity   Recognition with Multi-Domain Deep Learning Model },
  author={ Ling Pei and Songpengcheng Xia and Lei Chu and Fanyi Xiao and Qi Wu and Wenxian Yu and Robert Qiu },
  journal={ arXiv preprint arXiv:2009.09404v2 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2009.09404v2 },
  abstract={ Together with the rapid development of the Internet of Things (IoT), human activity recognition (HAR) using wearable Inertial Measurement Units (IMUs) becomes a promising technology for many research areas. Recently, deep learning-based methods pave a new way of understanding and performing analysis of the complex data in the HAR system. However, the performance of these methods is mostly based on the quality and quantity of the collected data. In this paper, we innovatively propose to build a large database based on virtual IMUs and then address technical issues by introducing a multiple-domain deep learning framework consisting of three technical parts. In the first part, we propose to learn the single-frame human activity from the noisy IMU data with hybrid convolutional neural networks (CNNs) in the semi-supervised form. For the second part, the extracted data features are fused according to the principle of uncertainty-aware consistency, which reduces the uncertainty by weighting the importance of the features. The transfer learning is performed in the last part based on the newly released Archive of Motion Capture as Surface Shapes (AMASS) dataset, containing abundant synthetic human poses, which enhances the variety and diversity of the training dataset and is beneficial for the process of training and feature transfer in the proposed method. The efficiency and effectiveness of the proposed method have been demonstrated in the real deep inertial poser (DIP) dataset. The experimental results show that the proposed methods can surprisingly converge within a few iterations and outperform all competing methods. }
}

@article{200908033v1,
  title={ Voice Controlled Upper Body Exoskeleton: A Development For Industrial   Application },
  author={ Shivam Tripathy and Rohan Panicker and Shubh Shrey and Rutvik Naik and S S Pachpore },
  journal={ arXiv preprint arXiv:2009.08033v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2009.08033v1 },
  abstract={ An exoskeleton is a wearable electromechanical structure that is intended to resemble and allow movements in a manner similar to the human skeletal system. They can be used by both disabled and able people alike to increase physical strength in carrying out tasks that would be otherwise difficult, or as a rehabilitation device to aid in physiotherapeutic activities of a weakened body part. This paper intends to introduce a voicecontrolled upper body exoskeleton for industrial applications which can aid workers wearing it by reducing stresses on their arms and shoulders over longer periods and add up to 20kg more strength in lifting applications. The 3D design, calculations and considerations, and load analysis are presented along with brief results of a basic prototype model of the exoskeleton. }
}

@article{200810726v1,
  title={ Unsupervised Multi-Modal Representation Learning for Affective Computing   with Multi-Corpus Wearable Data },
  author={ Kyle Ross and Paul Hungler and Ali Etemad },
  journal={ arXiv preprint arXiv:2008.10726v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2008.10726v1 },
  abstract={ With recent developments in smart technologies, there has been a growing focus on the use of artificial intelligence and machine learning for affective computing to further enhance the user experience through emotion recognition. Typically, machine learning models used for affective computing are trained using manually extracted features from biological signals. Such features may not generalize well for large datasets and may be sub-optimal in capturing the information from the raw input data. One approach to address this issue is to use fully supervised deep learning methods to learn latent representations of the biosignals. However, this method requires human supervision to label the data, which may be unavailable or difficult to obtain. In this work we propose an unsupervised framework reduce the reliance on human supervision. The proposed framework utilizes two stacked convolutional autoencoders to learn latent representations from wearable electrocardiogram (ECG) and electrodermal activity (EDA) signals. These representations are utilized within a random forest model for binary arousal classification. This approach reduces human supervision and enables the aggregation of datasets allowing for higher generalizability. To validate this framework, an aggregated dataset comprised of the AMIGOS, ASCERTAIN, CLEAS, and MAHNOB-HCI datasets is created. The results of our proposed method are compared with using convolutional neural networks, as well as methods that employ manual extraction of hand-crafted features. The methodology used for fusing the two modalities is also investigated. Lastly, we show that our method outperforms current state-of-the-art results that have performed arousal detection on the same datasets using ECG and EDA biosignals. The results show the wide-spread applicability for stacked convolutional autoencoders to be used with machine learning for affective computing. }
}

@article{200807092v1,
  title={ Understanding Brain Dynamics for Color Perception using Wearable EEG   headband },
  author={ Mahima Chaudhary and Sumona Mukhopadhyay and Marin Litoiu and Lauren E Sergio and Meaghan S Adams },
  journal={ arXiv preprint arXiv:2008.07092v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2008.07092v1 },
  abstract={ The perception of color is an important cognitive feature of the human brain. The variety of colors that impinge upon the human eye can trigger changes in brain activity which can be captured using electroencephalography (EEG). In this work, we have designed a multiclass classification model to detect the primary colors from the features of raw EEG signals. In contrast to previous research, our method employs spectral power features, statistical features as well as correlation features from the signal band power obtained from continuous Morlet wavelet transform instead of raw EEG, for the classification task. We have applied dimensionality reduction techniques such as Forward Feature Selection and Stacked Autoencoders to reduce the dimension of data eventually increasing the model's efficiency. Our proposed methodology using Forward Selection and Random Forest Classifier gave the best overall accuracy of 80.6\\textbackslash{}\% for intra-subject classification. Our approach shows promise in developing techniques for cognitive tasks using color cues such as controlling Internet of Thing (IoT) devices by looking at primary colors for individuals with restricted motor abilities. }
}

@article{200802567v1,
  title={ An Intelligent Non-Invasive Real Time Human Activity Recognition System   for Next-Generation Healthcare },
  author={ William Taylor and Syed Aziz Shah and Kia Dashtipour and Adnan Zahid and Qammer H. Abbasi and Muhammad Ali Imran },
  journal={ arXiv preprint arXiv:2008.02567v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2008.02567v1 },
  abstract={ Human motion detection is getting considerable attention in the field of Artificial Intelligence (AI) driven healthcare systems. Human motion can be used to provide remote healthcare solutions for vulnerable people by identifying particular movements such as falls, gait and breathing disorders. This can allow people to live more independent lifestyles and still have the safety of being monitored if more direct care is needed. At present wearable devices can provide real time monitoring by deploying equipment on a person's body. However, putting devices on a person's body all the time make it uncomfortable and the elderly tends to forget it to wear as well in addition to the insecurity of being tracked all the time. This paper demonstrates how human motions can be detected in quasi-real-time scenario using a non-invasive method. Patterns in the wireless signals presents particular human body motions as each movement induces a unique change in the wireless medium. These changes can be used to identify particular body motions. This work produces a dataset that contains patterns of radio wave signals obtained using software defined radios (SDRs) to establish if a subject is standing up or sitting down as a test case. The dataset was used to create a machine learning model, which was used in a developed application to provide a quasi-real-time classification of standing or sitting state. The machine learning model was able to achieve 96.70 \% accuracy using the Random Forest algorithm using 10 fold cross validation. A benchmark dataset of wearable devices was compared to the proposed dataset and results showed the proposed dataset to have similar accuracy of nearly 90 \%. The machine learning models developed in this paper are tested for two activities but the developed system is designed and applicable for detecting and differentiating x number of activities. }
}

@article{200803230v1,
  title={ ESPRESSO: Entropy and ShaPe awaRe timE-Series SegmentatiOn for   processing heterogeneous sensor data },
  author={ Shohreh Deldari and Daniel V. Smith and Amin Sadri and Flora D. Salim },
  journal={ arXiv preprint arXiv:2008.03230v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2008.03230v1 },
  abstract={ Extracting informative and meaningful temporal segments from high-dimensional wearable sensor data, smart devices, or IoT data is a vital preprocessing step in applications such as Human Activity Recognition (HAR), trajectory prediction, gesture recognition, and lifelogging. In this paper, we propose ESPRESSO (Entropy and ShaPe awaRe timE-Series SegmentatiOn), a hybrid segmentation model for multi-dimensional time-series that is formulated to exploit the entropy and temporal shape properties of time-series. ESPRESSO differs from existing methods that focus upon particular statistical or temporal properties of time-series exclusively. As part of model development, a novel temporal representation of time-series \$WCAC\$ was introduced along with a greedy search approach that estimate segments based upon the entropy metric. ESPRESSO was shown to offer superior performance to four state-of-the-art methods across seven public datasets of wearable and wear-free sensing. In addition, we undertake a deeper investigation of these datasets to understand how ESPRESSO and its constituent methods perform with respect to different dataset characteristics. Finally, we provide two interesting case-studies to show how applying ESPRESSO can assist in inferring daily activity routines and the emotional state of humans. }
}

@article{200707172v1,
  title={ Attend And Discriminate: Beyond the State-of-the-Art for Human Activity   Recognition using Wearable Sensors },
  author={ Alireza Abedin and Mahsa Ehsanpour and Qinfeng Shi and Hamid Rezatofighi and Damith C. Ranasinghe },
  journal={ arXiv preprint arXiv:2007.07172v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2007.07172v1 },
  abstract={ Wearables are fundamental to improving our understanding of human activities, especially for an increasing number of healthcare applications from rehabilitation to fine-grained gait analysis. Although our collective know-how to solve Human Activity Recognition (HAR) problems with wearables has progressed immensely with end-to-end deep learning paradigms, several fundamental opportunities remain overlooked. We rigorously explore these new opportunities to learn enriched and highly discriminating activity representations. We propose: i) learning to exploit the latent relationships between multi-channel sensor modalities and specific activities; ii) investigating the effectiveness of data-agnostic augmentation for multi-modal sensor data streams to regularize deep HAR models; and iii) incorporating a classification loss criterion to encourage minimal intra-class representation differences whilst maximising inter-class differences to achieve more discriminative features. Our contributions achieves new state-of-the-art performance on four diverse activity recognition problem benchmarks with large margins -- with up to 6\% relative margin improvement. We extensively validate the contributions from our design concepts through extensive experiments, including activity misalignment measures, ablation studies and insights shared through both quantitative and qualitative studies. }
}

@article{200614248v1,
  title={ Background Knowledge Injection for Interpretable Sequence Classification },
  author={ Severin Gsponer and Luca Costabello and Chan Le Van and Sumit Pai and Christophe Gueret and Georgiana Ifrim and Freddy Lecue },
  journal={ arXiv preprint arXiv:2006.14248v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2006.14248v1 },
  abstract={ Sequence classification is the supervised learning task of building models that predict class labels of unseen sequences of symbols. Although accuracy is paramount, in certain scenarios interpretability is a must. Unfortunately, such trade-off is often hard to achieve since we lack human-independent interpretability metrics. We introduce a novel sequence learning algorithm, that combines (i) linear classifiers - which are known to strike a good balance between predictive power and interpretability, and (ii) background knowledge embeddings. We extend the classic subsequence feature space with groups of symbols which are generated by background knowledge injected via word or graph embeddings, and use this new feature space to learn a linear classifier. We also present a new measure to evaluate the interpretability of a set of symbolic features based on the symbol embeddings. Experiments on human activity recognition from wearables and amino acid sequence classification show that our classification approach preserves predictive power, while delivering more interpretable models. }
}

@article{200408297v3,
  title={ Towards data-driven stroke rehabilitation via wearable sensors and deep   learning },
  author={ Aakash Kaku and Avinash Parnandi and Anita Venkatesan and Natasha Pandit and Heidi Schambra and Carlos Fernandez-Granda },
  journal={ arXiv preprint arXiv:2004.08297v3 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2004.08297v3 },
  abstract={ Recovery after stroke is often incomplete, but rehabilitation training may potentiate recovery by engaging endogenous neuroplasticity. In preclinical models of stroke, high doses of rehabilitation training are required to restore functional movement to the affected limbs of animals. In humans, however, the necessary dose of training to potentiate recovery is not known. This ignorance stems from the lack of objective, pragmatic approaches for measuring training doses in rehabilitation activities. Here, to develop a measurement approach, we took the critical first step of automatically identifying functional primitives, the basic building block of activities. Forty-eight individuals with chronic stroke performed a variety of rehabilitation activities while wearing inertial measurement units (IMUs) to capture upper body motion. Primitives were identified by human labelers, who labeled and segmented the associated IMU data. We performed automatic classification of these primitives using machine learning. We designed a convolutional neural network model that outperformed existing methods. The model includes an initial module to compute separate embeddings of different physical quantities in the sensor data. In addition, it replaces batch normalization (which performs normalization based on statistics computed from the training data) with instance normalization (which uses statistics computed from the test data). This increases robustness to possible distributional shifts when applying the method to new patients. With this approach, we attained an average classification accuracy of 70\%. Thus, using a combination of IMU-based motion capture and deep learning, we were able to identify primitives automatically. This approach builds towards objectively-measured rehabilitation training, enabling the identification and counting of functional primitives that accrues to a training dose. }
}

@article{200603259v2,
  title={ Real-time Human Activity Recognition Using Conditionally Parametrized   Convolutions on Mobile and Wearable Devices },
  author={ Xin Cheng and Lei Zhang and Yin Tang and Yue Liu and Hao Wu and Jun He },
  journal={ arXiv preprint arXiv:2006.03259v2 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2006.03259v2 },
  abstract={ Recently, deep learning has represented an important research trend in human activity recognition (HAR). In particular, deep convolutional neural networks (CNNs) have achieved state-of-the-art performance on various HAR datasets. For deep learning, improvements in performance have to heavily rely on increasing model size or capacity to scale to larger and larger datasets, which inevitably leads to the increase of operations. A high number of operations in deep leaning increases computational cost and is not suitable for real-time HAR using mobile and wearable sensors. Though shallow learning techniques often are lightweight, they could not achieve good performance. Therefore, deep learning methods that can balance the trade-off between accuracy and computation cost is highly needed, which to our knowledge has seldom been researched. In this paper, we for the first time propose a computation efficient CNN using conditionally parametrized convolution for real-time HAR on mobile and wearable devices. We evaluate the proposed method on four public benchmark HAR datasets consisting of WISDM dataset, PAMAP2 dataset, UNIMIB-SHAR dataset, and OPPORTUNITY dataset, achieving state-of-the-art accuracy without compromising computation cost. Various ablation experiments are performed to show how such a network with large capacity is clearly preferable to baseline while requiring a similar amount of operations. The method can be used as a drop-in replacement for the existing deep HAR architectures and easily deployed onto mobile and wearable devices for real-time HAR applications. }
}

@article{200409215v1,
  title={ CatNet: Class Incremental 3D ConvNets for Lifelong Egocentric Gesture   Recognition },
  author={ Zhengwei Wang and Qi She and Tejo Chalasani and Aljosa Smolic },
  journal={ arXiv preprint arXiv:2004.09215v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2004.09215v1 },
  abstract={ Egocentric gestures are the most natural form of communication for humans to interact with wearable devices such as VR/AR helmets and glasses. A major issue in such scenarios for real-world applications is that may easily become necessary to add new gestures to the system e.g., a proper VR system should allow users to customize gestures incrementally. Traditional deep learning methods require storing all previous class samples in the system and training the model again from scratch by incorporating previous samples and new samples, which costs humongous memory and significantly increases computation over time. In this work, we demonstrate a lifelong 3D convolutional framework -- c(C)la(a)ss increment(t)al net(Net)work (CatNet), which considers temporal information in videos and enables lifelong learning for egocentric gesture video recognition by learning the feature representation of an exemplar set selected from previous class samples. Importantly, we propose a two-stream CatNet, which deploys RGB and depth modalities to train two separate networks. We evaluate CatNets on a publicly available dataset -- EgoGesture dataset, and show that CatNets can learn many classes incrementally over a long period of time. Results also demonstrate that the two-stream architecture achieves the best performance on both joint training and class incremental training compared to 3 other one-stream architectures. The codes and pre-trained models used in this work are provided at https://github.com/villawang/CatNet. }
}

@article{200409376v1,
  title={ Conditional-UNet: A Condition-aware Deep Model for Coherent Human   Activity Recognition From Wearables },
  author={ Liming Zhang },
  journal={ arXiv preprint arXiv:2004.09376v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2004.09376v1 },
  abstract={ Recognizing human activities from multi-channel time series data collected from wearable sensors is ever more practical. However, in real-world conditions, coherent activities and body movements could happen at the same time, like moving head during walking or sitting. A new problem, so-called ''Coherent Human Activity Recognition (Co-HAR)'', is more complicated than normal multi-class classification tasks since signals of different movements are mixed and interfered with each other. On the other side, we consider such Co-HAR as a dense labelling problem that classify each sample on a time step with a label to provide high-fidelity and duration-varied support to applications. In this paper, a novel condition-aware deep architecture ''Conditional-UNet'' is developed to allow dense labeling for Co-HAR problem. We also contribute a first-of-its-kind Co-HAR dataset for head movement recognition under walk or sit condition for future research. Experiments on head gesture recognition show that our model achieve overall 2\%-3\% performance gain of F1 score over existing state-of-the-art deep methods, and more importantly, systematic and comprehensive improvements on real head gesture classes. }
}

@article{200311163v2,
  title={ Fusing Wearable IMUs with Multi-View Images for Human Pose Estimation: A   Geometric Approach },
  author={ Zhe Zhang and Chunyu Wang and Wenhu Qin and Wenjun Zeng },
  journal={ arXiv preprint arXiv:2003.11163v2 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2003.11163v2 },
  abstract={ We propose to estimate 3D human pose from multi-view images and a few IMUs attached at person's limbs. It operates by firstly detecting 2D poses from the two signals, and then lifting them to the 3D space. We present a geometric approach to reinforce the visual features of each pair of joints based on the IMUs. This notably improves 2D pose estimation accuracy especially when one joint is occluded. We call this approach Orientation Regularized Network (ORN). Then we lift the multi-view 2D poses to the 3D space by an Orientation Regularized Pictorial Structure Model (ORPSM) which jointly minimizes the projection error between the 3D and 2D poses, along with the discrepancy between the 3D pose and IMU orientations. The simple two-step approach reduces the error of the state-of-the-art by a large margin on a public dataset. Our code will be released at https://github.com/CHUNYUWANG/imu-human-pose-pytorch. }
}

@article{200402051v1,
  title={ Multivariate Regression of Mixed Responses for Evaluation of   Visualization Designs },
  author={ Xiaoning Kang and Xiaoyu Chen and Ran Jin and Hao Wu and Xinwei Deng },
  journal={ arXiv preprint arXiv:2004.02051v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2004.02051v1 },
  abstract={ Information visualization significantly enhances human perception by graphically representing complex data sets. The variety of visualization designs makes it challenging to efficiently evaluate all possible designs catering to users' preferences and characteristics. Most of existing evaluation methods perform user studies to obtain multivariate qualitative responses from users via questionnaires and interviews. However, these methods cannot support online evaluation of designs as they are often time-consuming. A statistical model is desired to predict users' preferences on visualization designs based on non-interference measurements (i.e., wearable sensor signals). In this work, we propose a multivariate regression of mixed responses (MRMR) to facilitate quantitative evaluation of visualization designs. The proposed MRMR method is able to provide accurate model prediction with meaningful variable selection. A simulation study and a user study of evaluating visualization designs with 14 effective participants are conducted to illustrate the merits of the proposed model. }
}

@article{200400467v1,
  title={ Quasi-Direct Drive Actuation for a Lightweight Hip Exoskeleton with High   Backdrivability and High Bandwidth },
  author={ Shuangyue Yu and Tzu-Hao Huang and Xiaolong Yang and Chunhai Jiao and Jianfu Yang and Hang Hu and Sainan Zhang and Yue Chen and Jingang Yi and Hao Su },
  journal={ arXiv preprint arXiv:2004.00467v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2004.00467v1 },
  abstract={ High-performance actuators are crucial to enable mechanical versatility of lower-limb wearable robots, which are required to be lightweight, highly backdrivable, and with high bandwidth. State-of-the-art actuators, e.g., series elastic actuators (SEAs), have to compromise bandwidth to improve compliance (i.e., backdrivability). In this paper, we describe the design and human-robot interaction modeling of a portable hip exoskeleton based on our custom quasi-direct drive (QDD) actuation (i.e., a high torque density motor with low ratio gear). We also present a model-based performance benchmark comparison of representative actuators in terms of torque capability, control bandwidth, backdrivability, and force tracking accuracy. This paper aims to corroborate the underlying philosophy of ''design for control'', namely meticulous robot design can simplify control algorithms while ensuring high performance. Following this idea, we create a lightweight bilateral hip exoskeleton (overall mass is 3.4 kg) to reduce joint loadings during normal activities, including walking and squatting. Experimental results indicate that the exoskeleton is able to produce high nominal torque (17.5 Nm), high backdrivability (0.4 Nm backdrive torque), high bandwidth (62.4 Hz), and high control accuracy (1.09 Nm root mean square tracking error, i.e., 5.4\% of the desired peak torque). Its controller is versatile to assist walking at different speeds (0.8-1.4 m/s) and squatting at 2 s cadence. This work demonstrates significant improvement in backdrivability and control bandwidth compared with state-of-the-art exoskeletons powered by the conventional actuation or SEA. }
}

@article{200309018v1,
  title={ Human Activity Recognition from Wearable Sensor Data Using   Self-Attention },
  author={ Saif Mahmud and M Tanjid Hasan Tonmoy and Kishor Kumar Bhaumik and A K M Mahbubur Rahman and M Ashraful Amin and Mohammad Shoyaib and Muhammad Asif Hossain Khan and Amin Ahsan Ali },
  journal={ arXiv preprint arXiv:2003.09018v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2003.09018v1 },
  abstract={ Human Activity Recognition from body-worn sensor data poses an inherent challenge in capturing spatial and temporal dependencies of time-series signals. In this regard, the existing recurrent or convolutional or their hybrid models for activity recognition struggle to capture spatio-temporal context from the feature space of sensor reading sequence. To address this complex problem, we propose a self-attention based neural network model that foregoes recurrent architectures and utilizes different types of attention mechanisms to generate higher dimensional feature representation used for classification. We performed extensive experiments on four popular publicly available HAR datasets: PAMAP2, Opportunity, Skoda and USC-HAD. Our model achieve significant performance improvement over recent state-of-the-art models in both benchmark test subjects and Leave-one-subject-out evaluation. We also observe that the sensor attention maps produced by our model is able capture the importance of the modality and placement of the sensors in predicting the different activity classes. }
}

@article{200301753v1,
  title={ Uncertainty Quantification for Deep Context-Aware Mobile Activity   Recognition and Unknown Context Discovery },
  author={ Zepeng Huo and Arash PakBin and Xiaohan Chen and Nathan Hurley and Ye Yuan and Xiaoning Qian and Zhangyang Wang and Shuai Huang and Bobak Mortazavi },
  journal={ arXiv preprint arXiv:2003.01753v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2003.01753v1 },
  abstract={ Activity recognition in wearable computing faces two key challenges: i) activity characteristics may be context-dependent and change under different contexts or situations; ii) unknown contexts and activities may occur from time to time, requiring flexibility and adaptability of the algorithm. We develop a context-aware mixture of deep models termed the \{\\textbackslash{}alpha\}-\\textbackslash{}b\{eta\} network coupled with uncertainty quantification (UQ) based upon maximum entropy to enhance human activity recognition performance. We improve accuracy and F score by 10\% by identifying high-level contexts in a data-driven way to guide model development. In order to ensure training stability, we have used a clustering-based pre-training in both public and in-house datasets, demonstrating improved accuracy through unknown context discovery. }
}

@article{200306327v1,
  title={ Human Activity Recognition using Multi-Head CNN followed by LSTM },
  author={ Waqar Ahmad and Misbah Kazmi and Hazrat Ali },
  journal={ arXiv preprint arXiv:2003.06327v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2003.06327v1 },
  abstract={ This study presents a novel method to recognize human physical activities using CNN followed by LSTM. Achieving high accuracy by traditional machine learning algorithms, (such as SVM, KNN and random forest method) is a challenging task because the data acquired from the wearable sensors like accelerometer and gyroscope is a time-series data. So, to achieve high accuracy, we propose a multi-head CNN model comprising of three CNNs to extract features for the data acquired from different sensors and all three CNNs are then merged, which are followed by an LSTM layer and a dense layer. The configuration of all three CNNs is kept the same so that the same number of features are obtained for every input to CNN. By using the proposed method, we achieve state-of-the-art accuracy, which is comparable to traditional machine learning algorithms and other deep neural network algorithms. }
}

@article{200203866v1,
  title={ Machine learning approaches for identifying prey handling activity in   otariid pinnipeds },
  author={ Rita Pucci and Alessio Micheli and Stefano Chessa and Jane Hunter },
  journal={ arXiv preprint arXiv:2002.03866v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2002.03866v1 },
  abstract={ Systems developed in wearable devices with sensors onboard are widely used to collect data of humans and animals activities with the perspective of an on-board automatic classification of data. An interesting application of these systems is to support animals' behaviour monitoring gathered by sensors' data analysis. This is a challenging area and in particular with fixed memories capabilities because the devices should be able to operate autonomously for long periods before being retrieved by human operators, and being able to classify activities onboard can significantly improve their autonomy. In this paper, we focus on the identification of prey handling activity in seals (when the animal start attaching and biting the prey), which is one of the main movement that identifies a successful foraging activity. Data taken into consideration are streams of 3D accelerometers and depth sensors values collected by devices attached directly on seals. To analyse these data, we propose an automatic model based on Machine Learning (ML) algorithms. In particular, we compare the performance (in terms of accuracy and F1score) of three ML algorithms: Input Delay Neural Networks, Support Vector Machines, and Echo State Networks. We attend to the final aim of developing an automatic classifier on-board. For this purpose, in this paper, the comparison is performed concerning the performance obtained by each ML approach developed and its memory footprint. In the end, we highlight the advantage of using an ML algorithm, in terms of feasibility in wild animals' monitoring. }
}

@article{200111337v1,
  title={ EEG-based Brain-Computer Interfaces (BCIs): A Survey of Recent Studies   on Signal Sensing Technologies and Computational Intelligence Approaches and   their Applications },
  author={ Xiaotong Gu and Zehong Cao and Alireza Jolfaei and Peng Xu and Dongrui Wu and Tzyy-Ping Jung and Chin-Teng Lin },
  journal={ arXiv preprint arXiv:2001.11337v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2001.11337v1 },
  abstract={ Brain-Computer Interface (BCI) is a powerful communication tool between users and systems, which enhances the capability of the human brain in communicating and interacting with the environment directly. Advances in neuroscience and computer science in the past decades have led to exciting developments in BCI, thereby making BCI a top interdisciplinary research area in computational neuroscience and intelligence. Recent technological advances such as wearable sensing devices, real-time data streaming, machine learning, and deep learning approaches have increased interest in electroencephalographic (EEG) based BCI for translational and healthcare applications. Many people benefit from EEG-based BCIs, which facilitate continuous monitoring of fluctuations in cognitive states under monotonous tasks in the workplace or at home. In this study, we survey the recent literature of EEG signal sensing technologies and computational intelligence approaches in BCI applications, compensated for the gaps in the systematic summary of the past five years (2015-2019). In specific, we first review the current status of BCI and its significant obstacles. Then, we present advanced signal sensing and enhancement technologies to collect and clean EEG signals, respectively. Furthermore, we demonstrate state-of-art computational intelligence techniques, including interpretable fuzzy models, transfer learning, deep learning, and combinations, to monitor, maintain, or track human cognitive states and operating performance in prevalent applications. Finally, we deliver a couple of innovative BCI-inspired healthcare applications and discuss some future research directions in EEG-based BCIs. }
}

@article{250419179v1,
  title={ A Design Framework for operationalizing Trustworthy Artificial   Intelligence in Healthcare: Requirements, Tradeoffs and Challenges for its   Clinical Adoption },
  author={ Pedro A. Moreno-Sánchez and Javier Del Ser and Mark van Gils and Jussi Hernesniemi },
  journal={ arXiv preprint arXiv:2504.19179v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2504.19179v1 },
  abstract={ Artificial Intelligence (AI) holds great promise for transforming healthcare, particularly in disease diagnosis, prognosis, and patient care. The increasing availability of digital medical data, such as images, omics, biosignals, and electronic health records, combined with advances in computing, has enabled AI models to approach expert-level performance. However, widespread clinical adoption remains limited, primarily due to challenges beyond technical performance, including ethical concerns, regulatory barriers, and lack of trust. To address these issues, AI systems must align with the principles of Trustworthy AI (TAI), which emphasize human agency and oversight, algorithmic robustness, privacy and data governance, transparency, bias and discrimination avoidance, and accountability. Yet, the complexity of healthcare processes (e.g., screening, diagnosis, prognosis, and treatment) and the diversity of stakeholders (clinicians, patients, providers, regulators) complicate the integration of TAI principles. To bridge the gap between TAI theory and practical implementation, this paper proposes a design framework to support developers in embedding TAI principles into medical AI systems. Thus, for each stakeholder identified across various healthcare processes, we propose a disease-agnostic collection of requirements that medical AI systems should incorporate to adhere to the principles of TAI. Additionally, we examine the challenges and tradeoffs that may arise when applying these principles in practice. To ground the discussion, we focus on cardiovascular diseases, a field marked by both high prevalence and active AI innovation, and demonstrate how TAI principles have been applied and where key obstacles persist. }
}

@article{250313570v1,
  title={ ExChanGeAI: An End-to-End Platform and Efficient Foundation Model for   Electrocardiogram Analysis and Fine-tuning },
  author={ Lucas Bickmann and Lucas Plagwitz and Antonius Büscher and Lars Eckardt and Julian Varghese },
  journal={ arXiv preprint arXiv:2503.13570v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.13570v1 },
  abstract={ Electrocardiogram data, one of the most widely available biosignal data, has become increasingly valuable with the emergence of deep learning methods, providing novel insights into cardiovascular diseases and broader health conditions. However, heterogeneity of electrocardiogram formats, limited access to deep learning model weights and intricate algorithmic steps for effective fine-tuning for own disease target labels result in complex workflows. In this work, we introduce ExChanGeAI, a web-based end-to-end platform that streamlines the reading of different formats, pre-processing, visualization and custom machine learning with local and privacy-preserving fine-tuning. ExChanGeAI is adaptable for use on both personal computers and scalable to high performance server environments. The platform offers state-of-the-art deep learning models for training from scratch, alongside our novel open-source electrocardiogram foundation model CardX, pre-trained on over one million electrocardiograms. Evaluation across three external validation sets, including an entirely new testset extracted from routine care, demonstrate the fine-tuning capabilities of ExChanGeAI. CardX outperformed the benchmark foundation model while requiring significantly fewer parameters and lower computational resources. The platform enables users to empirically determine the most suitable model for their specific tasks based on systematic validations.The code is available at https://imigitlab.uni-muenster.de/published/exchangeai . }
}

@article{241117785v1,
  title={ New Test-Time Scenario for Biosignal: Concept and Its Approach },
  author={ Yong-Yeon Jo and Byeong Tak Lee and Beom Joon Kim and Jeong-Ho Hong and Hak Seung Lee and Joon-myoung Kwon },
  journal={ arXiv preprint arXiv:2411.17785v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2411.17785v1 },
  abstract={ Online Test-Time Adaptation (OTTA) enhances model robustness by updating pre-trained models with unlabeled data during testing. In healthcare, OTTA is vital for real-time tasks like predicting blood pressure from biosignals, which demand continuous adaptation. We introduce a new test-time scenario with streams of unlabeled samples and occasional labeled samples. Our framework combines supervised and self-supervised learning, employing a dual-queue buffer and weighted batch sampling to balance data types. Experiments show improved accuracy and adaptability under real-world conditions. }
}

@article{231200817v3,
  title={ TimelyGPT: Extrapolatable Transformer Pre-training for Long-term   Time-Series Forecasting in Healthcare },
  author={ Ziyang Song and Qincheng Lu and Hao Xu and He Zhu and David L. Buckeridge and Yue Li },
  journal={ arXiv preprint arXiv:2312.00817v3 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2312.00817v3 },
  abstract={ Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success in Natural Language Processing and Computer Vision domains. However, the development of PTMs on healthcare time-series data is lagging behind.This underscores the limitations of the existing transformer-based architectures, particularly their scalability to handle large-scale time series and ability to capture long-term temporal dependencies. In this study, we present Timely Generative Pre-trained Transformer (TimelyGPT). TimelyGPT employs an extrapolatable position (xPos) embedding to encode trend and periodic patterns into time-series representations. It also integrates recurrent attention and temporal convolution modules to effectively capture global-local temporal dependencies. We evaluated TimelyGPT on two large-scale healthcare time series datasets corresponding to continuous biosignals and irregularly-sampled time series, respectively. Our experiments show that during pre-training, TimelyGPT excels in learning time-series representations from continuously monitored biosignals and irregularly-sampled time series data commonly observed in longitudinal electronic health records (EHRs). In forecasting continuous biosignals, TimelyGPT achieves accurate extrapolation up to 6,000 timesteps of body temperature during the sleep stage transition, given a short look-up window (i.e., prompt) containing only 2,000 timesteps. For irregularly-sampled time series, TimelyGPT with a proposed time-specific inference demonstrates high top recall scores in predicting future diagnoses using early diagnostic records, effectively handling irregular intervals between clinical records. Together, we envision TimelyGPT to be useful in a broad spectrum of health domains, including long-term patient health state forecasting and patient risk trajectory prediction. }
}

@article{240816291v1,
  title={ Flexible framework for generating synthetic electrocardiograms and   photoplethysmograms },
  author={ Katri Karhinoja and Antti Vasankari and Jukka-Pekka Sirkiä and Antti Airola and David Wong and Matti Kaisti },
  journal={ arXiv preprint arXiv:2408.16291v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.16291v1 },
  abstract={ By generating synthetic biosignals, the quantity and variety of health data can be increased. This is especially useful when training machine learning models by enabling data augmentation and introduction of more physiologically plausible variation to the data. For these purposes, we have developed a synthetic biosignal model for two signal modalities, electrocardiography (ECG) and photoplethysmography (PPG). The model produces realistic signals that account for physiological effects such as breathing modulation and changes in heart rate due to physical stress. Arrhythmic signals can be generated with beat intervals extracted from real measurements. The model also includes a flexible approach to adding different kinds of noise and signal artifacts. The noise is generated from power spectral densities extracted from both measured noisy signals and modeled power spectra. Importantly, the model also automatically produces labels for noise, segmentation (e.g. P and T waves, QRS complex, for electrocardiograms), and artifacts. We assessed how this comprehensive model can be used in practice to improve the performance of models trained on ECG or PPG data. For example, we trained an LSTM to detect ECG R-peaks using both real ECG signals from the MIT-BIH arrythmia set and our new generator. The F1 score of the model was 0.83 using real data, in comparison to 0.98 using our generator. In addition, the model can be used for example in signal segmentation, quality detection and bench-marking detection algorithms. The model code has been released in \\textbackslash{}url\{https://github.com/UTU-Health-Research/framework\_for\_synthetic\_biosignals\} }
}

@article{230907183v2,
  title={ Respiratory Disease Classification and Biometric Analysis Using   Biosignals from Digital Stethoscopes },
  author={ Constantino Álvarez Casado and Manuel Lage Cañellas and Matteo Pedone and Xiaoting Wu and Le Nguyen and Miguel Bordallo López },
  journal={ arXiv preprint arXiv:2309.07183v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2309.07183v2 },
  abstract={ Respiratory diseases remain a leading cause of mortality worldwide, highlighting the need for faster and more accurate diagnostic tools. This work presents a novel approach leveraging digital stethoscope technology for automatic respiratory disease classification and biometric analysis. Our approach has the potential to significantly enhance traditional auscultation practices. By leveraging one of the largest publicly available medical database of respiratory sounds, we train machine learning models to classify various respiratory health conditions. Our method differs from conventional methods by using Empirical Mode Decomposition (EMD) and spectral analysis techniques to isolate clinically relevant biosignals embedded within acoustic data captured by digital stethoscopes. This approach focuses on information closely tied to cardiovascular and respiratory patterns within the acoustic data. Spectral analysis and filtering techniques isolate Intrinsic Mode Functions (IMFs) strongly correlated with these physiological phenomena. These biosignals undergo a comprehensive feature extraction process for predictive modeling. These features then serve as input to train several machine learning models for both classification and regression tasks. Our approach achieves high accuracy in both binary classification (89\% balanced accuracy for healthy vs. diseased) and multi-class classification (72\% balanced accuracy for specific diseases like pneumonia and COPD). For the first time, this work introduces regression models capable of estimating age and body mass index (BMI) based solely on acoustic data, as well as a model for sex classification. Our findings underscore the potential of intelligent digital stethoscopes to significantly enhance assistive and remote diagnostic capabilities, contributing to advancements in digital health, telehealth, and remote patient monitoring. }
}

@article{240305562v1,
  title={ SDXL Finetuned with LoRA for Coloring Therapy: Generating Graphic   Templates Inspired by United Arab Emirates Culture },
  author={ Abdulla Alfalasi and Esrat Khan and Mohamed Alhashmi and Raed Aldweik and Davor Svetinovic },
  journal={ arXiv preprint arXiv:2403.05562v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2403.05562v1 },
  abstract={ A transformative approach to mental health therapy lies at the crossroads of cultural heritage and advanced technology. This paper introduces an innovative method that fuses machine learning techniques with traditional Emirati motifs, focusing on the United Arab Emirates (UAE). We utilize the Stable Diffusion XL (SDXL) model, enhanced with Low-Rank Adaptation (LoRA), to create culturally significant coloring templates featuring Al-Sadu weaving patterns. This novel approach leverages coloring therapy for its recognized stress-relieving benefits and embeds deep cultural resonance, making it a potent tool for therapeutic intervention and cultural preservation. Specifically targeting Generalized Anxiety Disorder (GAD), our method demonstrates significant potential in reducing associated symptoms. Additionally, the paper delves into the broader implications of color and music therapy, emphasizing the importance of culturally tailored content. The technical aspects of the SDXL model and its LoRA fine-tuning showcase its capability to generate high-quality, culturally specific images. This research stands at the forefront of integrating mental wellness practices with cultural heritage, providing a groundbreaking perspective on the synergy between technology, culture, and healthcare. In future work, we aim to employ biosignals to assess the level of engagement and effectiveness of color therapy. A key focus will be to examine the impact of the Emirati heritage Al Sadu art on Emirati individuals and compare their responses with those of other nationalities. This will provide deeper insights into the cultural specificity of therapeutic interventions and further the understanding of the unique interplay between cultural identity and mental health therapy. }
}

@article{240110794v1,
  title={ Deep Reinforcement Learning Empowered Activity-Aware Dynamic Health   Monitoring Systems },
  author={ Ziqiaing Ye and Yulan Gao and Yue Xiao and Zehui Xiong and Dusit Niyato },
  journal={ arXiv preprint arXiv:2401.10794v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2401.10794v1 },
  abstract={ In smart healthcare, health monitoring utilizes diverse tools and technologies to analyze patients' real-time biosignal data, enabling immediate actions and interventions. Existing monitoring approaches were designed on the premise that medical devices track several health metrics concurrently, tailored to their designated functional scope. This means that they report all relevant health values within that scope, which can result in excess resource use and the gathering of extraneous data due to monitoring irrelevant health metrics. In this context, we propose Dynamic Activity-Aware Health Monitoring strategy (DActAHM) for striking a balance between optimal monitoring performance and cost efficiency, a novel framework based on Deep Reinforcement Learning (DRL) and SlowFast Model to ensure precise monitoring based on users' activities. Specifically, with the SlowFast Model, DActAHM efficiently identifies individual activities and captures these results for enhanced processing. Subsequently, DActAHM refines health metric monitoring in response to the identified activity by incorporating a DRL framework. Extensive experiments comparing DActAHM against three state-of-the-art approaches demonstrate it achieves 27.3\% higher gain than the best-performing baseline that fixes monitoring actions over timeline. }
}

@article{230200225v2,
  title={ The Past, Current, and Future of Neonatal Intensive Care Units with   Artificial Intelligence },
  author={ Elif Keles and Ulas Bagci },
  journal={ arXiv preprint arXiv:2302.00225v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2302.00225v2 },
  abstract={ Machine learning and deep learning are two subsets of artificial intelligence that involve teaching computers to learn and make decisions from any sort of data. Most recent developments in artificial intelligence are coming from deep learning, which has proven revolutionary in almost all fields, from computer vision to health sciences. The effects of deep learning in medicine have changed the conventional ways of clinical application significantly. Although some sub-fields of medicine, such as pediatrics, have been relatively slow in receiving the critical benefits of deep learning, related research in pediatrics has started to accumulate to a significant level, too. Hence, in this paper, we review recently developed machine learning and deep learning-based solutions for neonatology applications. We systematically evaluate the roles of both classical machine learning and deep learning in neonatology applications, define the methodologies, including algorithmic developments, and describe the remaining challenges in the assessment of neonatal diseases by using PRISMA 2020 guidelines. To date, the primary areas of focus in neonatology regarding AI applications have included survival analysis, neuroimaging, analysis of vital parameters and biosignals, and retinopathy of prematurity diagnosis. We have categorically summarized 106 research articles from 1996 to 2022 and discussed their pros and cons, respectively. In this systematic review, we aimed to further enhance the comprehensiveness of the study. We also discuss possible directions for new AI models and the future of neonatology with the rising power of AI, suggesting roadmaps for the integration of AI into neonatal intensive care units. }
}

@article{230306452v1,
  title={ Hallucinated Heartbeats: Anomaly-Aware Remote Pulse Estimation },
  author={ Jeremy Speth and Nathan Vance and Benjamin Sporrer and Lu Niu and Patrick Flynn and Adam Czajka },
  journal={ arXiv preprint arXiv:2303.06452v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2303.06452v1 },
  abstract={ Camera-based physiological monitoring, especially remote photoplethysmography (rPPG), is a promising tool for health diagnostics, and state-of-the-art pulse estimators have shown impressive performance on benchmark datasets. We argue that evaluations of modern solutions may be incomplete, as we uncover failure cases for videos without a live person, or in the presence of severe noise. We demonstrate that spatiotemporal deep learning models trained only with live samples ''hallucinate'' a genuine-shaped pulse on anomalous and noisy videos, which may have negative consequences when rPPG models are used by medical personnel. To address this, we offer: (a) An anomaly detection model, built on top of the predicted waveforms. We compare models trained in open-set (unknown abnormal predictions) and closed-set (abnormal predictions known when training) settings; (b) An anomaly-aware training regime that penalizes the model for predicting periodic signals from anomalous videos. Extensive experimentation with eight research datasets (rPPG-specific: DDPM, CDDPM, PURE, UBFC, ARPM; deep fakes: DFDC; face presentation attack detection: HKBU-MARs; rPPG outlier: KITTI) show better accuracy of anomaly detection for deep learning models incorporating the proposed training (75.8\%), compared to models trained regularly (73.7\%) and to hand-crafted rPPG methods (52-62\%). }
}

@article{211209196v2,
  title={ Benchmarking Uncertainty Quantification on Biosignal Classification   Tasks under Dataset Shift },
  author={ Tong Xia and Jing Han and Cecilia Mascolo },
  journal={ arXiv preprint arXiv:2112.09196v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2112.09196v2 },
  abstract={ A biosignal is a signal that can be continuously measured from human bodies, such as respiratory sounds, heart activity (ECG), brain waves (EEG), etc, based on which, machine learning models have been developed with very promising performance for automatic disease detection and health status monitoring. However, dataset shift, i.e., data distribution of inference varies from the distribution of the training, is not uncommon for real biosignal-based applications. To improve the robustness, probabilistic models with uncertainty quantification are adapted to capture how reliable a prediction is. Yet, assessing the quality of the estimated uncertainty remains a challenge. In this work, we propose a framework to evaluate the capability of the estimated uncertainty in capturing different types of biosignal dataset shifts with various degrees. In particular, we use three classification tasks based on respiratory sounds and electrocardiography signals to benchmark five representative uncertainty quantification methods. Extensive experiments show that, although Ensemble and Bayesian models could provide relatively better uncertainty estimations under dataset shifts, all tested models fail to meet the promise in trustworthy prediction and model calibration. Our work paves the way for a comprehensive evaluation for any newly developed biosignal classifiers. }
}

@article{250311741v3,
  title={ BioMamba: Leveraging Spectro-Temporal Embedding in Bidirectional Mamba   for Enhanced Biosignal Classification },
  author={ Jian Qian and Teck Lun Goh and Bingyu Xie and Chengyao Zhu and Biao Wan and Yawen Guan and Rachel Ding Chen and Patrick Yin Chiang },
  journal={ arXiv preprint arXiv:2503.11741v3 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.11741v3 },
  abstract={ Biological signals, such as electroencephalograms (EEGs) and electrocardiograms (ECGs), play a pivotal role in numerous clinical practices, such as diagnosing brain and cardiac arrhythmic diseases. Existing methods for biosignal classification rely on Attention-based frameworks with dense Feed Forward layers, which lead to inefficient learning, high computational overhead, and suboptimal performance. In this work, we introduce BioMamba, a Spectro-Temporal Embedding strategy applied to the Bidirectional Mamba framework with Sparse Feed Forward layers to enable effective learning of biosignal sequences. By integrating these three key components, BioMamba effectively addresses the limitations of existing methods. Extensive experiments demonstrate that BioMamba significantly outperforms state-of-the-art methods with marked improvement in classification performance. The advantages of the proposed BioMamba include (1) Reliability: BioMamba consistently delivers robust results, confirmed across six evaluation metrics. (2) Efficiency: We assess both model and training efficiency, the BioMamba demonstrates computational effectiveness by reducing model size and resource consumption compared to existing approaches. (3) Generality: With the capacity to effectively classify a diverse set of tasks, BioMamba demonstrates adaptability and effectiveness across various domains and applications. }
}

@article{231209454v1,
  title={ Uncertainty Quantification in Machine Learning for Biosignal   Applications -- A Review },
  author={ Ivo Pascal de Jong and Andreea Ioana Sburlea and Matias Valdenegro-Toro },
  journal={ arXiv preprint arXiv:2312.09454v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2312.09454v1 },
  abstract={ Uncertainty Quantification (UQ) has gained traction in an attempt to fix the black-box nature of Deep Learning. Specifically (medical) biosignals such as electroencephalography (EEG), electrocardiography (ECG), electroocculography (EOG) and electromyography (EMG) could benefit from good UQ, since these suffer from a poor signal to noise ratio, and good human interpretability is pivotal for medical applications and Brain Computer Interfaces. In this paper, we review the state of the art at the intersection of Uncertainty Quantification and Biosignal with Machine Learning. We present various methods, shortcomings, uncertainty measures and theoretical frameworks that currently exist in this application domain. Overall it can be concluded that promising UQ methods are available, but that research is needed on how people and systems may interact with an uncertainty model in a (clinical) environment. }
}

@article{230614620v1,
  title={ Video object detection for privacy-preserving patient monitoring in   intensive care },
  author={ Raphael Emberger and Jens Michael Boss and Daniel Baumann and Marko Seric and Shufan Huo and Lukas Tuggener and Emanuela Keller and Thilo Stadelmann },
  journal={ arXiv preprint arXiv:2306.14620v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2306.14620v1 },
  abstract={ Patient monitoring in intensive care units, although assisted by biosensors, needs continuous supervision of staff. To reduce the burden on staff members, IT infrastructures are built to record monitoring data and develop clinical decision support systems. These systems, however, are vulnerable to artifacts (e.g. muscle movement due to ongoing treatment), which are often indistinguishable from real and potentially dangerous signals. Video recordings could facilitate the reliable classification of biosignals using object detection (OD) methods to find sources of unwanted artifacts. Due to privacy restrictions, only blurred videos can be stored, which severely impairs the possibility to detect clinically relevant events such as interventions or changes in patient status with standard OD methods. Hence, new kinds of approaches are necessary that exploit every kind of available information due to the reduced information content of blurred footage and that are at the same time easily implementable within the IT infrastructure of a normal hospital. In this paper, we propose a new method for exploiting information in the temporal succession of video frames. To be efficiently implementable using off-the-shelf object detectors that comply with given hardware constraints, we repurpose the image color channels to account for temporal consistency, leading to an improved detection rate of the object classes. Our method outperforms a standard YOLOv5 baseline model by +1.7\% mAP@.5 while also training over ten times faster on our proprietary dataset. We conclude that this approach has shown effectiveness in the preliminary experiments and holds potential for more general video OD in the future. }
}

@article{230510351v1,
  title={ BIOT: Cross-data Biosignal Learning in the Wild },
  author={ Chaoqi Yang and M. Brandon Westover and Jimeng Sun },
  journal={ arXiv preprint arXiv:2305.10351v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2305.10351v1 },
  abstract={ Biological signals, such as electroencephalograms (EEG), play a crucial role in numerous clinical applications, exhibiting diverse data formats and quality profiles. Current deep learning models for biosignals are typically specialized for specific datasets and clinical settings, limiting their broader applicability. Motivated by the success of large language models in text processing, we explore the development of foundational models that are trained from multiple data sources and can be fine-tuned on different downstream biosignal tasks.   To overcome the unique challenges associated with biosignals of various formats, such as mismatched channels, variable sample lengths, and prevalent missing values, we propose a Biosignal Transformer (\\textbackslash{}method). The proposed \\textbackslash{}method model can enable cross-data learning with mismatched channels, variable lengths, and missing values by tokenizing diverse biosignals into unified ''biosignal sentences''. Specifically, we tokenize each channel into fixed-length segments containing local signal features, flattening them to form consistent ''sentences''. Channel embeddings and \{\\textbackslash{}em relative\} position embeddings are added to preserve spatio-temporal features.   The \\textbackslash{}method model is versatile and applicable to various biosignal learning settings across different datasets, including joint pre-training for larger models. Comprehensive evaluations on EEG, electrocardiogram (ECG), and human activity sensory signals demonstrate that \\textbackslash{}method outperforms robust baselines in common settings and facilitates learning across multiple datasets with different formats. Use CHB-MIT seizure detection task as an example, our vanilla \\textbackslash{}method model shows 3\\textbackslash{}\% improvement over baselines in balanced accuracy, and the pre-trained \\textbackslash{}method models (optimized from other data sources) can further bring up to 4\\textbackslash{}\% improvements. }
}

@article{230212893v1,
  title={ Don't be fooled: label leakage in explanation methods and the importance   of their quantitative evaluation },
  author={ Neil Jethani and Adriel Saporta and Rajesh Ranganath },
  journal={ arXiv preprint arXiv:2302.12893v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2302.12893v1 },
  abstract={ Feature attribution methods identify which features of an input most influence a model's output. Most widely-used feature attribution methods (such as SHAP, LIME, and Grad-CAM) are ''class-dependent'' methods in that they generate a feature attribution vector as a function of class. In this work, we demonstrate that class-dependent methods can ''leak'' information about the selected class, making that class appear more likely than it is. Thus, an end user runs the risk of drawing false conclusions when interpreting an explanation generated by a class-dependent method. In contrast, we introduce ''distribution-aware'' methods, which favor explanations that keep the label's distribution close to its distribution given all features of the input. We introduce SHAP-KL and FastSHAP-KL, two baseline distribution-aware methods that compute Shapley values. Finally, we perform a comprehensive evaluation of seven class-dependent and three distribution-aware methods on three clinical datasets of different high-dimensional data types: images, biosignals, and text. }
}

@article{210312676v2,
  title={ Self-supervised representation learning from 12-lead ECG data },
  author={ Temesgen Mehari and Nils Strodthoff },
  journal={ arXiv preprint arXiv:2103.12676v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2103.12676v2 },
  abstract={ Clinical 12-lead electrocardiography (ECG) is one of the most widely encountered kinds of biosignals. Despite the increased availability of public ECG datasets, label scarcity remains a central challenge in the field. Self-supervised learning represents a promising way to alleviate this issue. In this work, we put forward the first comprehensive assessment of self-supervised representation learning from clinical 12-lead ECG data. To this end, we adapt state-of-the-art self-supervised methods based on instance discrimination and latent forecasting to the ECG domain. In a first step, we learn contrastive representations and evaluate their quality based on linear evaluation performance on a recently established, comprehensive, clinical ECG classification task. In a second step, we analyze the impact of self-supervised pretraining on finetuned ECG classifiers as compared to purely supervised performance. For the best-performing method, an adaptation of contrastive predictive coding, we find a linear evaluation performance only 0.5\% below supervised performance. For the finetuned models, we find improvements in downstream performance of roughly 1\% compared to supervised performance, label efficiency, as well as robustness against physiological noise. This work clearly establishes the feasibility of extracting discriminative representations from ECG data via self-supervised learning and the numerous advantages when finetuning such representations on downstream tasks as compared to purely supervised training. As first comprehensive assessment of its kind in the ECG domain carried out exclusively on publicly available datasets, we hope to establish a first step towards reproducible progress in the rapidly evolving field of representation learning for biosignals. }
}

@article{250217462v1,
  title={ The Case for Cleaner Biosignals: High-fidelity Neural Compressor Enables   Transfer from Cleaner iEEG to Noisier EEG },
  author={ Francesco Stefano Carzaniga and Gary Tom Hoppeler and Michael Hersche and Kaspar Anton Schindler and Abbas Rahimi },
  journal={ arXiv preprint arXiv:2502.17462v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.17462v1 },
  abstract={ All data modalities are not created equal, even when the signal they measure comes from the same source. In the case of the brain, two of the most important data modalities are the scalp electroencephalogram (EEG), and the intracranial electroencephalogram (iEEG). They are used by human experts, supported by deep learning (DL) models, to accomplish a variety of tasks, such as seizure detection and motor imagery classification. Although the differences between EEG and iEEG are well understood by human experts, the performance of DL models across these two modalities remains under-explored. To help characterize the importance of clean data on the performance of DL models, we propose BrainCodec, a high-fidelity EEG and iEEG neural compressor. We find that training BrainCodec on iEEG and then transferring to EEG yields higher reconstruction quality than training on EEG directly. In addition, we also find that training BrainCodec on both EEG and iEEG improves fidelity when reconstructing EEG. Our work indicates that data sources with higher SNR, such as iEEG, provide better performance across the board also in the medical time-series domain. BrainCodec also achieves up to a 64x compression on iEEG and EEG without a notable decrease in quality. BrainCodec markedly surpasses current state-of-the-art compression models both in final compression ratio and in reconstruction fidelity. We also evaluate the fidelity of the compressed signals objectively on a seizure detection and a motor imagery task performed by standard DL models. Here, we find that BrainCodec achieves a reconstruction fidelity high enough to ensure no performance degradation on the downstream tasks. Finally, we collect the subjective assessment of an expert neurologist, that confirms the high reconstruction quality of BrainCodec in a realistic scenario. The code is available at https://github.com/IBM/eeg-ieeg-brain-compressor. }
}

@article{250200275v1,
  title={ Simultaneous Estimation of Manipulation Skill and Hand Grasp Force from   Forearm Ultrasound Images },
  author={ Keshav Bimbraw and Srikar Nekkanti and Daniel B. Tiller II and Mihir Deshmukh and Berk Calli and Robert D. Howe and Haichong K. Zhang },
  journal={ arXiv preprint arXiv:2502.00275v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.00275v1 },
  abstract={ Accurate estimation of human hand configuration and the forces they exert is critical for effective teleoperation and skill transfer in robotic manipulation. A deeper understanding of human interactions with objects can further enhance teleoperation performance. To address this need, researchers have explored methods to capture and translate human manipulation skills and applied forces to robotic systems. Among these, biosignal-based approaches, particularly those using forearm ultrasound data, have shown significant potential for estimating hand movements and finger forces. In this study, we present a method for simultaneously estimating manipulation skills and applied hand force using forearm ultrasound data. Data collected from seven participants were used to train deep learning models for classifying manipulation skills and estimating grasp force. Our models achieved an average classification accuracy of 94.87 percent plus or minus 10.16 percent for manipulation skills and an average root mean square error (RMSE) of 0.51 plus or minus 0.19 Newtons for force estimation, as evaluated using five-fold cross-validation. These results highlight the effectiveness of forearm ultrasound in advancing human-machine interfacing and robotic teleoperation for complex manipulation tasks. This work enables new and effective possibilities for human-robot skill transfer and tele-manipulation, bridging the gap between human dexterity and robotic control. }
}

@article{240911061v1,
  title={ Force Myography based Torque Estimation in Human Knee and Ankle Joints },
  author={ Charlotte Marquardt and Arne Schulz and Miha Dezman and Gunther Kurz and Thorsten Stein and Tamim Asfour },
  journal={ arXiv preprint arXiv:2409.11061v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2409.11061v1 },
  abstract={ Online adaptation of exoskeleton control based on muscle activity sensing is a promising way to personalize exoskeletons based on the user's biosignals. While several electromyography (EMG) based methods have been shown to improve joint torque estimation, EMG sensors require direct skin contact and complex post-processing. In contrast, force myography (FMG) measures normal forces from changes in muscle volume due to muscle activity. We propose an FMG-based method to estimate knee and ankle joint torques by combining joint angles and velocities with muscle activity information. We learn a model for joint torque estimation using Gaussian process regression (GPR). The effectiveness of the proposed FMG-based method is validated on isokinetic motions performed by two subjects. The model is compared to a baseline model using only joint angle and velocity, as well as a model augmented by EMG data. The results show that integrating FMG into exoskeleton control improves the joint torque estimation for the ankle and knee and is therefore a promising way to improve adaptability to different exoskeleton users. }
}

@article{231003752v1,
  title={ A Deep Learning Sequential Decoder for Transient High-Density   Electromyography in Hand Gesture Recognition Using Subject-Embedded Transfer   Learning },
  author={ Golara Ahmadi Azar and Qin Hu and Melika Emami and Alyson Fletcher and Sundeep Rangan and S. Farokh Atashzar },
  journal={ arXiv preprint arXiv:2310.03752v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2310.03752v1 },
  abstract={ Hand gesture recognition (HGR) has gained significant attention due to the increasing use of AI-powered human-computer interfaces that can interpret the deep spatiotemporal dynamics of biosignals from the peripheral nervous system, such as surface electromyography (sEMG). These interfaces have a range of applications, including the control of extended reality, agile prosthetics, and exoskeletons. However, the natural variability of sEMG among individuals has led researchers to focus on subject-specific solutions. Deep learning methods, which often have complex structures, are particularly data-hungry and can be time-consuming to train, making them less practical for subject-specific applications. In this paper, we propose and develop a generalizable, sequential decoder of transient high-density sEMG (HD-sEMG) that achieves 73\% average accuracy on 65 gestures for partially-observed subjects through subject-embedded transfer learning, leveraging pre-knowledge of HGR acquired during pre-training. The use of transient HD-sEMG before gesture stabilization allows us to predict gestures with the ultimate goal of counterbalancing system control delays. The results show that the proposed generalized models significantly outperform subject-specific approaches, especially when the training data is limited, and there is a significant number of gesture classes. By building on pre-knowledge and incorporating a multiplicative subject-embedded structure, our method comparatively achieves more than 13\% average accuracy across partially observed subjects with minimal data availability. This work highlights the potential of HD-sEMG and demonstrates the benefits of modeling common patterns across users to reduce the need for large amounts of data for new users, enhancing practicality. }
}

@article{220809656v2,
  title={ A Domain Generalization Approach for Out-Of-Distribution 12-lead ECG   Classification with Convolutional Neural Networks },
  author={ Aristotelis Ballas and Christos Diou },
  journal={ arXiv preprint arXiv:2208.09656v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2208.09656v2 },
  abstract={ Deep Learning systems have achieved great success in the past few years, even surpassing human intelligence in several cases. As of late, they have also established themselves in the biomedical and healthcare domains, where they have shown a lot of promise, but have not yet achieved widespread adoption. This is in part due to the fact that most methods fail to maintain their performance when they are called to make decisions on data that originate from a different distribution than the one they were trained on, namely Out-Of-Distribution (OOD) data. For example, in the case of biosignal classification, models often fail to generalize well on datasets from different hospitals, due to the distribution discrepancy amongst different sources of data. Our goal is to demonstrate the Domain Generalization problem present between distinct hospital databases and propose a method that classifies abnormalities on 12-lead Electrocardiograms (ECGs), by leveraging information extracted across the architecture of a Deep Neural Network, and capturing the underlying structure of the signal. To this end, we adopt a ResNet-18 as the backbone model and extract features from several intermediate convolutional layers of the network. To evaluate our method, we adopt publicly available ECG datasets from four sources and handle them as separate domains. To simulate the distributional shift present in real-world settings, we train our model on a subset of the domains and leave-out the remaining ones. We then evaluate our model both on the data present at training time (intra-distribution) and the held-out data (out-of-distribution), achieving promising results and surpassing the baseline of a vanilla Residual Network in most of the cases. }
}

@article{211103977v2,
  title={ A Virtual Reality Simulation Pipeline for Online Mental Workload   Modeling },
  author={ Robert L. Wilson and Daniel Browne and Jonathan Wagstaff and Steve McGuire },
  journal={ arXiv preprint arXiv:2111.03977v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2111.03977v2 },
  abstract={ Seamless human robot interaction (HRI) and cooperative human-robot (HR) teaming critically rely upon accurate and timely human mental workload (MW) models. Cognitive Load Theory (CLT) suggests representative physical environments produce representative mental processes; physical environment fidelity corresponds with improved modeling accuracy. Virtual Reality (VR) systems provide immersive environments capable of replicating complicated scenarios, particularly those associated with high-risk, high-stress scenarios. Passive biosignal modeling shows promise as a noninvasive method of MW modeling. However, VR systems rarely include multimodal psychophysiological feedback or capitalize on biosignal data for online MW modeling. Here, we develop a novel VR simulation pipeline, inspired by the NASA Multi-Attribute Task Battery II (MATB-II) task architecture, capable of synchronous collection of objective performance, subjective performance, and passive human biosignals in a simulated hazardous exploration environment. Our system design extracts and publishes biofeatures through the Robot Operating System (ROS), facilitating real time psychophysiology-based MW model integration into complete end-to-end systems. A VR simulation pipeline capable of evaluating MWs online could be foundational for advancing HR systems and VR experiences by enabling these systems to adaptively alter their behaviors in response to operator MW. }
}

@article{211008717v1,
  title={ Hand Gesture Recognition Using Temporal Convolutions and Attention   Mechanism },
  author={ Elahe Rahimian and Soheil Zabihi and Amir Asif and Dario Farina and S. Farokh Atashzar and Arash Mohammadi },
  journal={ arXiv preprint arXiv:2110.08717v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2110.08717v1 },
  abstract={ Advances in biosignal signal processing and machine learning, in particular Deep Neural Networks (DNNs), have paved the way for the development of innovative Human-Machine Interfaces for decoding the human intent and controlling artificial limbs. DNN models have shown promising results with respect to other algorithms for decoding muscle electrical activity, especially for recognition of hand gestures. Such data-driven models, however, have been challenged by their need for a large number of trainable parameters and their structural complexity. Here we propose the novel Temporal Convolutions-based Hand Gesture Recognition architecture (TC-HGR) to reduce this computational burden. With this approach, we classified 17 hand gestures via surface Electromyogram (sEMG) signals by the adoption of attention mechanisms and temporal convolutions. The proposed method led to 81.65\% and 80.72\% classification accuracy for window sizes of 300ms and 200ms, respectively. The number of parameters to train the proposed TC-HGR architecture is 11.9 times less than that of its state-of-the-art counterpart. }
}

@article{210913225v1,
  title={ Predicting Driver Self-Reported Stress by Analyzing the Road Scene },
  author={ Cristina Bustos and Neska Elhaouij and Albert Sole-Ribalta and Javier Borge-Holthoefer and Agata Lapedriza and Rosalind Picard },
  journal={ arXiv preprint arXiv:2109.13225v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2109.13225v1 },
  abstract={ Several studies have shown the relevance of biosignals in driver stress recognition. In this work, we examine something important that has been less frequently explored: We develop methods to test if the visual driving scene can be used to estimate a drivers' subjective stress levels. For this purpose, we use the AffectiveROAD video recordings and their corresponding stress labels, a continuous human-driver-provided stress metric. We use the common class discretization for stress, dividing its continuous values into three classes: low, medium, and high. We design and evaluate three computer vision modeling approaches to classify the driver's stress levels: (1) object presence features, where features are computed using automatic scene segmentation; (2) end-to-end image classification; and (3) end-to-end video classification. All three approaches show promising results, suggesting that it is possible to approximate the drivers' subjective stress from the information found in the visual scene. We observe that the video classification, which processes the temporal information integrated with the visual information, obtains the highest accuracy of \$0.72\$, compared to a random baseline accuracy of \$0.33\$ when tested on a set of nine drivers. }
}

@article{200913453v1,
  title={ Universal Physiological Representation Learning with Soft-Disentangled   Rateless Autoencoders },
  author={ Mo Han and Ozan Ozdenizci and Toshiaki Koike-Akino and Ye Wang and Deniz Erdogmus },
  journal={ arXiv preprint arXiv:2009.13453v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2009.13453v1 },
  abstract={ Human computer interaction (HCI) involves a multidisciplinary fusion of technologies, through which the control of external devices could be achieved by monitoring physiological status of users. However, physiological biosignals often vary across users and recording sessions due to unstable physical/mental conditions and task-irrelevant activities. To deal with this challenge, we propose a method of adversarial feature encoding with the concept of a Rateless Autoencoder (RAE), in order to exploit disentangled, nuisance-robust, and universal representations. We achieve a good trade-off between user-specific and task-relevant features by making use of the stochastic disentanglement of the latent representations by adopting additional adversarial networks. The proposed model is applicable to a wider range of unknown users and tasks as well as different classifiers. Results on cross-subject transfer evaluations show the advantages of the proposed framework, with up to an 11.6\% improvement in the average subject-transfer classification accuracy. }
}

@article{250319820v1,
  title={ A Systematic Review of EEG-based Machine Intelligence Algorithms for   Depression Diagnosis, and Monitoring },
  author={ Amir Nassibi and Christos Papavassiliou and Ildar Rakhmatulin and Danilo Mandic and S. Farokh Atashzar },
  journal={ arXiv preprint arXiv:2503.19820v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.19820v1 },
  abstract={ Depression disorder is a serious health condition that has affected the lives of millions of people around the world. Diagnosis of depression is a challenging practice that relies heavily on subjective studies and, in most cases, suffers from late findings. Electroencephalography (EEG) biomarkers have been suggested and investigated in recent years as a potential transformative objective practice. In this article, for the first time, a detailed systematic review of EEG-based depression diagnosis approaches is conducted using advanced machine learning techniques and statistical analyses. For this, 938 potentially relevant articles (since 1985) were initially detected and filtered into 139 relevant articles based on the review scheme 'preferred reporting items for systematic reviews and meta-analyses (PRISMA).' This article compares and discusses the selected articles and categorizes them according to the type of machine learning techniques and statistical analyses. Algorithms, preprocessing techniques, extracted features, and data acquisition systems are discussed and summarized. This review paper explains the existing challenges of the current algorithms and sheds light on the future direction of the field. This systematic review outlines the issues and challenges in machine intelligence for the diagnosis of EEG depression that can be addressed in future studies and possibly in future wearable technologies. }
}

@article{250315674v1,
  title={ Pervasive Sensing for Livestock Health and Activity Monitoring: Current   Methods and Techniques },
  author={ Jeffrey D Shulkin and Abhipol Vibhatasilpin and Vedant Adhana },
  journal={ arXiv preprint arXiv:2503.15674v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.15674v1 },
  abstract={ Pervasive sensing is transforming health and activity monitoring by enabling continuous and automated data collection through advanced sensing modalities. While extensive research has been conducted on human subjects, its application in livestock remains underexplored. In large-scale agriculture, real-time monitoring of biological signals and behavioral patterns can facilitate early disease detection, optimize feeding and breeding strategies, and ensure compliance with welfare standards. This survey examines key sensing technologies -- including structural vibration, radio frequency (RF), computer vision, and wearables -- highlighting their benefits and challenges in livestock monitoring. By comparing these approaches, we provide insights into their effectiveness, limitations, and potential for integration into modern smart farming systems. Finally, we discuss research gaps and future directions to advance pervasive sensing in livestock health and activity monitoring. }
}

@article{250215632v1,
  title={ Continual Person Identification using Footstep-Induced Floor Vibrations   on Heterogeneous Floor Structures },
  author={ Yiwen Dong and Hae Young Noh },
  journal={ arXiv preprint arXiv:2502.15632v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.15632v1 },
  abstract={ Person identification is important for smart buildings to provide personalized services such as health monitoring, activity tracking, and personnel management. However, previous person identification relies on pre-collected data from everyone, which is impractical in many buildings and public facilities in which visitors are typically expected. This calls for a continual person identification system that gradually learns people's identities on the fly. Existing studies use cameras to achieve this goal, but they require direct line-of-sight and also have raised privacy concerns in public. Other modalities such as wearables and pressure mats are limited by the requirement of device-carrying or dense deployment. Thus, prior studies introduced footstep-induced structural vibration sensing, which is non-intrusive and perceived as more privacy-friendly. However, this approach has a significant challenge: the high variability of vibration data due to structural heterogeneity and human gait variations, which makes online person identification algorithms perform poorly. In this paper, we characterize the variability in footstep-induced structural vibration data for accurate online person identification. To achieve this, we quantify and decompose different sources of variability and then design a feature transformation function to reduce the variability within each person's data to make different people's data more separable. We evaluate our approach through field experiments with 20 people. The results show a 70\% variability reduction and a 90\% accuracy for online person identification. }
}

@article{240811837v1,
  title={ MicroXercise: A Micro-Level Comparative and Explainable System for   Remote Physical Therapy },
  author={ Hanchen David Wang and Nibraas Khan and Anna Chen and Nilanjan Sarkar and Pamela Wisniewski and Meiyi Ma },
  journal={ arXiv preprint arXiv:2408.11837v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2408.11837v1 },
  abstract={ Recent global estimates suggest that as many as 2.41 billion individuals have health conditions that would benefit from rehabilitation services. Home-based Physical Therapy (PT) faces significant challenges in providing interactive feedback and meaningful observation for therapists and patients. To fill this gap, we present MicroXercise, which integrates micro-motion analysis with wearable sensors, providing therapists and patients with a comprehensive feedback interface, including video, text, and scores. Crucially, it employs multi-dimensional Dynamic Time Warping (DTW) and attribution-based explainable methods to analyze the existing deep learning neural networks in monitoring exercises, focusing on a high granularity of exercise. This synergistic approach is pivotal, providing output matching the input size to precisely highlight critical subtleties and movements in PT, thus transforming complex AI analysis into clear, actionable feedback. By highlighting these micro-motions in different metrics, such as stability and range of motion, MicroXercise significantly enhances the understanding and relevance of feedback for end-users. Comparative performance metrics underscore its effectiveness over traditional methods, such as a 39\% and 42\% improvement in Feature Mutual Information (FMI) and Continuity. MicroXercise is a step ahead in home-based physical therapy, providing a technologically advanced and intuitively helpful solution to enhance patient care and outcomes. }
}

@article{240408212v2,
  title={ Mental Stress Detection: Development and Evaluation of a Wearable In-Ear   Plethysmography },
  author={ Hika Barki and Wan-Young Chung },
  journal={ arXiv preprint arXiv:2404.08212v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2404.08212v2 },
  abstract={ Mental stress is a prevalent condition that can have negative impacts on one's health. Early detection and treatment are crucial for preventing related illnesses and maintaining overall wellness. This study presents a new method for identifying mental stress using a wearable biosensor worn in the ear. Data was gathered from 14 participants in a controlled environment using stress-inducing tasks such as memory and math tests. The raw photoplethysmography data was then processed by filtering, segmenting, and transforming it into scalograms using a continuous wavelet transform (CWT) which are based on two different mother wavelets, namely, a generalized Morse wavelet and the analytic Morlet (Gabor) wavelet. The scalograms were then passed through a convolutional neural network classifier, GoogLeNet, to classify the signals as stressed or non-stressed. The method achieved an outstanding result using the generalized Morse wavelet with an accuracy of 91.02\% and an F1-score of 90.95\%. This method demonstrates promise as a reliable tool for early detection and treatment of mental stress by providing real-time monitoring and allowing for preventive measures to be taken before it becomes a serious issue. }
}

@article{230110009v1,
  title={ Remote patient monitoring using artificial intelligence: Current state,   applications, and challenges },
  author={ Thanveer Shaik and Xiaohui Tao and Niall Higgins and Lin Li and Raj Gururajan and Xujuan Zhou and U. Rajendra Acharya },
  journal={ arXiv preprint arXiv:2301.10009v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2301.10009v1 },
  abstract={ The adoption of artificial intelligence (AI) in healthcare is growing rapidly. Remote patient monitoring (RPM) is one of the common healthcare applications that assist doctors to monitor patients with chronic or acute illness at remote locations, elderly people in-home care, and even hospitalized patients. The reliability of manual patient monitoring systems depends on staff time management which is dependent on their workload. Conventional patient monitoring involves invasive approaches which require skin contact to monitor health status. This study aims to do a comprehensive review of RPM systems including adopted advanced technologies, AI impact on RPM, challenges and trends in AI-enabled RPM. This review explores the benefits and challenges of patient-centric RPM architectures enabled with Internet of Things wearable devices and sensors using the cloud, fog, edge, and blockchain technologies. The role of AI in RPM ranges from physical activity classification to chronic disease monitoring and vital signs monitoring in emergency settings. This review results show that AI-enabled RPM architectures have transformed healthcare monitoring applications because of their ability to detect early deterioration in patients' health, personalize individual patient health parameter monitoring using federated learning, and learn human behavior patterns using techniques such as reinforcement learning. This review discusses the challenges and trends to adopt AI to RPM systems and implementation issues. The future directions of AI in RPM applications are analyzed based on the challenges and trends }
}

@article{210912739v2,
  title={ Layered, Tunable Graphene Oxide-Nylon Heterostructures for Wearable   Electrocardiogram Sensors },
  author={ Nicholas G. Hallfors and Dejan Maksimovski and Ilyas A. H. Farhat and Maguy Abi Jaoude and Aarthi R. Devarajan and Kin Liao and Mohammed Ismail and H. Pade and R. Y. Adhikari and Abdel F. Isakovic },
  journal={ arXiv preprint arXiv:2109.12739v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2109.12739v2 },
  abstract={ Nanoscale engineered materials combined with wearable wireless technologies can deliver a new level of health monitoring. A reduced graphene oxide-nylon composite material is developed and tested, demonstrating its usefulness as a material for sensors in wearable, long-term electrocardiogram (ECG) monitoring via a comparison to one of the widely used ECG sensors. The structural analysis by scanning electron (SEM) and atomic force microscopy (AFM) shows a limited number of defects on a macroscopic scale. Fourier Transform Infrared (FTIR) and Raman spectroscopy confirm the presence of rGOx, and the ratio of D- and G-features as a function of thickness correlates with the resistivity analysis. The negligible effect of the defects and the tunability of electrical and optical properties, together with live ECG data, demonstrate its signal transduction capability. }
}

@article{210503503v2,
  title={ Photonic Network Coding and Partial Protection for Optical Core   Networks: Two for a Tango },
  author={ Dao Thanh Hai },
  journal={ arXiv preprint arXiv:2105.03503v2 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2105.03503v2 },
  abstract={ The digital transformation is creating basically a digital version of our physical world and the currency in that digital space is data. Massive amount of data has been generated ranging from wearable devices monitoring our physical health every single millisecond to autonomous vehicles generating roughly 5Tb hourly to even astronomical activities producing an order of Exabytes on daily basis and then ultra-broadband Internet comes into play, moving such data to the cloud. Internet traffic therefore has been experiencing explosive growth and in this context, optical transport networks forming the backbone of the Internet are pushed for transformation in system capacity. While the intuitive solution of deploying multiple fibers can address the pressing demand for increased capacity, doing so does not bring improvement in economic of scales in terms of cost, power consumption and spectral efficiency. This necessitates for a different approach so that the fiber capacity could be utilized in a more efficient manner. In this paper, we focus on innovative techniques, that is, photonic network coding and partial protection, to reduce the effective traffic load in order to achieve greater capacity efficiency for optical transport networks. Specifically, the application of network coding is examined by upgrading the functionalities of intermediate nodes with all-optical processing (i.e., encoding and decoding) capabilities. Besides, partial protection relying on the premise of providing just enough bandwidth in case of failure events is investigated for saving the redundant protection capacity. That it takes two to tango, combining photonic network coding and partial protection therefore bring to light new opportunities and challenges. In mining such new avenue, we present insights on how to derive compounding gains to maximize spectral efficiency via a case study. }
}

@article{201111542v3,
  title={ Exploring Contrastive Learning in Human Activity Recognition for   Healthcare },
  author={ Chi Ian Tang and Ignacio Perez-Pozuelo and Dimitris Spathis and Cecilia Mascolo },
  journal={ arXiv preprint arXiv:2011.11542v3 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2011.11542v3 },
  abstract={ Human Activity Recognition (HAR) constitutes one of the most important tasks for wearable and mobile sensing given its implications in human well-being and health monitoring. Motivated by the limitations of labeled datasets in HAR, particularly when employed in healthcare-related applications, this work explores the adoption and adaptation of SimCLR, a contrastive learning technique for visual representations, to HAR. The use of contrastive learning objectives causes the representations of corresponding views to be more similar, and those of non-corresponding views to be more different. After an extensive evaluation exploring 64 combinations of different signal transformations for augmenting the data, we observed significant performance differences owing to the order and the function thereof. In particular, preliminary results indicated an improvement over supervised and unsupervised learning methods when using fine-tuning and random rotation for augmentation, however, future work should explore under which conditions SimCLR is beneficial for HAR systems and other healthcare-related applications. }
}

@article{210105766v1,
  title={ Ajalon: Simplifying the Authoring of Wearable Cognitive Assistants },
  author={ Truong An Pham and Junjue Wang and Yu Xiao and Padmanabhan Pillai and Roger Iyengar and Roberta Klatzky and Mahadev Satyanarayanan },
  journal={ arXiv preprint arXiv:2101.05766v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2101.05766v1 },
  abstract={ Wearable Cognitive Assistance (WCA) amplifies human cognition in real time through a wearable device and low-latency wireless access to edge computing infrastructure. It is inspired by, and broadens, the metaphor of GPS navigation tools that provide real-time step-by-step guidance, with prompt error detection and correction. WCA applications are likely to be transformative in education, health care, industrial troubleshooting, manufacturing, and many other areas. Today, WCA application development is difficult and slow, requiring skills in areas such as machine learning and computer vision that are not widespread among software developers. This paper describes Ajalon, an authoring toolchain for WCA applications that reduces the skill and effort needed at each step of the development pipeline. Our evaluation shows that Ajalon significantly reduces the effort needed to create new WCA applications. }
}

@article{210101284v1,
  title={ Advancing Computing's Foundation of US Industry \& Society },
  author={ Thomas M. Conte and Ian T. Foster and William Gropp and Mark D. Hill },
  journal={ arXiv preprint arXiv:2101.01284v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2101.01284v1 },
  abstract={ While past information technology (IT) advances have transformed society, future advances hold even greater promise. For example, we have only just begun to reap the changes from artificial intelligence (AI), especially machine learning (ML). Underlying IT's impact are the dramatic improvements in computer hardware, which deliver performance that unlock new capabilities. For example, recent successes in AI/ML required the synergy of improved algorithms and hardware architectures (e.g., general-purpose graphics processing units). However, unlike in the 20th Century and early 2000s, tomorrow's performance aspirations must be achieved without continued semiconductor scaling formerly provided by Moore's Law and Dennard Scaling. How will one deliver the next 100x improvement in capability at similar or less cost to enable great value? Can we make the next AI leap without 100x better hardware?   This whitepaper argues for a multipronged effort to develop new computing approaches beyond Moore's Law to advance the foundation that computing provides to US industry, education, medicine, science, and government. This impact extends far beyond the IT industry itself, as IT is now central for providing value across society, for example in semi-autonomous vehicles, tele-education, health wearables, viral analysis, and efficient administration. Herein we draw upon considerable visioning work by CRA's Computing Community Consortium (CCC) and the IEEE Rebooting Computing Initiative (IEEE RCI), enabled by thought leader input from industry, academia, and the US government. }
}

@article{241000366v1,
  title={ Easydiagnos: a framework for accurate feature selection for automatic   diagnosis in smart healthcare },
  author={ Prasenjit Maji and Amit Kumar Mondal and Hemanta Kumar Mondal and Saraju P. Mohanty },
  journal={ arXiv preprint arXiv:2410.00366v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.00366v1 },
  abstract={ The rapid advancements in artificial intelligence (AI) have revolutionized smart healthcare, driving innovations in wearable technologies, continuous monitoring devices, and intelligent diagnostic systems. However, security, explainability, robustness, and performance optimization challenges remain critical barriers to widespread adoption in clinical environments. This research presents an innovative algorithmic method using the Adaptive Feature Evaluator (AFE) algorithm to improve feature selection in healthcare datasets and overcome problems. AFE integrating Genetic Algorithms (GA), Explainable Artificial Intelligence (XAI), and Permutation Combination Techniques (PCT), the algorithm optimizes Clinical Decision Support Systems (CDSS), thereby enhancing predictive accuracy and interpretability. The proposed method is validated across three diverse healthcare datasets using six distinct machine learning algorithms, demonstrating its robustness and superiority over conventional feature selection techniques. The results underscore the transformative potential of AFE in smart healthcare, enabling personalized and transparent patient care. Notably, the AFE algorithm, when combined with a Multi-layer Perceptron (MLP), achieved an accuracy of up to 98.5\%, highlighting its capability to improve clinical decision-making processes in real-world healthcare applications. }
}

@article{231212587v2,
  title={ Real-Time Diagnostic Integrity Meets Efficiency: A Novel   Platform-Agnostic Architecture for Physiological Signal Compression },
  author={ Neel R Vora and Amir Hajighasemi and Cody T. Reynolds and Amirmohammad Radmehr and Mohamed Mohamed and Jillur Rahman Saurav and Abdul Aziz and Jai Prakash Veerla and Mohammad S Nasr and Hayden Lotspeich and Partha Sai Guttikonda and Thuong Pham and Aarti Darji and Parisa Boodaghi Malidarreh and Helen H Shang and Jay Harvey and Kan Ding and Phuc Nguyen and Jacob M Luber },
  journal={ arXiv preprint arXiv:2312.12587v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2312.12587v2 },
  abstract={ Head-based signals such as EEG, EMG, EOG, and ECG collected by wearable systems will play a pivotal role in clinical diagnosis, monitoring, and treatment of important brain disorder diseases.   However, the real-time transmission of the significant corpus physiological signals over extended periods consumes substantial power and time, limiting the viability of battery-dependent physiological monitoring wearables.   This paper presents a novel deep-learning framework employing a variational autoencoder (VAE) for physiological signal compression to reduce wearables' computational complexity and energy consumption.   Our approach achieves an impressive compression ratio of 1:293 specifically for spectrogram data, surpassing state-of-the-art compression techniques such as JPEG2000, H.264, Direct Cosine Transform (DCT), and Huffman Encoding, which do not excel in handling physiological signals.   We validate the efficacy of the compressed algorithms using collected physiological signals from real patients in the Hospital and deploy the solution on commonly used embedded AI chips (i.e., ARM Cortex V8 and Jetson Nano). The proposed framework achieves a 91\% seizure detection accuracy using XGBoost, confirming the approach's reliability, practicality, and scalability. }
}

@article{230311340v2,
  title={ HDformer: A Higher Dimensional Transformer for Diabetes Detection   Utilizing Long Range Vascular Signals },
  author={ Ella Lan },
  journal={ arXiv preprint arXiv:2303.11340v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2303.11340v2 },
  abstract={ Diabetes mellitus is a global concern, and early detection can prevent serious complications. 50\% of people with diabetes live undiagnosed, disproportionately afflicting low-income groups. Non-invasive methods have emerged for timely detection; however, their limited accuracy constrains clinical usage. In this research, we present a novel Higher-Dimensional Transformer (HDformer), the first Transformer-based architecture which utilizes long-range photoplethysmography (PPG) to detect diabetes. The long-range PPG maximizes the signal contextual information when compared to the less-than 30 second signals commonly used in existing research. To increase the computational efficiency of HDformer long-range processing, a new attention module, Time Square Attention (TSA), is invented to reduce the volume of tokens by more than 10x, while retaining the local/global dependencies. TSA converts the 1D inputs into 2D representations, grouping the adjacent points into a single 2D token. It then generates dynamic patches and feeds them into a gated mixture-of-experts (MoE) network, optimizing the learning on different attention areas. HDformer achieves state-of-the-art results (sensitivity 98.4, accuracy 97.3, specificity 92.8, AUC 0.929) on the standard MIMIC-III dataset, surpassing existing research. Furthermore, we develop an end-to-end solution where a low-cost wearable is prototyped to connect with the HDformer in the Cloud via a mobile app. This scalable, convenient, and affordable approach provides instantaneous detection and continuous monitoring for individuals. It aids doctors in easily screening for diabetes and safeguards underprivileged communities. The enhanced versatility of HDformer allows for efficient processing and learning of long-range signals in general one-dimensional time-series sequences, particularly for all biomedical waveforms. }
}

@article{230406506v1,
  title={ DiaTrend: A dataset from advanced diabetes technology to enable   development of novel analytic solutions },
  author={ Temiloluwa Prioleau and Abigail Bartolome and Richard Comi and Catherine Stanger },
  journal={ arXiv preprint arXiv:2304.06506v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2304.06506v1 },
  abstract={ Objective digital data is scarce yet needed in many domains to enable research that can transform the standard of healthcare. While data from consumer-grade wearables and smartphones is more accessible, there is critical need for similar data from clinical-grade devices used by patients with a diagnosed condition. The prevalence of wearable medical devices in the diabetes domain sets the stage for unique research and development within this field and beyond. However, the scarcity of open-source datasets presents a major barrier to progress. To facilitate broader research on diabetes-relevant problems and accelerate development of robust computational solutions, we provide the DiaTrend dataset. The DiaTrend dataset is composed of intensive longitudinal data from wearable medical devices, including a total of 27,561 days of continuous glucose monitor data and 8,220 days of insulin pump data from 54 patients with diabetes. This dataset is useful for developing novel analytic solutions that can reduce the disease burden for people living with diabetes and increase knowledge on chronic condition management in outpatient settings. }
}

@article{221211273v1,
  title={ A Wearable EEG System for Closed-Loop Neuromodulation of High-Frequency   Sleep-Related Oscillations },
  author={ Scott Bressler and Ryan Neely and Heather Read and Ryan Yost and David Wang },
  journal={ arXiv preprint arXiv:2212.11273v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2212.11273v1 },
  abstract={ In healthy sleepers, cortical alpha oscillations are present during the transition from wakefulness to sleep, and dissipate at sleep onset. For individuals with insomnia, alpha power is elevated during the wake-sleep transition and can persist throughout the night. Neuromodulation techniques using phase-locked stimulation have been put forth as alternatives to drugs for improving slow-wave sleep quality. Due to technical limitations, this approach has not been tested on faster frequency alpha oscillations. Here we examine the feasibility of using an endpoint-corrected version of the Hilbert Transform (ecHT) algorithm implemented on-device to measure alpha phase and deliver phase-locked auditory stimulation to modulate alpha and promote sleep initiation. First, the ecHT algorithm is implemented on a tabletop electroencephalogram (EEG) device and used to measure the timing of the auditory evoked response and its delivery at precise phases of the alpha oscillation. Secondly, a pilot at-home study tests feasibility to use a wearable version of the neuromodulation device for real-time phase-locked stimulation in the alpha (8-12 Hz) frequency range. Auditory stimulation was delivered at the intended phases of alpha with high precision, and alpha oscillations were affected differently by stimuli delivered at opposing phases. Our wearable system was capable of measuring sleep micro- and macro-events present in the EEG that were appropriate for clinical sleep scoring. Sleep onset latencies were reduced for a subset of subjects displaying sleep onset insomnia symptoms in the stimulation condition. This study demonstrates the feasibility of closed-loop tracking and neuromodulation of alpha oscillations using a wearable EEG device. Preliminary results suggest that this approach could be used to accelerate sleep initiation in individuals with objective insomnia symptoms. }
}

@article{210301014v4,
  title={ Beat-to-Beat Fetal Heart Rate Analysis Using Portable Medical Device and   Wavelet Transformation Technique },
  author={ Maria Farahi and Alicia Casals and Omid Sarrafzadeh and Yasaman Zamani and Hooran Ahmadi and Naeimeh Behbood and Hessam Habibian },
  journal={ arXiv preprint arXiv:2103.01014v4 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2103.01014v4 },
  abstract={ A beat-to-beat Tele-fetal Monitoring and comparison with clinical data are studied with a wavelet transformation approach. Tele-fetal monitoring is a big progress toward a wearable medical device for a pregnant woman capable of obtaining prenatal care at home. We apply a wavelet transformation algorithm for fetal cardiac monitoring using a portable fetal Doppler medical device. Choosing an appropriate mother wavelet, 85 different mother wavelets are investigated. The efficiency of the proposed method is evaluated using two data sets including public and clinical. From publicly available data on PhysioBank, and simultaneous clinical measurement, we prove that the comparison between obtained fetal heart rate by the algorithm and the baselines yields a promising accuracy beyond 95\%. Finally, we conclude that the proposed algorithm would be a robust technique for any similar tele-fetal monitoring approach. }
}

@article{200905394v1,
  title={ A Review on Security and Privacy of Internet of Medical Things },
  author={ Mohan Krishna Kagita and Navod Thilakarathne and Thippa Reddy Gadekallu and Praveen Kumar Reddy Maddikunta },
  journal={ arXiv preprint arXiv:2009.05394v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2009.05394v1 },
  abstract={ The Internet of Medical Things (IoMT) are increasing the accuracy, reliability, and the production capability of electronic devices by playing a very important part in the industry of healthcare. The available medical resources and services related to healthcare are working to get an interconnection with each other by the digital healthcare system by the contribution of the researchers. Sensors, wearable devices, medical devices, and clinical devices are all connected to form an ecosystem of the Internet of Medical Things. The different applications of healthcare are enabled by the Internet of Medical Things to reduce the healthcare costs, to attend the medical responses on time and it also helps in increasing the quality of the medical treatment. The healthcare industry is transformed by the Internet of Medical Things as it delivers targeted and personalized medical care and it also seamlessly enables the communication of medical data. Devices used in the medical field and their application are connected to the system of healthcare of Information technology with the help of the digital world. }
}

@article{250417204v1,
  title={ Factually: Exploring Wearable Fact-Checking for Augmented Truth   Discernment },
  author={ Chitralekha Gupta and Hanjun Wu and Praveen Sasikumar and Shreyas Sridhar and Priambudi Bagaskara and Suranga Nanayakkara },
  journal={ arXiv preprint arXiv:2504.17204v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2504.17204v1 },
  abstract={ Wearable devices are transforming human capabilities by seamlessly augmenting cognitive functions. In this position paper, we propose a voice-based, interactive learning companion designed to amplify and extend cognitive abilities through informal learning. Our vision is threefold: (1) to enable users to discover new knowledge on-the-go through contextual interactive quizzes, fostering critical thinking and mindfulness, (2) to proactively detect misinformation, empowering users to critically assess information in real time, and (3) to provide spoken language correction and prompting hints for second language learning and effective communication. As an initial step toward this vision, we present Factually - a proactive, wearable fact-checking system integrated into devices like smartwatches or rings. Factually discreetly alerts users to potential falsehoods via vibrotactile feedback, helping them assess information critically. We demonstrate its utility through three illustrative scenarios, highlighting its potential to extend cognitive abilities for real-time misinformation detection. Early qualitative feedback suggests that Factually can enhance users' fact-checking capabilities, offering both practical and experiential benefits. }
}

@article{250405983v1,
  title={ Modular Soft Wearable Glove for Real-Time Gesture Recognition and   Dynamic 3D Shape Reconstruction },
  author={ Huazhi Dong and Chunpeng Wang and Mingyuan Jiang and Francesco Giorgio-Serchi and Yunjie Yang },
  journal={ arXiv preprint arXiv:2504.05983v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2504.05983v1 },
  abstract={ With the increasing demand for human-computer interaction (HCI), flexible wearable gloves have emerged as a promising solution in virtual reality, medical rehabilitation, and industrial automation. However, the current technology still has problems like insufficient sensitivity and limited durability, which hinder its wide application. This paper presents a highly sensitive, modular, and flexible capacitive sensor based on line-shaped electrodes and liquid metal (EGaIn), integrated into a sensor module tailored to the human hand's anatomy. The proposed system independently captures bending information from each finger joint, while additional measurements between adjacent fingers enable the recording of subtle variations in inter-finger spacing. This design enables accurate gesture recognition and dynamic hand morphological reconstruction of complex movements using point clouds. Experimental results demonstrate that our classifier based on Convolution Neural Network (CNN) and Multilayer Perceptron (MLP) achieves an accuracy of 99.15\% across 30 gestures. Meanwhile, a transformer-based Deep Neural Network (DNN) accurately reconstructs dynamic hand shapes with an Average Distance (AD) of 2.076\\textbackslash{}pm3.231 mm, with the reconstruction accuracy at individual key points surpassing SOTA benchmarks by 9.7\% to 64.9\%. The proposed glove shows excellent accuracy, robustness and scalability in gesture recognition and hand reconstruction, making it a promising solution for next-generation HCI systems. }
}

@article{250309987v1,
  title={ Beyond Human: Cognitive and Physical Augmentation through AI, Robotics,   and XR -- Opportunities and Risks },
  author={ Jie Li and Anusha Withana and Alexandra Diening and Kai Kunze and Masahiko Inami },
  journal={ arXiv preprint arXiv:2503.09987v1 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2503.09987v1 },
  abstract={ As human augmentation technologies evolve, the convergence of AI, robotics, and extended reality (XR) is redefining human potential -- enhancing cognition, perception, and physical abilities. However, these advancements also introduce ethical dilemmas, security risks, and concerns over loss of control. This workshop explores both the transformative potential and the unintended consequences of augmentation technologies. Bringing together experts from HCI, neuroscience, robotics, and ethics, we will examine real-world applications, emerging risks, and governance strategies for responsible augmentation. The session will feature keynote talks and interactive discussions, addressing topics such as AI-enhanced cognition, wearable robotics, neural interfaces, and XR-driven augmentation. By fostering multidisciplinary dialogue, this workshop aims to generate actionable insights for responsible innovation, proposing ethical frameworks to balance human empowerment with risk mitigation. We invite researchers, practitioners, and industry leaders to contribute their perspectives and help shape the future of human augmentation. }
}

@article{241013847v2,
  title={ Adaptive Compressive Tactile Subsampling: Enabling High Spatiotemporal   Resolution in Scalable Robotic Skin },
  author={ Ariel Slepyan and Dian Li and Aidan Aug and Sriramana Sankar and Trac Tran and Nitish Thakor },
  journal={ arXiv preprint arXiv:2410.13847v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.13847v2 },
  abstract={ Robots, like humans, require full-body, high-resolution tactile sensing to operate safely and effectively in unstructured environments, enabling reflexive responses and closed-loop control. However, the high pixel counts necessary for dense, large-area coverage limit readout rates of most tactile arrays to below 100 Hz, hindering their use in high-speed tasks. We introduce Adaptive Compressive Tactile Subsampling (ACTS), a scalable and data-driven method that dramatically enhances the performance of traditional tactile matrices by leveraging sparse recovery and a learned tactile dictionary. Tested on a 1024-pixel tactile sensor array (32X32), ACTS achieved frame rates up to 1,000 Hz, an 18X improvement over conventional raster scanning, with minimal reconstruction error. For the first time, ACTS enables wearable, large-area, high-density tactile sensing systems that can deliver high-speed results. We demonstrate rapid object classification within 20 ms of contact, high-speed projectile detection, ricochet angle estimation, and soft deformation tracking, in tactile and robotics applications, all using flexible, high-density tactile arrays. These include high-resolution tactile gloves, pressure insoles, and full-body configurations covering robotic arms and human-sized mannequins. ACTS transforms standard, low-cost, and robust tactile sensors into high-speed systems, supporting applications from object manipulation to human-robot interaction. By enabling comprehensive, scalable, and efficient tactile coverage for robots and wearables, ACTS advances robotics toward lifelike, responsive, and adaptable operation in dynamic environments. }
}

@article{240905006v2,
  title={ HelmetPoser: A Helmet-Mounted IMU Dataset for Data-Driven Estimation of   Human Head Motion in Diverse Conditions },
  author={ Jianping Li and Qiutong Leng and Jinxing Liu and Xinhang Xu and Tongxin Jin and Muqing Cao and Thien-Minh Nguyen and Shenghai Yuan and Kun Cao and Lihua Xie },
  journal={ arXiv preprint arXiv:2409.05006v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2409.05006v2 },
  abstract={ Helmet-mounted wearable positioning systems are crucial for enhancing safety and facilitating coordination in industrial, construction, and emergency rescue environments. These systems, including LiDAR-Inertial Odometry (LIO) and Visual-Inertial Odometry (VIO), often face challenges in localization due to adverse environmental conditions such as dust, smoke, and limited visual features. To address these limitations, we propose a novel head-mounted Inertial Measurement Unit (IMU) dataset with ground truth, aimed at advancing data-driven IMU pose estimation. Our dataset captures human head motion patterns using a helmet-mounted system, with data from ten participants performing various activities. We explore the application of neural networks, specifically Long Short-Term Memory (LSTM) and Transformer networks, to correct IMU biases and improve localization accuracy. Additionally, we evaluate the performance of these methods across different IMU data window dimensions, motion patterns, and sensor types. We release a publicly available dataset, demonstrate the feasibility of advanced neural network approaches for helmet-based localization, and provide evaluation metrics to establish a baseline for future studies in this field. Data and code can be found at https://lqiutong.github.io/HelmetPoser.github.io/. }
}

@article{250205797v2,
  title={ Seamless Integration: The Evolution, Design, and Future Impact of   Wearable Technology },
  author={ David Pearl and James Intriligator and Xuanjiang Liu },
  journal={ arXiv preprint arXiv:2502.05797v2 },
  year={ 2025 },
  url={ http://arxiv.org/pdf/2502.05797v2 },
  abstract={ The rapid evolution of wearable technology marks a transformative phase in human-computer interaction, seamlessly integrating digital functionality into daily life. This paper explores the historical trajectory, current advancements, and future potential of wearables, emphasizing their impact on healthcare, productivity, and personal well-being. Key developments include the integration of artificial intelligence (AI), Internet of Things (IoT), and augmented reality (AR), driving personalization, real-time adaptability, and enhanced user experiences. The study highlights user-centered design principles, ethical considerations, and interdisciplinary collaboration as critical factors in creating wearables that are intuitive, inclusive, and secure. Furthermore, the paper examines sustainability trends, such as modular designs and eco-friendly materials, aligning innovation with environmental responsibility. By addressing challenges like data privacy, algorithmic bias, and usability, wearable technology is poised to redefine the interaction between humans and technology, offering unprecedented opportunities for enrichment and empowerment in diverse contexts. This comprehensive analysis provides a roadmap for advancing wearables to meet emerging societal needs while fostering ethical and sustainable growth. }
}

@article{241200396v1,
  title={ ARMOR: Egocentric Perception for Humanoid Robot Collision Avoidance and   Motion Planning },
  author={ Daehwa Kim and Mario Srouji and Chen Chen and Jian Zhang },
  journal={ arXiv preprint arXiv:2412.00396v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2412.00396v1 },
  abstract={ Humanoid robots have significant gaps in their sensing and perception, making it hard to perform motion planning in dense environments. To address this, we introduce ARMOR, a novel egocentric perception system that integrates both hardware and software, specifically incorporating wearable-like depth sensors for humanoid robots. Our distributed perception approach enhances the robot's spatial awareness, and facilitates more agile motion planning. We also train a transformer-based imitation learning (IL) policy in simulation to perform dynamic collision avoidance, by leveraging around 86 hours worth of human realistic motions from the AMASS dataset. We show that our ARMOR perception is superior against a setup with multiple dense head-mounted, and externally mounted depth cameras, with a 63.7\% reduction in collisions, and 78.7\% improvement on success rate. We also compare our IL policy against a sampling-based motion planning expert cuRobo, showing 31.6\% less collisions, 16.9\% higher success rate, and 26x reduction in computational latency. Lastly, we deploy our ARMOR perception on our real-world GR1 humanoid from Fourier Intelligence. We are going to update the link to the source code, HW description, and 3D CAD files in the arXiv version of this text. }
}

@article{241111658v1,
  title={ Introducing IHARDS-CNN: A Cutting-Edge Deep Learning Method for Human   Activity Recognition Using Wearable Sensors },
  author={ Nazanin Sedaghati and Masoud Kargar and Sina Abbaskhani },
  journal={ arXiv preprint arXiv:2411.11658v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2411.11658v1 },
  abstract={ Human activity recognition, facilitated by smart devices, has recently garnered significant attention. Deep learning algorithms have become pivotal in daily activities, sports, and healthcare. Nevertheless, addressing the challenge of extracting features from sensor data processing necessitates the utilization of diverse algorithms in isolation, subsequently transforming them into a standard mode. This research introduces a novel approach called IHARDS-CNN, amalgamating data from three distinct datasets (UCI-HAR, WISDM, and KU-HAR) for human activity recognition. The data collected from sensors embedded in smartwatches or smartphones encompass five daily activity classes. This study initially outlines the dataset integration approach, follows with a comprehensive statistical analysis, and assesses dataset accuracy. The proposed methodology employs a one-dimensional deep convolutional neural network for classification. Compared to extant activity recognition methods, this approach stands out for its high speed, reduced detection steps, and absence of the need to aggregate classified results. Despite fewer detection steps, empirical results demonstrate an impressive accuracy of nearly 100\%, marking it the highest among existing methods. Evaluation outcomes further highlight superior classification performance when compared to analogous architectures. }
}

@article{241110905v1,
  title={ Body-Resonance Human Body Communication },
  author={ Samyadip Sarkar and Qi Huang and Sarthak Antal and Mayukh Nath and Shreyas Sen },
  journal={ arXiv preprint arXiv:2411.10905v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2411.10905v1 },
  abstract={ Seamless interaction between Humans and AI-empowered battery-operated miniaturized electronic devices, exponentially transforming the wearable technology industry while forming an anthropomorphic artificial nervous system for distributed computing around the human body, demands high-speed low-power connectivity. If interconnected via radio frequency (RF) based wireless communication techniques, that being radiative, incur substantial absorption losses from the body during non-line-of-sight scenarios and consume higher power (more than 10s of mW). Although as a promising alternative with its non-radiative nature that resulted in 100X improvement in energy efficiency (sub-10 pJ/bit) and better signal confinement, Electro-Quasistatic Human Body Communication (EQS HBC) incurs moderate path loss (60-70 dB), limited data rate (less than 20 Mbps), making it less suitable for applications demanding fast connectivity like HD audio-video streaming, AR-VR-based products, distributed computing with wearable AI devices. Hence, to meet the requirement of energy-efficient connectivity at 100s of Mbps between wearables, we propose Body-Resonance (BR) HBC, which operates in the near-intermediate field and utilizes the transmission-line-like behavior of the body channel to offer 30X improvement in channel capacity. Our work sheds new light on the wireless communication system for wearables with potential to increase the channel gain by 20 dB with a 10X improvement in bandwidth compared to the EQS HBC for communication over on-body channels (whole-body coverage area). Experimentally demonstrating BR HBC, we presented low-loss (40-50 dB) and wide-band (hundreds of MHz) body channels that are 10X less leaky than radiative wireless communication, hence, can revolutionize the design of wireless communication system for several applications with wearables from healthcare, defense, to consumer electronics. }
}

@article{241013605v1,
  title={ Transformer-Based Approaches for Sensor-Based Human Activity   Recognition: Opportunities and Challenges },
  author={ Clayton Souza Leite and Henry Mauranen and Aziza Zhanabatyrova and Yu Xiao },
  journal={ arXiv preprint arXiv:2410.13605v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.13605v1 },
  abstract={ Transformers have excelled in natural language processing and computer vision, paving their way to sensor-based Human Activity Recognition (HAR). Previous studies show that transformers outperform their counterparts exclusively when they harness abundant data or employ compute-intensive optimization algorithms. However, neither of these scenarios is viable in sensor-based HAR due to the scarcity of data in this field and the frequent need to perform training and inference on resource-constrained devices. Our extensive investigation into various implementations of transformer-based versus non-transformer-based HAR using wearable sensors, encompassing more than 500 experiments, corroborates these concerns. We observe that transformer-based solutions pose higher computational demands, consistently yield inferior performance, and experience significant performance degradation when quantized to accommodate resource-constrained devices. Additionally, transformers demonstrate lower robustness to adversarial attacks, posing a potential threat to user trust in HAR. }
}

@article{241005449v1,
  title={ Skin Controlled Electronic and Neuromorphic Tattoos },
  author={ Dmitry Kireev and Nandu Koripally and Samuel Liu and Gabriella Coloyan Fleming and Philip Varkey and Joseph Belle and Sivasakthya Mohan and Sang Sub Han and Dong Xu and Yeonwoong Jung and Xiangfeng Duan and Jean Anne C. Incorvia and Deji Akinwande },
  journal={ arXiv preprint arXiv:2410.05449v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2410.05449v1 },
  abstract={ Wearable human activity sensors developed in the past decade show a distinct trend of becoming thinner and more imperceptible while retaining their electrical qualities, with graphene e-tattoos, as the ultimate example. A persistent challenge in modern wearables, however, is signal degradation due to the distance between the sensor's recording site and the signal transmission medium. To address this, we propose here to directly utilize human skin as a signal transmission medium as well as using low-cost gel electrodes for rapid probing of 2D transistor-based wearables. We demonstrate that the hypodermis layer of the skin can effectively serve as an electrolyte, enabling electrical potential application to semiconducting films made from graphene and other 2D materials placed on top of the skin. Graphene transistor tattoos, when biased through the body, exhibit high charge carrier mobility (up to 6500 2V-1s-1), with MoS2 and PtSe2 transistors showing mobilities up to 30 cm2V-1s-1 and 1 cm2V-1s-1, respectively. Finally, by introducing a layer of Nafion to the device structure, we observed neuromorphic functionality, transforming these e-tattoos into neuromorphic bioelectronic devices controlled through the skin itself. The neuromorphic bioelectronic tattoos have the potential for developing self-aware and stand-alone smart wearables, crucial for understanding and improving overall human performance. }
}

@article{240507606v2,
  title={ AIris: An AI-powered Wearable Assistive Device for the Visually Impaired },
  author={ Dionysia Danai Brilli and Evangelos Georgaras and Stefania Tsilivaki and Nikos Melanitis and Konstantina Nikita },
  journal={ arXiv preprint arXiv:2405.07606v2 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2405.07606v2 },
  abstract={ Assistive technologies for the visually impaired have evolved to facilitate interaction with a complex and dynamic world. In this paper, we introduce AIris, an AI-powered wearable device that provides environmental awareness and interaction capabilities to visually impaired users. AIris combines a sophisticated camera mounted on eyewear with a natural language processing interface, enabling users to receive real-time auditory descriptions of their surroundings. We have created a functional prototype system that operates effectively in real-world conditions. AIris demonstrates the ability to accurately identify objects and interpret scenes, providing users with a sense of spatial awareness previously unattainable with traditional assistive devices. The system is designed to be cost-effective and user-friendly, supporting general and specialized tasks: face recognition, scene description, text reading, object recognition, money counting, note-taking, and barcode scanning. AIris marks a transformative step, bringing AI enhancements to assistive technology, enabling rich interactions with a human-like feel. }
}

@article{240408213v1,
  title={ GazePointAR: A Context-Aware Multimodal Voice Assistant for Pronoun   Disambiguation in Wearable Augmented Reality },
  author={ Jaewook Lee and Jun Wang and Elizabeth Brown and Liam Chu and Sebastian S. Rodriguez and Jon E. Froehlich },
  journal={ arXiv preprint arXiv:2404.08213v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2404.08213v1 },
  abstract={ Voice assistants (VAs) like Siri and Alexa are transforming human-computer interaction; however, they lack awareness of users' spatiotemporal context, resulting in limited performance and unnatural dialogue. We introduce GazePointAR, a fully-functional context-aware VA for wearable augmented reality that leverages eye gaze, pointing gestures, and conversation history to disambiguate speech queries. With GazePointAR, users can ask ''what's over there?'' or ''how do I solve this math problem?'' simply by looking and/or pointing. We evaluated GazePointAR in a three-part lab study (N=12): (1) comparing GazePointAR to two commercial systems; (2) examining GazePointAR's pronoun disambiguation across three tasks; (3) and an open-ended phase where participants could suggest and try their own context-sensitive queries. Participants appreciated the naturalness and human-like nature of pronoun-driven queries, although sometimes pronoun use was counter-intuitive. We then iterated on GazePointAR and conducted a first-person diary study examining how GazePointAR performs in-the-wild. We conclude by enumerating limitations and design considerations for future context-aware VAs. }
}

@article{230514062v4,
  title={ Amplitude-Independent Machine Learning for PPG through Visibility Graphs   and Transfer Learning },
  author={ Yuyang Miao and Harry J. Davies and Danilo P. Mandic },
  journal={ arXiv preprint arXiv:2305.14062v4 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2305.14062v4 },
  abstract={ Photoplethysmography (PPG) refers to the measurement of variations in blood volume using light and is a feature of most wearable devices. The PPG signals provide insight into the body's circulatory system and can be employed to extract various bio-features, such as heart rate and vascular ageing. Although several algorithms have been proposed for this purpose, many exhibit limitations, including heavy reliance on human calibration, high signal quality requirements, and a lack of generalisation. In this paper, we introduce a PPG signal processing framework that integrates graph theory and computer vision algorithms, to provide an analysis framework which is amplitude-independent and invariant to affine transformations. It also requires minimal preprocessing, fuses information through RGB channels and exhibits robust generalisation across tasks and datasets. The proposed VGTL-net achieves state-of-the-art performance in the prediction of vascular ageing and demonstrates robust estimation of continuous blood pressure waveforms. }
}

@article{231217330v1,
  title={ Count What You Want: Exemplar Identification and Few-shot Counting of   Human Actions in the Wild },
  author={ Yifeng Huang and Duc Duy Nguyen and Lam Nguyen and Cuong Pham and Minh Hoai },
  journal={ arXiv preprint arXiv:2312.17330v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2312.17330v1 },
  abstract={ This paper addresses the task of counting human actions of interest using sensor data from wearable devices. We propose a novel exemplar-based framework, allowing users to provide exemplars of the actions they want to count by vocalizing predefined sounds ''one'', ''two'', and ''three''. Our method first localizes temporal positions of these utterances from the audio sequence. These positions serve as the basis for identifying exemplars representing the action class of interest. A similarity map is then computed between the exemplars and the entire sensor data sequence, which is further fed into a density estimation module to generate a sequence of estimated density values. Summing these density values provides the final count. To develop and evaluate our approach, we introduce a diverse and realistic dataset consisting of real-world data from 37 subjects and 50 action categories, encompassing both sensor and audio data. The experiments on this dataset demonstrate the viability of the proposed method in counting instances of actions from new classes and subjects that were not part of the training data. On average, the discrepancy between the predicted count and the ground truth value is 7.47, significantly lower than the errors of the frequency-based and transformer-based methods. Our project, code and dataset can be found at https://github.com/cvlab-stonybrook/ExRAC. }
}

@article{231112674v1,
  title={ Contrastive Left-Right Wearable Sensors (IMUs) Consistency Matching for   HAR },
  author={ Dominique Nshimyimana and Vitor Fortes Rey and Paul Lukowic },
  journal={ arXiv preprint arXiv:2311.12674v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2311.12674v1 },
  abstract={ Machine learning algorithms are improving rapidly, but annotating training data remains a bottleneck for many applications. In this paper, we show how real data can be used for self-supervised learning without any transformations by taking advantage of the symmetry present in the activities. Our approach involves contrastive matching of two different sensors (left and right wrist or leg-worn IMUs) to make representations of co-occurring sensor data more similar and those of non-co-occurring sensor data more different. We test our approach on the Opportunity and MM-Fit datasets. In MM-Fit we show significant improvement over the baseline supervised and self-supervised method SimCLR, while for Opportunity there is significant improvement over the supervised baseline and slight improvement when compared to SimCLR. Moreover, our method improves supervised baselines even when using only a small amount of the data for training. Future work should explore under which conditions our method is beneficial for human activity recognition systems and other related applications. }
}

@article{230907152v1,
  title={ Novel Smart N95 Filtering Facepiece Respirator with Real-time Adaptive   Fit Functionality and Wireless Humidity Monitoring for Enhanced Wearable   Comfort },
  author={ Kangkyu Kwon and Yoon Jae Lee and Yeongju Jung and Ira Soltis and Chanyeong Choi and Yewon Na and Lissette Romero and Myung Chul Kim and Nathan Rodeheaver and Hodam Kim and Michael S. Lloyd and Ziqing Zhuang and William King and Susan Xu and Seung-Hwan Ko and Jinwoo Lee and Woon-Hong Yeo },
  journal={ arXiv preprint arXiv:2309.07152v1 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2309.07152v1 },
  abstract={ The widespread emergence of the COVID-19 pandemic has transformed our lifestyle, and facial respirators have become an essential part of daily life. Nevertheless, the current respirators possess several limitations such as poor respirator fit because they are incapable of covering diverse human facial sizes and shapes, potentially diminishing the effect of wearing respirators. In addition, the current facial respirators do not inform the user of the air quality within the smart facepiece respirator in case of continuous long-term use. Here, we demonstrate the novel smart N-95 filtering facepiece respirator that incorporates the humidity sensor and pressure sensory feedback-enabled self-fit adjusting functionality for the effective performance of the facial respirator to prevent the transmission of airborne pathogens. The laser-induced graphene (LIG) constitutes the humidity sensor, and the pressure sensor array based on the dielectric elastomeric sponge monitors the respirator contact on the face of the user, providing the sensory information for a closed-loop feedback mechanism. As a result of the self-fit adjusting mode along with elastomeric lining, the fit factor is increased by 3.20 and 5 times at average and maximum respectively. We expect that the experimental proof-of-concept of this work will offer viable solutions to the current commercial respirators to address the limitations. }
}

@article{230516487v2,
  title={ EgoHumans: An Egocentric 3D Multi-Human Benchmark },
  author={ Rawal Khirodkar and Aayush Bansal and Lingni Ma and Richard Newcombe and Minh Vo and Kris Kitani },
  journal={ arXiv preprint arXiv:2305.16487v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2305.16487v2 },
  abstract={ We present EgoHumans, a new multi-view multi-human video benchmark to advance the state-of-the-art of egocentric human 3D pose estimation and tracking. Existing egocentric benchmarks either capture single subject or indoor-only scenarios, which limit the generalization of computer vision algorithms for real-world applications. We propose a novel 3D capture setup to construct a comprehensive egocentric multi-human benchmark in the wild with annotations to support diverse tasks such as human detection, tracking, 2D/3D pose estimation, and mesh recovery. We leverage consumer-grade wearable camera-equipped glasses for the egocentric view, which enables us to capture dynamic activities like playing tennis, fencing, volleyball, etc. Furthermore, our multi-view setup generates accurate 3D ground truth even under severe or complete occlusion. The dataset consists of more than 125k egocentric images, spanning diverse scenes with a particular focus on challenging and unchoreographed multi-human activities and fast-moving egocentric views. We rigorously evaluate existing state-of-the-art methods and highlight their limitations in the egocentric scenario, specifically on multi-human tracking. To address such limitations, we propose EgoFormer, a novel approach with a multi-stream transformer architecture and explicit 3D spatial reasoning to estimate and track the human pose. EgoFormer significantly outperforms prior art by 13.6\% IDF1 on the EgoHumans dataset. }
}

@article{220909092v2,
  title={ TASKED: Transformer-based Adversarial learning for human activity   recognition using wearable sensors via Self-KnowledgE Distillation },
  author={ Sungho Suh and Vitor Fortes Rey and Paul Lukowicz },
  journal={ arXiv preprint arXiv:2209.09092v2 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2209.09092v2 },
  abstract={ Wearable sensor-based human activity recognition (HAR) has emerged as a principal research area and is utilized in a variety of applications. Recently, deep learning-based methods have achieved significant improvement in the HAR field with the development of human-computer interaction applications. However, they are limited to operating in a local neighborhood in the process of a standard convolution neural network, and correlations between different sensors on body positions are ignored. In addition, they still face significant challenging problems with performance degradation due to large gaps in the distribution of training and test data, and behavioral differences between subjects. In this work, we propose a novel Transformer-based Adversarial learning framework for human activity recognition using wearable sensors via Self-KnowledgE Distillation (TASKED), that accounts for individual sensor orientations and spatial and temporal features. The proposed method is capable of learning cross-domain embedding feature representations from multiple subjects datasets using adversarial learning and the maximum mean discrepancy (MMD) regularization to align the data distribution over multiple domains. In the proposed method, we adopt the teacher-free self-knowledge distillation to improve the stability of the training procedure and the performance of human activity recognition. Experimental results show that TASKED not only outperforms state-of-the-art methods on the four real-world public HAR datasets (alone or combined) but also improves the subject generalization effectively. }
}

@article{221106173v1,
  title={ Investigating Enhancements to Contrastive Predictive Coding for Human   Activity Recognition },
  author={ Harish Haresamudram and Irfan Essa and Thomas Ploetz },
  journal={ arXiv preprint arXiv:2211.06173v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2211.06173v1 },
  abstract={ The dichotomy between the challenging nature of obtaining annotations for activities, and the more straightforward nature of data collection from wearables, has resulted in significant interest in the development of techniques that utilize large quantities of unlabeled data for learning representations. Contrastive Predictive Coding (CPC) is one such method, learning effective representations by leveraging properties of time-series data to setup a contrastive future timestep prediction task. In this work, we propose enhancements to CPC, by systematically investigating the encoder architecture, the aggregator network, and the future timestep prediction, resulting in a fully convolutional architecture, thereby improving parallelizability. Across sensor positions and activities, our method shows substantial improvements on four of six target datasets, demonstrating its ability to empower a wide range of application scenarios. Further, in the presence of very limited labeled data, our technique significantly outperforms both supervised and self-supervised baselines, positively impacting situations where collecting only a few seconds of labeled data may be possible. This is promising, as CPC does not require specialized data transformations or reconstructions for learning effective representations. }
}

@article{211000244v3,
  title={ Lightweight Transformer in Federated Setting for Human Activity   Recognition },
  author={ Ali Raza and Kim Phuc Tran and Ludovic Koehl and Shujun Li and Xianyi Zeng and Khaled Benzaidi },
  journal={ arXiv preprint arXiv:2110.00244v3 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2110.00244v3 },
  abstract={ Human activity recognition (HAR) is a machine learning task with important applications in healthcare especially in the context of home care of patients and older adults. HAR is often based on data collected from smart sensors, particularly smart home IoT devices such as smartphones, wearables and other body sensors. Deep learning techniques like convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been used for HAR, both in centralized and federated settings. However, these techniques have certain limitations: RNNs cannot be easily parallelized, CNNs have the limitation of sequence length, and both are computationally expensive. Moreover, in home healthcare applications the centralized approach can raise serious privacy concerns since the sensors used by a HAR classifier collect a lot of highly personal and sensitive data about people in the home. In this paper, to address some of such challenges facing HAR, we propose a novel lightweight (one-patch) transformer, which can combine the advantages of RNNs and CNNs without their major limitations, and also TransFed, a more privacy-friendly, federated learning-based HAR classifier using our proposed lightweight transformer. We designed a testbed to construct a new HAR dataset from five recruited human participants, and used the new dataset to evaluate the performance of the proposed HAR classifier in both federated and centralized settings. Additionally, we use another public dataset to evaluate the performance of the proposed HAR classifier in centralized setting to compare it with existing HAR classifiers. The experimental results showed that our proposed new solution outperformed state-of-the-art HAR classifiers based on CNNs and RNNs, whiling being more computationally efficient. }
}

@article{220909391v1,
  title={ QuestSim: Human Motion Tracking from Sparse Sensors with Simulated   Avatars },
  author={ Alexander Winkler and Jungdam Won and Yuting Ye },
  journal={ arXiv preprint arXiv:2209.09391v1 },
  year={ 2022 },
  url={ http://arxiv.org/pdf/2209.09391v1 },
  abstract={ Real-time tracking of human body motion is crucial for interactive and immersive experiences in AR/VR. However, very limited sensor data about the body is available from standalone wearable devices such as HMDs (Head Mounted Devices) or AR glasses. In this work, we present a reinforcement learning framework that takes in sparse signals from an HMD and two controllers, and simulates plausible and physically valid full body motions. Using high quality full body motion as dense supervision during training, a simple policy network can learn to output appropriate torques for the character to balance, walk, and jog, while closely following the input signals. Our results demonstrate surprisingly similar leg motions to ground truth without any observations of the lower body, even when the input is only the 6D transformations of the HMD. We also show that a single policy can be robust to diverse locomotion styles, different body sizes, and novel environments. }
}

@article{200802397v4,
  title={ DANA: Dimension-Adaptive Neural Architecture for Multivariate Sensor   Data },
  author={ Mohammad Malekzadeh and Richard G. Clegg and Andrea Cavallaro and Hamed Haddadi },
  journal={ arXiv preprint arXiv:2008.02397v4 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2008.02397v4 },
  abstract={ Motion sensors embedded in wearable and mobile devices allow for dynamic selection of sensor streams and sampling rates, enabling several applications, such as power management and data-sharing control. While deep neural networks (DNNs) achieve competitive accuracy in sensor data classification, DNNs generally process incoming data from a fixed set of sensors with a fixed sampling rate, and changes in the dimensions of their inputs cause considerable accuracy loss, unnecessary computations, or failure in operation. We introduce a dimension-adaptive pooling (DAP) layer that makes DNNs flexible and more robust to changes in sensor availability and in sampling rate. DAP operates on convolutional filter maps of variable dimensions and produces an input of fixed dimensions suitable for feedforward and recurrent layers. We also propose a dimension-adaptive training (DAT) procedure for enabling DNNs that use DAP to better generalize over the set of feasible data dimensions at inference time. DAT comprises the random selection of dimensions during the forward passes and optimization with accumulated gradients of several backward passes. Combining DAP and DAT, we show how to transform non-adaptive DNNs into a Dimension-Adaptive Neural Architecture (DANA), while keeping the same number of parameters. Compared to existing approaches, our solution provides better classification accuracy over the range of possible data dimensions at inference time and does not require up-sampling or imputation, thus reducing unnecessary computations. Experiments on seven datasets (four benchmark real-world datasets for human activity recognition and three synthetic datasets) show that DANA prevents significant losses in classification accuracy of the state-of-the-art DNNs and, compared to baselines, it better captures correlated patterns in sensor data under dynamic sensor availability and varying sampling rates. }
}

@article{210100702v1,
  title={ A Novel Multi-Stage Training Approach for Human Activity Recognition   from Multimodal Wearable Sensor Data Using Deep Neural Network },
  author={ Tanvir Mahmud and A. Q. M. Sazzad Sayyed and Shaikh Anowarul Fattah and Sun-Yuan Kung },
  journal={ arXiv preprint arXiv:2101.00702v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2101.00702v1 },
  abstract={ Deep neural network is an effective choice to automatically recognize human actions utilizing data from various wearable sensors. These networks automate the process of feature extraction relying completely on data. However, various noises in time series data with complex inter-modal relationships among sensors make this process more complicated. In this paper, we have proposed a novel multi-stage training approach that increases diversity in this feature extraction process to make accurate recognition of actions by combining varieties of features extracted from diverse perspectives. Initially, instead of using single type of transformation, numerous transformations are employed on time series data to obtain variegated representations of the features encoded in raw data. An efficient deep CNN architecture is proposed that can be individually trained to extract features from different transformed spaces. Later, these CNN feature extractors are merged into an optimal architecture finely tuned for optimizing diversified extracted features through a combined training stage or multiple sequential training stages. This approach offers the opportunity to explore the encoded features in raw sensor data utilizing multifarious observation windows with immense scope for efficient selection of features for final convergence. Extensive experimentations have been carried out in three publicly available datasets that provide outstanding performance consistently with average five-fold cross-validation accuracy of 99.29\% on UCI HAR database, 99.02\% on USC HAR database, and 97.21\% on SKODA database outperforming other state-of-the-art approaches. }
}

@article{201107591v1,
  title={ Does spontaneous motion lead to intuitive Body-Machine Interfaces? A   fitness study of different body segments for wearable telerobotics },
  author={ Matteo Macchini and Jan Frogg and Fabrizio Schiano and Dario Floreano },
  journal={ arXiv preprint arXiv:2011.07591v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2011.07591v1 },
  abstract={ Human-Robot Interfaces (HRIs) represent a crucial component in telerobotic systems. Body-Machine Interfaces (BoMIs) based on body motion can feel more intuitive than standard HRIs for naive users as they leverage humans' natural control capability over their movements. Among the different methods used to map human gestures into robot commands, data-driven approaches select a set of body segments and transform their motion into commands for the robot based on the users' spontaneous motion patterns. Despite being a versatile and generic method, there is no scientific evidence that implementing an interface based on spontaneous motion maximizes its effectiveness. In this study, we compare a set of BoMIs based on different body segments to investigate this aspect. We evaluate the interfaces in a teleoperation task of a fixed-wing drone and observe users' performance and feedback. To this aim, we use a framework that allows a user to control the drone with a single Inertial Measurement Unit (IMU) and without prior instructions. We show through a user study that selecting the body segment for a BoMI based on spontaneous motion can lead to sub-optimal performance. Based on our findings, we suggest additional metrics based on biomechanical and behavioral factors that might improve data-driven methods for the design of HRIs. }
}

@article{200901224v1,
  title={ American Sign Language Recognition Using RF Sensing },
  author={ Sevgi Z. Gurbuz and Ali C. Gurbuz and Evie A. Malaia and Darrin J. Griffin and Chris Crawford and M. Mahbubur Rahman and Emre Kurtoglu and Ridvan Aksu and Trevor Macks and Robiulhossain Mdrafi },
  journal={ arXiv preprint arXiv:2009.01224v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2009.01224v1 },
  abstract={ Many technologies for human-computer interaction have been designed for hearing individuals and depend upon vocalized speech, precluding users of American Sign Language (ASL) in the Deaf community from benefiting from these advancements. While great strides have been made in ASL recognition with video or wearable gloves, the use of video in homes has raised privacy concerns, while wearable gloves severely restrict movement and infringe on daily life. Methods: This paper proposes the use of RF sensors for HCI applications serving the Deaf community. A multi-frequency RF sensor network is used to acquire non-invasive, non-contact measurements of ASL signing irrespective of lighting conditions. The unique patterns of motion present in the RF data due to the micro-Doppler effect are revealed using time-frequency analysis with the Short-Time Fourier Transform. Linguistic properties of RF ASL data are investigated using machine learning (ML). Results: The information content, measured by fractal complexity, of ASL signing is shown to be greater than that of other upper body activities encountered in daily living. This can be used to differentiate daily activities from signing, while features from RF data show that imitation signing by non-signers is 99\\textbackslash{}\% differentiable from native ASL signing. Feature-level fusion of RF sensor network data is used to achieve 72.5\\textbackslash{}\% accuracy in classification of 20 native ASL signs. Implications: RF sensing can be used to study dynamic linguistic properties of ASL and design Deaf-centric smart environments for non-invasive, remote recognition of ASL. ML algorithms should be benchmarked on native, not imitation, ASL data. }
}

@article{200610717v1,
  title={ A flexible spiraling-metasurface as a versatile haptic interface },
  author={ Osama R. Bilal and Vincenzo Costanza and Ali Israr and Antonio Palermo and Paolo Celli and Frances Lau and Chiara Daraio },
  journal={ arXiv preprint arXiv:2006.10717v1 },
  year={ 2020 },
  url={ http://arxiv.org/pdf/2006.10717v1 },
  abstract={ Haptic feedback is the most significant sensory interface following visual cues. Developing thin, flexible surfaces that function as haptic interfaces is important for augmenting virtual reality, wearable devices, robotics and prostheses. For example, adding a haptic feedback interface to prosthesis could improve their acceptance among amputees. State of the art programmable interfaces targeting the skin feel-of-touch through mechano-receptors are limited by inadequate sensory feedback, cumbersome mechanisms or narrow frequency of operation. Here, we present a flexible metasurface as a generic haptic interface capable of producing complex tactile patterns on the human skin at wide range of frequencies. The metasurface is composed of multiple ''pixels'' that can locally amplify both input displacements and forces. Each of these pixels encodes various deformation patterns capable of producing different sensations on contact. The metasurface can transform a harmonic signal containing multiple frequencies into a complex preprogrammed tactile pattern. Our findings, corroborated by user studies conducted on human candidates, can open new avenues for wearable and robotic interfaces. }
}

@article{230905927v2,
  title={ Frequency-Aware Masked Autoencoders for Multimodal Pretraining on   Biosignals },
  author={ Ran Liu and Ellen L. Zippi and Hadi Pouransari and Chris Sandino and Jingping Nie and Hanlin Goh and Erdrin Azemi and Ali Moin },
  journal={ arXiv preprint arXiv:2309.05927v2 },
  year={ 2023 },
  url={ http://arxiv.org/pdf/2309.05927v2 },
  abstract={ Leveraging multimodal information from biosignals is vital for building a comprehensive representation of people's physical and mental states. However, multimodal biosignals often exhibit substantial distributional shifts between pretraining and inference datasets, stemming from changes in task specification or variations in modality compositions. To achieve effective pretraining in the presence of potential distributional shifts, we propose a frequency-aware masked autoencoder (\$\\textbackslash{}texttt\{bio\}\$FAME) that learns to parameterize the representation of biosignals in the frequency space. \$\\textbackslash{}texttt\{bio\}\$FAME incorporates a frequency-aware transformer, which leverages a fixed-size Fourier-based operator for global token mixing, independent of the length and sampling rate of inputs. To maintain the frequency components within each input channel, we further employ a frequency-maintain pretraining strategy that performs masked autoencoding in the latent space. The resulting architecture effectively utilizes multimodal information during pretraining, and can be seamlessly adapted to diverse tasks and modalities at test time, regardless of input size and order. We evaluated our approach on a diverse set of transfer experiments on unimodal time series, achieving an average of \$\\textbackslash{}uparrow\$5.5\% improvement in classification accuracy over the previous state-of-the-art. Furthermore, we demonstrated that our architecture is robust in modality mismatch scenarios, including unpredicted modality dropout or substitution, proving its practical utility in real-world applications. Code is available at https://github.com/apple/ml-famae . }
}

@article{240217572v1,
  title={ Hyperdimensional computing: a fast, robust and interpretable paradigm   for biological data },
  author={ Michiel Stock and Dimitri Boeckaerts and Pieter Dewulf and Steff Taelman and Maxime Van Haeverbeke and Wim Van Criekinge and Bernard De Baets },
  journal={ arXiv preprint arXiv:2402.17572v1 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2402.17572v1 },
  abstract={ Advances in bioinformatics are primarily due to new algorithms for processing diverse biological data sources. While sophisticated alignment algorithms have been pivotal in analyzing biological sequences, deep learning has substantially transformed bioinformatics, addressing sequence, structure, and functional analyses. However, these methods are incredibly data-hungry, compute-intensive and hard to interpret. Hyperdimensional computing (HDC) has recently emerged as an intriguing alternative. The key idea is that random vectors of high dimensionality can represent concepts such as sequence identity or phylogeny. These vectors can then be combined using simple operators for learning, reasoning or querying by exploiting the peculiar properties of high-dimensional spaces. Our work reviews and explores the potential of HDC for bioinformatics, emphasizing its efficiency, interpretability, and adeptness in handling multimodal and structured data. HDC holds a lot of potential for various omics data searching, biosignal analysis and health applications. }
}

@article{240209558v3,
  title={ Bidirectional Generative Pre-training for Improving Healthcare   Time-series Representation Learning },
  author={ Ziyang Song and Qincheng Lu and He Zhu and David Buckeridge and Yue Li },
  journal={ arXiv preprint arXiv:2402.09558v3 },
  year={ 2024 },
  url={ http://arxiv.org/pdf/2402.09558v3 },
  abstract={ Learning time-series representations for discriminative tasks, such as classification and regression, has been a long-standing challenge in the healthcare domain. Current pre-training methods are limited in either unidirectional next-token prediction or randomly masked token prediction. We propose a novel architecture called Bidirectional Timely Generative Pre-trained Transformer (BiTimelyGPT), which pre-trains on biosignals and longitudinal clinical records by both next-token and previous-token prediction in alternating transformer layers. This pre-training task preserves original distribution and data shapes of the time-series. Additionally, the full-rank forward and backward attention matrices exhibit more expressive representation capabilities. Using biosignals and longitudinal clinical records, BiTimelyGPT demonstrates superior performance in predicting neurological functionality, disease diagnosis, and physiological signs. By visualizing the attention heatmap, we observe that the pre-trained BiTimelyGPT can identify discriminative segments from biosignal time-series sequences, even more so after fine-tuning on the task. }
}

@article{210308657v1,
  title={ Towards New Multiwavelets: Associated Filters and Algorithms. Part I:   Theoretical Framework and Investigation of Biomedical Signals, ECG and   Coronavirus Cases },
  author={ Malika Jallouli and Makerem Zemni and Anouar Ben Mabrouk and Mohamed Ali Mahjoub },
  journal={ arXiv preprint arXiv:2103.08657v1 },
  year={ 2021 },
  url={ http://arxiv.org/pdf/2103.08657v1 },
  abstract={ Biosignals are nowadays important subjects for scientific researches from both theory and applications especially with the appearance of new pandemics threatening humanity such as the new Coronavirus. One aim in the present work is to prove that Wavelets may be successful machinery to understand such phenomena by applying a step forward extension of wavelets to multiwavelets. We proposed in a first step to improve the multiwavelet notion by constructing more general families using independent components for multi-scaling and multiwavelet mother functions. A special multiwavelet is then introduced, continuous and discrete multiwavelet transforms are associated, as well as new filters and algorithms of decomposition and reconstruction. The constructed multiwavelet framework is applied for some experimentations showing fast algorithms, ECG signal, and a strain of Coronavirus processing. }
}